{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 07:54:55.674914: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-24 07:54:56.169480: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Bidirectional, Dropout, Activation, Dense, LSTM\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier as scikeras_KerasClassifier\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../dataset_mngr'))\n",
    "\n",
    "import split_merge as sm\n",
    "import balance_light as balance\n",
    "import model_mngr as modmgr\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "PATH_DATA = \"/Data\"\n",
    "PATH_DATA_DTS=PATH_DATA+\"/DTS_FULL/\"\n",
    "PATH_MODELS= \"/usr/local/models/\"\n",
    "\n",
    "SUFFIX_TRAIN=\"_TRAIN.zip\"\n",
    "SUFFIX_VAL=\"_VAL.zip\"\n",
    "SUFFIX_CONF=\"_CONF.zip\"\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update and save the scaler if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_20d\"\n",
    "\n",
    "df_class=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_TRAIN,sep=\",\",index_col=[\"OPEN_DATETIME\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class=df_class.dropna(subset=[label])\n",
    "df_class=df_class.sort_index()\n",
    "\n",
    "df_norm,norm_scaler= balance.normalize_df(df_in=df_class,str_label=label,tuple_ft_range=(-1,1))\n",
    "\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2405\"\n",
    "scaler_name=file_name+\"_scaler.save\"\n",
    "joblib.dump(norm_scaler,filename=PATH_MODELS+scaler_name)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train et val df, normalize,  undersample  and preparation for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_20d\"\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2405\"\n",
    "scaler_name=file_name+\"_scaler.save\"\n",
    "scaler=joblib.load(PATH_MODELS+scaler_name)\n",
    "\n",
    "\n",
    "df_class=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_TRAIN,sep=\",\",index_col=[\"OPEN_DATETIME\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class=df_class.dropna(subset=[label])\n",
    "df_class=df_class.loc['1995-01-01':] # drop rows < 1995-01-01\n",
    "df_class=df_class.sort_index()\n",
    "df_class_val=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_VAL,sep=\",\",index_col=[\"OPEN_DATETIME\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class_val=df_class_val.dropna(subset=[label])\n",
    "df_class_val=df_class_val.sort_index()\n",
    "\n",
    "# normalize df_class and df_class_val\n",
    "df_class_train_norm=balance.normalize_df_scaler(df_in=df_class, str_label=label,scaler=scaler)\n",
    "df_class_val_norm=balance.normalize_df_scaler(df_in=df_class_val, str_label=label,scaler=scaler)\n",
    "\n",
    "list_feat = df_class.columns.values.tolist()\n",
    "list_feat.remove(label)\n",
    "\n",
    "nb_val=20000 #211000\n",
    "df_class_train_norm=balance.class_custom_undersampler(df_class_train_norm,label,nb_val)\n",
    "df_x_train, col_y_train = sm.split_df_x_y(\n",
    "    df_in=df_class_train_norm, list_features=list_feat, str_label=label, drop_na=True)\n",
    "# print(col_y_train.value_counts().sort_index())\n",
    "\n",
    "nb_val=5000#53000\n",
    "df_class_val_norm=balance.class_custom_undersampler(df_class_val_norm,label,nb_val)\n",
    "df_x_val, col_y_val = sm.split_df_x_y(\n",
    "    df_in=df_class_val_norm, list_features=list_feat, str_label=label, drop_na=True)\n",
    "\n",
    "sequence_length = 10\n",
    "\n",
    "x_train=df_x_train.values\n",
    "y_train=col_y_train.values\n",
    "x_val=df_x_val.values\n",
    "y_val=col_y_val.values\n",
    "x_train_lstm,y_train_lstm=sm.prepare_sequences(x_train,y_train,sequence_length)\n",
    "x_val_lstm,y_val_lstm=sm.prepare_sequences(x_val,y_val,sequence_length)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit__batch_size': 256, 'model__dropout': 0.05, 'model__layers': [64, 10], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.9}\n",
      "layers=[64, 10] meta={'classes_': array([0, 1, 2, 3]), 'target_type_': 'multilabel-indicator', 'y_dtype_': dtype('bool'), 'y_ndim_': 2, 'X_dtype_': dtype('float64'), 'X_shape_': (79991, 10, 27), 'n_features_in_': 10, 'target_encoder_': ClassifierLabelEncoder(), 'n_classes_': 4, 'n_outputs_': 1, 'n_outputs_expected_': 1, 'feature_encoder_': FunctionTransformer()}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - accuracy: 0.2763 - loss: 1.3819 - val_accuracy: 0.2656 - val_loss: 1.3833\n",
      "Epoch 2/3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 51ms/step - accuracy: 0.2850 - loss: 1.3771 - val_accuracy: 0.2805 - val_loss: 1.3753\n",
      "Epoch 3/3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 40ms/step - accuracy: 0.2977 - loss: 1.3697 - val_accuracy: 0.2888 - val_loss: 1.3677\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step\n",
      "Accuracy on Validation Set: 0.28877995097794007 cpt=1\n",
      "layers=[64, 10] meta={'classes_': array([0, 1, 2, 3]), 'target_type_': 'multilabel-indicator', 'y_dtype_': dtype('bool'), 'y_ndim_': 2, 'X_dtype_': dtype('float64'), 'X_shape_': (79991, 10, 27), 'n_features_in_': 10, 'target_encoder_': ClassifierLabelEncoder(), 'n_classes_': 4, 'n_outputs_': 1, 'n_outputs_expected_': 1, 'feature_encoder_': FunctionTransformer()}\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 39ms/step - accuracy: 0.2729 - loss: 1.3837 - val_accuracy: 0.2754 - val_loss: 1.3817\n",
      "Epoch 2/3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.2855 - loss: 1.3771 - val_accuracy: 0.2996 - val_loss: 1.3696\n",
      "Epoch 3/3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - accuracy: 0.3013 - loss: 1.3675 - val_accuracy: 0.3150 - val_loss: 1.3572\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step\n",
      "Accuracy on Validation Set: 0.3149917462858286 cpt=2\n",
      "layers=[64, 10] meta={'classes_': array([0, 1, 2, 3]), 'target_type_': 'multilabel-indicator', 'y_dtype_': dtype('bool'), 'y_ndim_': 2, 'X_dtype_': dtype('float64'), 'X_shape_': (79991, 10, 27), 'n_features_in_': 10, 'target_encoder_': ClassifierLabelEncoder(), 'n_classes_': 4, 'n_outputs_': 1, 'n_outputs_expected_': 1, 'feature_encoder_': FunctionTransformer()}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 41ms/step - accuracy: 0.2734 - loss: 1.3829 - val_accuracy: 0.2791 - val_loss: 1.3811\n",
      "Epoch 2/3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - accuracy: 0.2914 - loss: 1.3753 - val_accuracy: 0.2914 - val_loss: 1.3681\n",
      "Epoch 3/3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.3673 - val_accuracy: 0.3011 - val_loss: 1.3604\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step\n",
      "Accuracy on Validation Set: 0.3010854884698114 cpt=3\n",
      "Optim fail cpt=3 param suivant cpt_param=1\n",
      "Optim fail cpt=3\n"
     ]
    }
   ],
   "source": [
    "list_param_valid = [\n",
    "                    {'fit__batch_size': 256, 'model__dropout': 0.05, 'model__layers': [64, 10], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.9},\n",
    "                    # {'fit__batch_size': 32, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.7},\n",
    "                    # {'fit__batch_size': 32, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.5},\n",
    "                    # {'fit__batch_size': 64, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.9},\n",
    "                    # {'fit__batch_size': 64, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.7},\n",
    "                    # {'fit__batch_size': 64, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.5},\n",
    "]\n",
    "\n",
    "input_dim = x_train.shape[-1]\n",
    "num_classes = 4\n",
    "epochs = 3#350\n",
    "suffix=\"lstm_v1\"\n",
    "filename_tmp_model =PATH_MODELS+dts_name+\"_\"+suffix+\".keras\"\n",
    "patience = 3\n",
    "\n",
    "val_accuracy=0.0\n",
    "obj_acc=0.4\n",
    "cpt_param=0\n",
    "try_limit=3\n",
    "pct_check_class=0.7 # check if at least n% of the validation set per class\n",
    "\n",
    "len_val=len(x_val_lstm)\n",
    "check_class_limit=(len_val/num_classes)*pct_check_class\n",
    "check_class=False # check if at least obj_acc accuracy per class\n",
    "\n",
    "while(cpt_param<len(list_param_valid) and check_class==False):\n",
    "    param_valid=list_param_valid[cpt_param]\n",
    "    print(param_valid)\n",
    "    cpt=0\n",
    "\n",
    "    while(cpt<try_limit and check_class==False):\n",
    "        cpt+=1\n",
    "        es = EarlyStopping(monitor=\"val_loss\", patience=patience, mode=\"min\", verbose=2)\n",
    "        mc = ModelCheckpoint(filename_tmp_model, monitor=\"val_loss\",\n",
    "                            mode=\"min\", save_freq=\"epoch\", save_best_only=True)\n",
    "        lstm_model = scikeras_KerasClassifier(model=modmgr.create_scikeras_lstm_model, optimizer=\"adam\",optimizer__momentum=param_valid['optimizer__momentum'],\n",
    "                                            optimizer__lr=param_valid['optimizer__lr'], model__layers=param_valid['model__layers'], model__dropout=param_valid['model__dropout'],\n",
    "                                                callbacks=[es, mc], verbose=1)\n",
    "\n",
    "        history = lstm_model.fit(\n",
    "            x_train_lstm, y_train_lstm, batch_size=param_valid['fit__batch_size'], epochs=epochs, validation_data=(x_val_lstm, y_val_lstm))\n",
    "\n",
    "        train_loss = history.history_['loss']\n",
    "        val_loss = history.history_['val_loss']\n",
    "\n",
    "        # Plot loss\n",
    "        # epochs_done = range(1, len(train_loss) + 1)\n",
    "        # plt.plot(epochs_done, train_loss, 'bo-', label='Training Loss')\n",
    "        # plt.plot(epochs_done, val_loss, 'ro-', label='Validation Loss')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "\n",
    "        saved_model = load_model(filename_tmp_model)\n",
    "        # saved_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['loss'])\n",
    "        # loss, accuracy = saved_model.evaluate(x_valid, y_valid)\n",
    "\n",
    "        # Prediction on validation\n",
    "        y_pred = saved_model.predict(x_val_lstm)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Accuracy on validation\n",
    "        val_accuracy = metrics.accuracy_score(y_val_lstm.argmax(axis=1), y_pred_classes)\n",
    "        print(f\"Accuracy on Validation Set: {val_accuracy} {cpt=}\")\n",
    "\n",
    "        # check prediction au moins 30 par classe\n",
    "        if val_accuracy>=obj_acc:\n",
    "            check_class=True\n",
    "            for i in range(num_classes):\n",
    "                nb_lab=sum(y_pred_classes == i)\n",
    "                if nb_lab<check_class_limit  :\n",
    "                    check_class=False\n",
    "                    print(f\"Check class {i=} {nb_lab=} {check_class=} {check_class_limit=}\")\n",
    "\n",
    "    if cpt>=try_limit :\n",
    "        cpt_param+=1\n",
    "        print(f\"Optim fail {cpt=} param suivant {cpt_param=}\")\n",
    "\n",
    "if cpt>=try_limit :\n",
    "    print(f\"Optim fail {cpt=}\")\n",
    "\n",
    "else :\n",
    "    confusion = metrics.confusion_matrix(y_val_lstm.argmax(axis=1), y_pred_classes)\n",
    "    print(confusion)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Categ {i}: real {sum(y_val_lstm.argmax(axis=1) == i)} predict {sum(y_pred_classes == i)}\")\n",
    "\n",
    "    #check saved model\n",
    "    saved_model = load_model(filename_tmp_model)\n",
    "    y_pred = saved_model.predict(x_val_lstm)\n",
    "    confusion = metrics.confusion_matrix(y_val_lstm.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[-1]\n",
    "window_size = sequence_length\n",
    "dropout = 0.2\n",
    "num_classes = 4\n",
    "\n",
    "# cat_y_train = keras.utils.to_categorical(col_y_train, num_classes)\n",
    "# cat_y_valid = keras.utils.to_categorical(col_y_valid, num_classes)\n",
    "\n",
    "# df_x_train_exp = np.expand_dims(df_x_train, axis=2)\n",
    "# df_x_valid_exp = np.expand_dims(df_x_valid, axis=2)\n",
    "\n",
    "\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(LSTM(units=20, return_sequences=False,#True\n",
    "               input_shape=(window_size, input_dim)))\n",
    "#,kernel_regularizer=l2(0.1), recurrent_regularizer=l2(0.1), bias_regularizer=l2(0.1)\n",
    "model_LSTM.add(Dropout(rate=dropout))   \n",
    "# model_LSTM.add(Dropout(rate=dropout))\n",
    "# model_LSTM.add(Bidirectional(LSTM((window_size * 2), return_sequences=True)))\n",
    "# model_LSTM.add(Dropout(rate=dropout))\n",
    "# model_LSTM.add(Bidirectional(LSTM(window_size, return_sequences=False)))\n",
    "model_LSTM.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "model_LSTM.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model_LSTM.fit(x_train_lstm, y_train_lstm, batch_size=1024,\n",
    "                         shuffle=False, epochs=20, validation_data=(x_val_lstm, y_val_lstm))#,verbose=0\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Plot loss\n",
    "epochs = range(1, len(train_accuracy) + 1)\n",
    "plt.plot(epochs, train_accuracy, 'bo-', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'ro-', label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12302096189872760406\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# print if keras can use the gpu to train the model\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
