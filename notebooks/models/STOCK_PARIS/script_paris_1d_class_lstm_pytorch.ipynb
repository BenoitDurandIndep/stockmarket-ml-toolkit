{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'balance' from 'G:\\\\Python\\\\MarketDataEnrichment\\\\dataset_mngr\\\\balance.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import importlib\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset,RandomSampler\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import sqlite_io as sio\n",
    "\n",
    "import split_merge as sm\n",
    "import balance  # wait for new release https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1081\n",
    "import model_mngr as modmgr\n",
    "\n",
    "importlib.reload(sio)\n",
    "importlib.reload(modmgr)\n",
    "importlib.reload(sm)\n",
    "importlib.reload(balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = \"G:\\\\Python\\\\Data\"\n",
    "# PATH_DATA = \"C:\\\\Projets\\\\Data\"\n",
    "PATH_DB_FWK=PATH_DATA+\"\\\\sqlite\\\\dataset_market.db\"\n",
    "PATH_DB_STOCK=PATH_DATA+\"\\\\sqlite\\\\dataset_paris_stock_adjusted.db\"\n",
    "PATH_DATA_DTS=PATH_DATA+\"\\\\DTS_FULL\\\\\"\n",
    "\n",
    "SUFFIX_TRAIN=\"_TRAIN.zip\"\n",
    "SUFFIX_VAL=\"_VAL.zip\"\n",
    "SUFFIX_CONF=\"_CONF.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONNECTION TO SQLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"con_stock\" in locals():\n",
    "        sio.close_connection(con_stock)\n",
    "con_stock = sio.get_connection(str_db_path=PATH_DB_STOCK)\n",
    "\n",
    "if \"con_fwk\" in locals():\n",
    "        sio.close_connection(con_fwk)\n",
    "con_fwk = sio.get_connection(str_db_path=PATH_DB_FWK)\n",
    "\n",
    "my_session_maker = sessionmaker(bind=con_fwk)\n",
    "session=my_session_maker()\n",
    "\n",
    "table_stock=\"DS_PARIS_1D_ADJ_CLEAN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import add_indicators as indic\n",
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "\n",
    "df_base=sio.get_candles_to_df(session=session,con=con_stock, target_table=table_stock,tradable=True)\n",
    "df_work=pd.DataFrame()\n",
    "for code_value in df_base.index.get_level_values('CODE').unique():\n",
    "    sub_df=df_base[df_base.index.get_level_values('CODE') == code_value]\n",
    "    df_work_tmp = indic.add_indicators_to_df(con=con_fwk, df_in=sub_df, dts_name=dts_name,symbol=multi_symbol)\n",
    "    df_work = pd.concat([df_work, df_work_tmp])\n",
    "    \n",
    "df_work.sort_index(inplace=True)\n",
    "df_work.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_work[10000:10010]\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# print(df_work.describe())\n",
    "\n",
    "df_work.round(5).to_csv(\n",
    "    PATH_DATA_DTS+dts_name+\"_BASE.zip\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START HERE FOR BASE DATASET (all labels included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>OPEN</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>VOLUME</th>\n",
       "      <th>sma20</th>\n",
       "      <th>pos_sma20</th>\n",
       "      <th>sma50</th>\n",
       "      <th>sma200</th>\n",
       "      <th>pos_sma50</th>\n",
       "      <th>...</th>\n",
       "      <th>adx14</th>\n",
       "      <th>adx14_neg</th>\n",
       "      <th>adx14_pos</th>\n",
       "      <th>adx14_dif</th>\n",
       "      <th>avg_vol14</th>\n",
       "      <th>pos_avg_vol14</th>\n",
       "      <th>pos_sma20_200</th>\n",
       "      <th>williamsr_14</th>\n",
       "      <th>perf_sma_50_5d</th>\n",
       "      <th>perf_sma_200_5d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPEN_DATETIME</th>\n",
       "      <th>CODE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-04-26</th>\n",
       "      <th>AB.PA</th>\n",
       "      <td>12.98</td>\n",
       "      <td>12.98</td>\n",
       "      <td>12.20</td>\n",
       "      <td>12.68</td>\n",
       "      <td>62866.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.68000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-27</th>\n",
       "      <th>AB.PA</th>\n",
       "      <td>12.74</td>\n",
       "      <td>12.83</td>\n",
       "      <td>12.61</td>\n",
       "      <td>12.70</td>\n",
       "      <td>22370.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.69000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-28</th>\n",
       "      <th>AB.PA</th>\n",
       "      <td>12.70</td>\n",
       "      <td>12.70</td>\n",
       "      <td>12.41</td>\n",
       "      <td>12.50</td>\n",
       "      <td>8211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.62667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.01003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-29</th>\n",
       "      <th>AB.PA</th>\n",
       "      <td>12.60</td>\n",
       "      <td>12.65</td>\n",
       "      <td>12.46</td>\n",
       "      <td>12.64</td>\n",
       "      <td>4676.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.63000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-30</th>\n",
       "      <th>AB.PA</th>\n",
       "      <td>12.63</td>\n",
       "      <td>12.71</td>\n",
       "      <td>12.55</td>\n",
       "      <td>12.65</td>\n",
       "      <td>4470.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.63400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      OPEN   HIGH    LOW  CLOSE   VOLUME  sma20  pos_sma20  \\\n",
       "OPEN_DATETIME CODE                                                           \n",
       "2010-04-26    AB.PA  12.98  12.98  12.20  12.68  62866.0    NaN        NaN   \n",
       "2010-04-27    AB.PA  12.74  12.83  12.61  12.70  22370.0    NaN        NaN   \n",
       "2010-04-28    AB.PA  12.70  12.70  12.41  12.50   8211.0    NaN        NaN   \n",
       "2010-04-29    AB.PA  12.60  12.65  12.46  12.64   4676.0    NaN        NaN   \n",
       "2010-04-30    AB.PA  12.63  12.71  12.55  12.65   4470.0    NaN        NaN   \n",
       "\n",
       "                        sma50  sma200  pos_sma50  ...  adx14  adx14_neg  \\\n",
       "OPEN_DATETIME CODE                                ...                     \n",
       "2010-04-26    AB.PA  12.68000     NaN    0.00000  ...    0.0        0.0   \n",
       "2010-04-27    AB.PA  12.69000     NaN    0.00079  ...    0.0        0.0   \n",
       "2010-04-28    AB.PA  12.62667     NaN   -0.01003  ...    0.0        0.0   \n",
       "2010-04-29    AB.PA  12.63000     NaN    0.00079  ...    0.0        0.0   \n",
       "2010-04-30    AB.PA  12.63400     NaN    0.00127  ...    0.0        0.0   \n",
       "\n",
       "                     adx14_pos  adx14_dif  avg_vol14  pos_avg_vol14  \\\n",
       "OPEN_DATETIME CODE                                                    \n",
       "2010-04-26    AB.PA        0.0        0.0        NaN            NaN   \n",
       "2010-04-27    AB.PA        0.0        0.0        NaN            NaN   \n",
       "2010-04-28    AB.PA        0.0        0.0        NaN            NaN   \n",
       "2010-04-29    AB.PA        0.0        0.0        NaN            NaN   \n",
       "2010-04-30    AB.PA        0.0        0.0        NaN            NaN   \n",
       "\n",
       "                     pos_sma20_200  williamsr_14  perf_sma_50_5d  \\\n",
       "OPEN_DATETIME CODE                                                 \n",
       "2010-04-26    AB.PA            NaN           NaN             NaN   \n",
       "2010-04-27    AB.PA            NaN           NaN             NaN   \n",
       "2010-04-28    AB.PA            NaN           NaN             NaN   \n",
       "2010-04-29    AB.PA            NaN           NaN             NaN   \n",
       "2010-04-30    AB.PA            NaN           NaN             NaN   \n",
       "\n",
       "                     perf_sma_200_5d  \n",
       "OPEN_DATETIME CODE                    \n",
       "2010-04-26    AB.PA              NaN  \n",
       "2010-04-27    AB.PA              NaN  \n",
       "2010-04-28    AB.PA              NaN  \n",
       "2010-04-29    AB.PA              NaN  \n",
       "2010-04-30    AB.PA              NaN  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "# dts_name=\"PARIS_TREND_1D_50D_V1\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "\n",
    "df_work=pd.read_csv(PATH_DATA_DTS+dts_name+\"_BASE.zip\",sep=\",\",index_col=[\"OPEN_DATETIME\",\"CODE\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_work.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with no pos_sma200 \n",
    "df_work=df_work.dropna(subset=['pos_sma200'])\n",
    "\n",
    "# if williamsr_14 >0 =0 if williamsr_14<-100 = -100\n",
    "# df_work['williamsr_14']=df_work['williamsr_14'].apply(lambda x: 0 if x>0 else x)\n",
    "# df_work['williamsr_14']=df_work['williamsr_14'].apply(lambda x: -100 if x<-100 else x)\n",
    "\n",
    "# if williamsr_14 >0 =0 if williamsr_14<-100 = -100\n",
    "df_work.loc[df_work['williamsr_14'] > 0, 'williamsr_14'] = 0\n",
    "df_work.loc[df_work['williamsr_14'] < -100, 'williamsr_14'] = -100\n",
    "\n",
    "# print min and max of the columns williamsr_14, perf_sma_50_5d, perf_sma_200_5d\n",
    "# print(f\"{df_work['williamsr_14'].min()=}\")  inf-100\n",
    "# print(f\"{df_work['williamsr_14'].max()=}\") sup 0\n",
    "\n",
    "# df_check=df_work[df_work['perf_sma_50_5d'] > 1]\n",
    "# df_check=df_check[df_check['ret_1d'] <= 2]\n",
    "# print(df_check.index.get_level_values('CODE').unique())\n",
    "# df_check[df_check.index.get_level_values('CODE')=='AI.PA']\n",
    "# df_check.head(5)\n",
    "# df_check=df_work[df_work.index.get_level_values('CODE')=='AI.PA']\n",
    "# CATG\n",
    "# mask = df_work['stdev20_1d'] > 1000\n",
    "# df_work.drop(df_work[mask].index, inplace=True)\n",
    "# df_check[6000:6010]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work[10000:10010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           LABEL\n",
      "0   lab_perf_20d\n",
      "1   lab_perf_50d\n",
      "2  lab_perf_125d\n"
     ]
    }
   ],
   "source": [
    "df_work = indic.drop_indicators_by_type(\n",
    "    con=con_fwk, df_in=df_work, dts_name=dts_name, symbol=multi_symbol, ind_type=0)\n",
    "list_label = indic.get_ind_list_by_type_for_dts(\n",
    "    con=con_fwk, dts_name=dts_name, symbol_code=multi_symbol, ind_type=2)\n",
    "print(list_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>OPEN</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>VOLUME</th>\n",
       "      <th>pos_sma20</th>\n",
       "      <th>pos_sma50</th>\n",
       "      <th>pos_sma200</th>\n",
       "      <th>pos_sma50_200</th>\n",
       "      <th>pos_sma20_50</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_donchian20_lo</th>\n",
       "      <th>adx14</th>\n",
       "      <th>adx14_neg</th>\n",
       "      <th>adx14_pos</th>\n",
       "      <th>adx14_dif</th>\n",
       "      <th>pos_avg_vol14</th>\n",
       "      <th>pos_sma20_200</th>\n",
       "      <th>williamsr_14</th>\n",
       "      <th>perf_sma_50_5d</th>\n",
       "      <th>perf_sma_200_5d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPEN_DATETIME</th>\n",
       "      <th>CODE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">1995-11-24</th>\n",
       "      <th>BN.PA</th>\n",
       "      <td>5.3121</td>\n",
       "      <td>5.3292</td>\n",
       "      <td>5.3035</td>\n",
       "      <td>5.3121</td>\n",
       "      <td>1112239.0</td>\n",
       "      <td>0.01177</td>\n",
       "      <td>0.01509</td>\n",
       "      <td>0.03347</td>\n",
       "      <td>0.01811</td>\n",
       "      <td>0.00328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07840</td>\n",
       "      <td>20.28870</td>\n",
       "      <td>17.98263</td>\n",
       "      <td>24.68813</td>\n",
       "      <td>6.70549</td>\n",
       "      <td>0.57971</td>\n",
       "      <td>0.02145</td>\n",
       "      <td>-43.92655</td>\n",
       "      <td>-0.00423</td>\n",
       "      <td>0.00475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOI.PA</th>\n",
       "      <td>2.4317</td>\n",
       "      <td>2.4317</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>3392.0</td>\n",
       "      <td>-0.03791</td>\n",
       "      <td>-0.04912</td>\n",
       "      <td>0.10459</td>\n",
       "      <td>0.16165</td>\n",
       "      <td>-0.01165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>40.68240</td>\n",
       "      <td>27.60340</td>\n",
       "      <td>14.71410</td>\n",
       "      <td>-12.88930</td>\n",
       "      <td>0.08447</td>\n",
       "      <td>0.14812</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-0.00193</td>\n",
       "      <td>0.00800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDI.PA</th>\n",
       "      <td>5.5472</td>\n",
       "      <td>5.5958</td>\n",
       "      <td>5.5472</td>\n",
       "      <td>5.5958</td>\n",
       "      <td>85024.0</td>\n",
       "      <td>0.01444</td>\n",
       "      <td>0.04278</td>\n",
       "      <td>0.15177</td>\n",
       "      <td>0.10453</td>\n",
       "      <td>0.02793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>14.69556</td>\n",
       "      <td>19.13562</td>\n",
       "      <td>20.95757</td>\n",
       "      <td>1.82195</td>\n",
       "      <td>0.11627</td>\n",
       "      <td>0.13538</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00866</td>\n",
       "      <td>0.00701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEC.PA</th>\n",
       "      <td>3.9661</td>\n",
       "      <td>3.9661</td>\n",
       "      <td>3.9661</td>\n",
       "      <td>3.9661</td>\n",
       "      <td>162.0</td>\n",
       "      <td>-0.02968</td>\n",
       "      <td>-0.05170</td>\n",
       "      <td>-0.03635</td>\n",
       "      <td>0.01619</td>\n",
       "      <td>-0.02270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02507</td>\n",
       "      <td>14.74654</td>\n",
       "      <td>55.61796</td>\n",
       "      <td>43.88082</td>\n",
       "      <td>-11.73714</td>\n",
       "      <td>0.63943</td>\n",
       "      <td>-0.00688</td>\n",
       "      <td>-71.42857</td>\n",
       "      <td>-0.00965</td>\n",
       "      <td>0.00166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GFC.PA</th>\n",
       "      <td>3.3486</td>\n",
       "      <td>3.3486</td>\n",
       "      <td>3.2076</td>\n",
       "      <td>3.2093</td>\n",
       "      <td>32614.0</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>0.03218</td>\n",
       "      <td>0.24682</td>\n",
       "      <td>0.20795</td>\n",
       "      <td>0.02969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06339</td>\n",
       "      <td>33.09500</td>\n",
       "      <td>12.06264</td>\n",
       "      <td>26.76666</td>\n",
       "      <td>14.70403</td>\n",
       "      <td>2.22395</td>\n",
       "      <td>0.24381</td>\n",
       "      <td>-61.60998</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.01102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAT.PA</th>\n",
       "      <td>24.8529</td>\n",
       "      <td>24.8529</td>\n",
       "      <td>24.8529</td>\n",
       "      <td>24.8529</td>\n",
       "      <td>154.0</td>\n",
       "      <td>-0.00517</td>\n",
       "      <td>-0.01589</td>\n",
       "      <td>-0.05827</td>\n",
       "      <td>-0.04307</td>\n",
       "      <td>-0.01078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05672</td>\n",
       "      <td>11.41274</td>\n",
       "      <td>49.78475</td>\n",
       "      <td>43.88114</td>\n",
       "      <td>-5.90361</td>\n",
       "      <td>0.26857</td>\n",
       "      <td>-0.05338</td>\n",
       "      <td>-31.34328</td>\n",
       "      <td>-0.00242</td>\n",
       "      <td>-0.00235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LI.PA</th>\n",
       "      <td>2.0554</td>\n",
       "      <td>2.0554</td>\n",
       "      <td>2.0435</td>\n",
       "      <td>2.0435</td>\n",
       "      <td>143309.0</td>\n",
       "      <td>0.01303</td>\n",
       "      <td>-0.00332</td>\n",
       "      <td>-0.02706</td>\n",
       "      <td>-0.02382</td>\n",
       "      <td>-0.01615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03726</td>\n",
       "      <td>36.83981</td>\n",
       "      <td>9.74307</td>\n",
       "      <td>17.22198</td>\n",
       "      <td>7.47891</td>\n",
       "      <td>2.38002</td>\n",
       "      <td>-0.03958</td>\n",
       "      <td>-41.56051</td>\n",
       "      <td>-0.00162</td>\n",
       "      <td>-0.00054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RE.PA</th>\n",
       "      <td>4.8710</td>\n",
       "      <td>4.8710</td>\n",
       "      <td>4.7188</td>\n",
       "      <td>4.7188</td>\n",
       "      <td>33953.0</td>\n",
       "      <td>0.00203</td>\n",
       "      <td>-0.01415</td>\n",
       "      <td>-0.00043</td>\n",
       "      <td>0.01391</td>\n",
       "      <td>-0.01614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06898</td>\n",
       "      <td>12.87952</td>\n",
       "      <td>19.22271</td>\n",
       "      <td>23.88377</td>\n",
       "      <td>4.66107</td>\n",
       "      <td>0.72411</td>\n",
       "      <td>-0.00245</td>\n",
       "      <td>-44.43796</td>\n",
       "      <td>-0.01148</td>\n",
       "      <td>0.00182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAVE.PA</th>\n",
       "      <td>15.3615</td>\n",
       "      <td>15.6143</td>\n",
       "      <td>15.3615</td>\n",
       "      <td>15.3615</td>\n",
       "      <td>28928.0</td>\n",
       "      <td>0.03593</td>\n",
       "      <td>0.05608</td>\n",
       "      <td>0.05257</td>\n",
       "      <td>-0.00332</td>\n",
       "      <td>0.01945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08714</td>\n",
       "      <td>48.09569</td>\n",
       "      <td>4.50842</td>\n",
       "      <td>30.13034</td>\n",
       "      <td>25.62192</td>\n",
       "      <td>0.41972</td>\n",
       "      <td>0.01606</td>\n",
       "      <td>-36.89894</td>\n",
       "      <td>0.00149</td>\n",
       "      <td>0.00358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEP.PA</th>\n",
       "      <td>1.4524</td>\n",
       "      <td>1.4524</td>\n",
       "      <td>1.3325</td>\n",
       "      <td>1.3325</td>\n",
       "      <td>83775.0</td>\n",
       "      <td>-0.13757</td>\n",
       "      <td>-0.15298</td>\n",
       "      <td>-0.16537</td>\n",
       "      <td>-0.01463</td>\n",
       "      <td>-0.01787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>26.08714</td>\n",
       "      <td>65.25452</td>\n",
       "      <td>17.32163</td>\n",
       "      <td>-47.93289</td>\n",
       "      <td>3.00736</td>\n",
       "      <td>-0.03224</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-0.00313</td>\n",
       "      <td>-0.00357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          OPEN     HIGH      LOW    CLOSE     VOLUME  \\\n",
       "OPEN_DATETIME CODE                                                     \n",
       "1995-11-24    BN.PA     5.3121   5.3292   5.3035   5.3121  1112239.0   \n",
       "              BOI.PA    2.4317   2.4317   2.3817   2.3817     3392.0   \n",
       "              CDI.PA    5.5472   5.5958   5.5472   5.5958    85024.0   \n",
       "              ELEC.PA   3.9661   3.9661   3.9661   3.9661      162.0   \n",
       "              GFC.PA    3.3486   3.3486   3.2076   3.2093    32614.0   \n",
       "              LAT.PA   24.8529  24.8529  24.8529  24.8529      154.0   \n",
       "              LI.PA     2.0554   2.0554   2.0435   2.0435   143309.0   \n",
       "              RE.PA     4.8710   4.8710   4.7188   4.7188    33953.0   \n",
       "              SAVE.PA  15.3615  15.6143  15.3615  15.3615    28928.0   \n",
       "              TEP.PA    1.4524   1.4524   1.3325   1.3325    83775.0   \n",
       "\n",
       "                       pos_sma20  pos_sma50  pos_sma200  pos_sma50_200  \\\n",
       "OPEN_DATETIME CODE                                                       \n",
       "1995-11-24    BN.PA      0.01177    0.01509     0.03347        0.01811   \n",
       "              BOI.PA    -0.03791   -0.04912     0.10459        0.16165   \n",
       "              CDI.PA     0.01444    0.04278     0.15177        0.10453   \n",
       "              ELEC.PA   -0.02968   -0.05170    -0.03635        0.01619   \n",
       "              GFC.PA     0.00242    0.03218     0.24682        0.20795   \n",
       "              LAT.PA    -0.00517   -0.01589    -0.05827       -0.04307   \n",
       "              LI.PA      0.01303   -0.00332    -0.02706       -0.02382   \n",
       "              RE.PA      0.00203   -0.01415    -0.00043        0.01391   \n",
       "              SAVE.PA    0.03593    0.05608     0.05257       -0.00332   \n",
       "              TEP.PA    -0.13757   -0.15298    -0.16537       -0.01463   \n",
       "\n",
       "                       pos_sma20_50  ...  pos_donchian20_lo     adx14  \\\n",
       "OPEN_DATETIME CODE                   ...                                \n",
       "1995-11-24    BN.PA         0.00328  ...            0.07840  20.28870   \n",
       "              BOI.PA       -0.01165  ...            0.00000  40.68240   \n",
       "              CDI.PA        0.02793  ...            0.04362  14.69556   \n",
       "              ELEC.PA      -0.02270  ...            0.02507  14.74654   \n",
       "              GFC.PA        0.02969  ...            0.06339  33.09500   \n",
       "              LAT.PA       -0.01078  ...            0.05672  11.41274   \n",
       "              LI.PA        -0.01615  ...            0.03726  36.83981   \n",
       "              RE.PA        -0.01614  ...            0.06898  12.87952   \n",
       "              SAVE.PA       0.01945  ...            0.08714  48.09569   \n",
       "              TEP.PA       -0.01787  ...            0.00000  26.08714   \n",
       "\n",
       "                       adx14_neg  adx14_pos  adx14_dif  pos_avg_vol14  \\\n",
       "OPEN_DATETIME CODE                                                      \n",
       "1995-11-24    BN.PA     17.98263   24.68813    6.70549        0.57971   \n",
       "              BOI.PA    27.60340   14.71410  -12.88930        0.08447   \n",
       "              CDI.PA    19.13562   20.95757    1.82195        0.11627   \n",
       "              ELEC.PA   55.61796   43.88082  -11.73714        0.63943   \n",
       "              GFC.PA    12.06264   26.76666   14.70403        2.22395   \n",
       "              LAT.PA    49.78475   43.88114   -5.90361        0.26857   \n",
       "              LI.PA      9.74307   17.22198    7.47891        2.38002   \n",
       "              RE.PA     19.22271   23.88377    4.66107        0.72411   \n",
       "              SAVE.PA    4.50842   30.13034   25.62192        0.41972   \n",
       "              TEP.PA    65.25452   17.32163  -47.93289        3.00736   \n",
       "\n",
       "                       pos_sma20_200  williamsr_14  perf_sma_50_5d  \\\n",
       "OPEN_DATETIME CODE                                                   \n",
       "1995-11-24    BN.PA          0.02145     -43.92655        -0.00423   \n",
       "              BOI.PA         0.14812    -100.00000        -0.00193   \n",
       "              CDI.PA         0.13538      -0.00000         0.00866   \n",
       "              ELEC.PA       -0.00688     -71.42857        -0.00965   \n",
       "              GFC.PA         0.24381     -61.60998         0.00240   \n",
       "              LAT.PA        -0.05338     -31.34328        -0.00242   \n",
       "              LI.PA         -0.03958     -41.56051        -0.00162   \n",
       "              RE.PA         -0.00245     -44.43796        -0.01148   \n",
       "              SAVE.PA        0.01606     -36.89894         0.00149   \n",
       "              TEP.PA        -0.03224    -100.00000        -0.00313   \n",
       "\n",
       "                       perf_sma_200_5d  \n",
       "OPEN_DATETIME CODE                      \n",
       "1995-11-24    BN.PA            0.00475  \n",
       "              BOI.PA           0.00800  \n",
       "              CDI.PA           0.00701  \n",
       "              ELEC.PA          0.00166  \n",
       "              GFC.PA           0.01102  \n",
       "              LAT.PA          -0.00235  \n",
       "              LI.PA           -0.00054  \n",
       "              RE.PA            0.00182  \n",
       "              SAVE.PA          0.00358  \n",
       "              TEP.PA          -0.00357  \n",
       "\n",
       "[10 rows x 53 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_work=df_work.droplevel('CODE') !!!!!!\n",
    "df_work.sort_index(inplace=True)\n",
    "df_work[10000:10010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_studied = \"lab_perf_50d\"\n",
    "algo_studied = \"LSTM_CLASS\"\n",
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "\n",
    "df_work_lab = indic.drop_indicators_not_selected(con=con_fwk, df_in=df_work, dts_name=dts_name, symbol=multi_symbol,label=lab_studied,algo=algo_studied)\n",
    "# print(df_work_lab.head(5))\n",
    "\n",
    "# move CODE to column to be able to slit the dataset\n",
    "df_work_lab['TICKER'] = df_work_lab.index.get_level_values('CODE')\n",
    "df_work_lab=df_work_lab.droplevel('CODE')\n",
    "\n",
    "df_split=sm.split_df_by_label_strat(\n",
    "    df_in=df_work_lab, list_label=[lab_studied], split_timeframe=\"M\",random_split=False,split_strat=(80,10,10))\n",
    "df_selected = df_split['df_'+lab_studied+'_train']\n",
    "df_valid = df_split['df_'+lab_studied+'_valid']\n",
    "df_confirm = df_split['df_'+lab_studied+'_confirm']\n",
    "df_selected.sort_index(inplace=True)\n",
    "df_valid.sort_index(inplace=True)\n",
    "df_confirm.sort_index(inplace=True)\n",
    "\n",
    "print(f\"selected: {df_selected.shape=} valid: {df_valid.shape=} confirm: {df_confirm.shape=}\")\n",
    "df_selected[10000:10010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        min      max\n",
      "lab_perf_50d_class                  \n",
      "0                  -0.83200 -0.07920\n",
      "1                  -0.07919 -0.00751\n",
      "2                  -0.00750  0.04948\n",
      "3                   0.04949  0.12576\n",
      "4                   0.12577  4.92040\n"
     ]
    }
   ],
   "source": [
    "label=lab_studied\n",
    "df_class=balance.add_class_by_lab_nb_lines(df_in=df_selected,str_label=lab_studied,nb_class=5,bool_replace_label=False)\n",
    "min_max_lab_by_class = df_class.groupby(label+'_class')[label].agg(['min', 'max'])\n",
    "print(min_max_lab_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=lab_studied\n",
    "df_class=balance.add_class_by_lab_nb_lines(df_in=df_selected,str_label=lab_studied,nb_class=5,bool_replace_label=True)\n",
    "df_class.sort_index(inplace=True)\n",
    "categ_50={0:[-1,-0.0792],1:[-0.0792,-0.0075],2:[-0.0075,0.04948],3:[0.04948,0.12576],4:[0.12576,5]}\n",
    "# categ_20={0:[-1,-0.0520],1:[-0.0520,-0.0089],2:[-0.0089,0.0235],3:[0.0235,0.0713],4:[0.0713,4]}\n",
    "df_class_val=balance.add_lab_by_class(df_in=df_valid,str_label=lab_studied, categ=categ_50,bool_replace_label=True) # categ\n",
    "df_class_val.sort_index(inplace=True)\n",
    "df_class_conf=balance.add_lab_by_class(df_in=df_confirm,str_label=lab_studied, categ=categ_50,bool_replace_label=True) # categ\n",
    "df_class_conf.sort_index(inplace=True)\n",
    "print(df_class.loc[:, label].dropna().iloc[[0, -1]])\n",
    "print(df_class_val.loc[:, label].dropna().iloc[[0, -1]])\n",
    "print(df_class_conf.loc[:, label].dropna().iloc[[0, -1]])\n",
    "# df_class_clean=df_class.drop(['OPEN','HIGH','LOW','CLOSE','VOLUME','lab_perf_125d','lab_perf_20d','lab_perf_50d'],axis=1)\n",
    "data = df_class[label]\n",
    "print(data.value_counts().sort_index())\n",
    "data_val = df_class_val[label]\n",
    "print(data_val.value_counts().sort_index())\n",
    "data_conf = df_class_conf[label]\n",
    "print(data_conf.value_counts().sort_index())\n",
    "df_class[10000:10010]\n",
    "# min_max_lab_by_class = df_class.groupby(label+'_class')[label].agg(['min', 'max'])\n",
    "# print(min_max_lab_by_class)\n",
    "\n",
    "# lab_perf_20d : train min nb rows 211000 validation 53000 confirm 55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SAVE DATASETS\n",
    "file_name=\"PARIS_TREND_1D_50D_V2\"\n",
    "df_class.round(5).to_csv(\n",
    "    PATH_DATA_DTS+file_name+SUFFIX_TRAIN, sep=\",\")\n",
    "df_class_val.round(5).to_csv(\n",
    "    PATH_DATA_DTS+file_name+SUFFIX_VAL, sep=\",\")\n",
    "df_class_conf.round(5).to_csv(\n",
    "    PATH_DATA_DTS+file_name+SUFFIX_CONF, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and save scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Projets\\\\Data\\\\DTS_FULL\\\\PARIS_TREND_1D_50D_V2_train_colab_lstm_norm_2405_scaler.save']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dts_name=\"PARIS_TREND_1D_50D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_50d\"\n",
    "\n",
    "df_class=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_TRAIN,sep=\",\",index_col=[\"OPEN_DATETIME\",\"TICKER\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class=df_class.dropna(subset=[label])\n",
    "df_class=df_class.sort_index()\n",
    "\n",
    "df_norm,norm_scaler= balance.normalize_df(df_in=df_class,str_label=label,tuple_ft_range=(-1,1))\n",
    "\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2405\"\n",
    "scaler_name=file_name+\"_scaler.save\"\n",
    "joblib.dump(norm_scaler,filename=PATH_DATA_DTS+scaler_name)\n",
    "\n",
    "# df_class_val=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_VAL,sep=\",\",index_col=[\"OPEN_DATETIME\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "# df_class_val.dropna(subset=[label], inplace=True)\n",
    "# df_class_val.sort_index(inplace=True)\n",
    "\n",
    "# list_feat = df_class.columns.values.tolist()\n",
    "# list_feat.remove(label)\n",
    "# X, y = sm.split_df_x_y(\n",
    "#     df_in=df_class, list_features=list_feat, str_label=label, drop_na=True)\n",
    "# nb_val=211000\n",
    "# method = RandomUnderSampler(sampling_strategy={0:nb_val,1:nb_val,2:nb_val,3:nb_val}) \n",
    "# df_x_train, col_y_train=  method.fit_resample(X, y)\n",
    "# print(col_y_train.value_counts().sort_index())\n",
    "\n",
    "# X, y = sm.split_df_x_y(\n",
    "#     df_in=df_class_val, list_features=list_feat, str_label=label, drop_na=True)\n",
    "# nb_val=53000\n",
    "# method = RandomUnderSampler(sampling_strategy={0:nb_val,1:nb_val,2:nb_val,3:nb_val}) # 53000 pour lab 20 et nn pour lab 50\n",
    "# df_x_val, col_y_val=  method.fit_resample(X, y)\n",
    "# print(col_y_val.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train et val df, normalize,  undersample  and preparation for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts_name=\"PARIS_TREND_1D_50D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_50d\"\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2405\"\n",
    "scaler_name=file_name+\"_scaler.save\"\n",
    "scaler=joblib.load(PATH_DATA_DTS+scaler_name)\n",
    "\n",
    "\n",
    "df_class=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_TRAIN,sep=\",\",index_col=[\"OPEN_DATETIME\",\"TICKER\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class=df_class.dropna(subset=[label])\n",
    "df_class=df_class.loc['1995-01-01':] # drop rows < 1995-01-01\n",
    "df_class=df_class.sort_index()\n",
    "df_class_val=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_VAL,sep=\",\",index_col=[\"OPEN_DATETIME\",\"TICKER\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class_val=df_class_val.dropna(subset=[label])\n",
    "df_class_val=df_class_val.sort_index()\n",
    "\n",
    "# normalize df_class and df_class_val\n",
    "df_class_train_norm=balance.normalize_df_scaler(df_in=df_class, str_label=label,scaler=scaler)\n",
    "df_class_val_norm=balance.normalize_df_scaler(df_in=df_class_val, str_label=label,scaler=scaler)\n",
    "\n",
    "print(f\"{df_class_train_norm.shape=} {df_class_val_norm.shape=}\")\n",
    "print(df_class_train_norm[10000:10005])\n",
    "# print type of index of df_class_train_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{type(df_class_train_norm.index[0])= } {type(df_class_train_norm.index[1])= }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "list_feat = df_class.columns.values.tolist()\n",
    "list_feat.remove(label)\n",
    "\n",
    "sequence_length = 10\n",
    "col_sequence = \"SEQUENCE\"\n",
    "\n",
    "# for each TICKER in index of df_class_train_norm, sort data with index and prepare sequences\n",
    "df_class_train_norm_sorted = df_class_train_norm.sort_index(level=['TICKER', 'OPEN_DATETIME'])\n",
    "df_class_val_norm_sorted = df_class_val_norm.sort_index(level=['TICKER', 'OPEN_DATETIME'])\n",
    "\n",
    "# Prepare sequences for each TICKER\n",
    "df_class_train_seq = pd.DataFrame()\n",
    "cnt=0\n",
    "for ticker in df_class_train_norm_sorted.index.get_level_values('TICKER').unique():\n",
    "    sub_df=df_class_train_norm_sorted[df_class_train_norm_sorted.index.get_level_values('TICKER') == ticker]\n",
    "    sub_df = sm.prepare_sequences_df(\n",
    "        df_in=sub_df, list_features=list_feat, sequence_length=sequence_length, str_new_col=col_sequence)\n",
    "    cnt+=1\n",
    "    if cnt%20==0:\n",
    "        print(f\"time {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} {cnt=} {ticker=}\")\n",
    "        gc.collect()\n",
    "    # if cnt==3:\n",
    "    #     break\n",
    "    \n",
    "# concatenate all TICKER data in the same df\n",
    "    df_class_train_seq = pd.concat([df_class_train_seq, sub_df])\n",
    "\n",
    "print((f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} train seq ok\"))\n",
    "\n",
    "df_class_val_seq = pd.DataFrame()\n",
    "cnt=0\n",
    "for ticker in df_class_val_norm_sorted.index.get_level_values('TICKER').unique():\n",
    "    sub_df=df_class_val_norm_sorted[df_class_val_norm_sorted.index.get_level_values('TICKER') == ticker]\n",
    "    sub_df = sm.prepare_sequences_df(\n",
    "        df_in=sub_df, list_features=list_feat, sequence_length=sequence_length, str_new_col=col_sequence)\n",
    "    cnt+=1\n",
    "    if cnt%20==0:\n",
    "        print(f\"time {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} {cnt=} {ticker=}\")\n",
    "        gc.collect()\n",
    "    # if cnt==3:\n",
    "    #     break\n",
    "    \n",
    "# concatenate all TICKER data in the same df\n",
    "    df_class_val_seq = pd.concat([df_class_val_seq, sub_df])\n",
    "\n",
    "print(f\"{df_class_train_seq.shape=} {df_class_val_seq.shape=}\")\n",
    "print(df_class_train_seq[10000:10005])\n",
    "\n",
    "# df_class_train_norm=sm.prepare_sequences_df(df_in=df_class_train_norm,list_features=list_feat,sequence_length=sequence_length,str_new_col=col_sequence)\n",
    "# df_class_val_norm=sm.prepare_sequences_df(df_in=df_class_val_norm,list_features=list_feat,sequence_length=sequence_length,str_new_col=col_sequence)\n",
    "\n",
    "# df_x_train, col_y_train = sm.split_df_x_y(\n",
    "#     df_in=df_class_train_norm, list_features=list_feat, str_label=label, drop_na=True)\n",
    "\n",
    "# df_x_val, col_y_val = sm.split_df_x_y(\n",
    "#     df_in=df_class_val_norm, list_features=list_feat, str_label=label, drop_na=True)\n",
    "\n",
    "\n",
    "\n",
    "# x_train=df_x_train.values\n",
    "# y_train=col_y_train.values\n",
    "# x_val=df_x_val.values\n",
    "# y_val=col_y_val.values\n",
    "# x_train_lstm,y_train_lstm=sm.prepare_sequences(x_train,y_train,sequence_length)\n",
    "# x_val_lstm,y_val_lstm=sm.prepare_sequences(x_val,y_val,sequence_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO put this in a function ??\n",
    "gc.collect()\n",
    "def format_float(x):\n",
    "    return '{:.5f}'.format(x) if x is not None else None\n",
    "\n",
    "def array_to_string(x):\n",
    "    return np.array2string(x,separator='_') if x is not None else None\n",
    "\n",
    "\n",
    "vfunc = np.vectorize(format_float) \n",
    "\n",
    "df_class_train_seq2=df_class_train_seq.copy()\n",
    "df_class_val_seq2=df_class_val_seq.copy()\n",
    "\n",
    "df_class_train_seq2[col_sequence] = df_class_train_seq2[col_sequence].apply(vfunc)\n",
    "df_class_val_seq2[col_sequence] = df_class_val_seq2[col_sequence].apply(vfunc)\n",
    "\n",
    "df_class_train_seq2[col_sequence] = df_class_train_seq2[col_sequence].apply(array_to_string)\n",
    "df_class_val_seq2[col_sequence] = df_class_val_seq2[col_sequence].apply(array_to_string)\n",
    "\n",
    "df_class_train_seq2.round(5).to_csv(\n",
    "    PATH_DATA_DTS+dts_name+\"_TRAIN_seq_6\", sep=\",\", float_format='%.5f')\n",
    "df_class_val_seq2.round(5).to_csv(\n",
    "    PATH_DATA_DTS+dts_name+\"_VAL_seq_6\", sep=\",\", float_format='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START HERE TO LOAD DATASETS WITH SEQUENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts_name=\"PARIS_TREND_1D_50D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_50d\"\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2405\"\n",
    "col_sequence = \"SEQUENCE\"\n",
    "df_class_train_csv=pd.read_csv(PATH_DATA_DTS+dts_name+\"_TRAIN_seq_6.zip\",sep=\",\",index_col=[\"TICKER\",\"OPEN_DATETIME\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class_train_csv=df_class_train_csv.dropna(subset=[col_sequence])\n",
    "df_class_train_csv=df_class_train_csv.sort_index()\n",
    "df_class_val_csv=pd.read_csv(PATH_DATA_DTS+dts_name+\"_VAL_seq_6.zip\",sep=\",\",index_col=[\"TICKER\",\"OPEN_DATETIME\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class_val_csv=df_class_val_csv.dropna(subset=[col_sequence])\n",
    "df_class_val_csv=df_class_val_csv.sort_index()\n",
    "gc.collect()\n",
    "# keep only index, label and sequence\n",
    "df_class_train_csv=df_class_train_csv[[label,col_sequence]]\n",
    "df_class_val_csv=df_class_val_csv[[label,col_sequence]]\n",
    "\n",
    "df_class_train_csv[col_sequence] = df_class_train_csv[col_sequence].str.replace(\"_\", \",\").apply(ast.literal_eval)\n",
    "df_class_train_csv[col_sequence]  = df_class_train_csv[col_sequence] .apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "df_class_val_csv[col_sequence] = df_class_val_csv[col_sequence].str.replace(\"_\", \",\").apply(ast.literal_eval)\n",
    "df_class_val_csv[col_sequence]  = df_class_val_csv[col_sequence] .apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "\n",
    "print(f\"{df_class_train_csv.shape=}\")\n",
    "print(df_class_train_csv[1015:1020])\n",
    "print(f\"{df_class_val_csv.shape=}\")\n",
    "print(df_class_val_csv[1015:1020])\n",
    "\n",
    "# decision is made between market sessions so we have shift the label of 1 day for each ticker\n",
    "df_class_train_csv[label] = df_class_train_csv.groupby(level='TICKER')[label].shift(1)\n",
    "df_class_train_csv=df_class_train_csv.dropna(subset=[label])\n",
    "df_class_val_csv[label] = df_class_val_csv.groupby(level='TICKER')[label].shift(1)\n",
    "df_class_val_csv=df_class_val_csv.dropna(subset=[label])\n",
    "print(df_class_train_csv[1014:1019])\n",
    "print(df_class_val_csv[1014:1019])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_train_csv.to_pickle(PATH_DATA_DTS+dts_name+\"_TRAIN_seq_6.pckl\")\n",
    "df_class_val_csv.to_pickle(PATH_DATA_DTS+dts_name+\"_VAL_seq_6.pckl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START HERE TO DIRECTLY LOAD THE PICKLE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      lab_perf_50d  \\\n",
      "TICKER OPEN_DATETIME                 \n",
      "AB.PA  2015-02-11              0.0   \n",
      "       2015-02-12              1.0   \n",
      "       2015-02-13              0.0   \n",
      "       2015-02-16              0.0   \n",
      "       2015-02-17              0.0   \n",
      "\n",
      "                                                               SEQUENCE  \n",
      "TICKER OPEN_DATETIME                                                     \n",
      "AB.PA  2015-02-11     [[-0.39987, -0.49388, -0.45321, 0.33119, 0.245...  \n",
      "       2015-02-12     [[-0.40479, -0.49565, -0.45233, 0.33764, 0.256...  \n",
      "       2015-02-13     [[-0.39282, -0.4837, -0.43821, 0.39571, 0.2664...  \n",
      "       2015-02-16     [[-0.35487, -0.4481, -0.40128, 0.51362, 0.2836...  \n",
      "       2015-02-17     [[-0.35342, -0.4441, -0.39371, 0.5362, 0.29874...  \n"
     ]
    }
   ],
   "source": [
    "dts_name=\"PARIS_TREND_1D_50D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_50d\"\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2405\"\n",
    "col_sequence = \"SEQUENCE\"\n",
    "\n",
    "df_class_train_csv=pd.read_pickle(PATH_DATA_DTS+dts_name+\"_TRAIN_seq_6.pckl\")  #the train will be split in train + val\n",
    "df_class_test_csv=pd.read_pickle(PATH_DATA_DTS+dts_name+\"_VAL_seq_6.pckl\") #the val is finally used as a test dataset\n",
    "print(df_class_train_csv[1014:1019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       lab_perf_50d  \\\n",
      "OPEN_DATETIME TICKER                  \n",
      "1995-05-18    LI.PA             1.0   \n",
      "              RE.PA             4.0   \n",
      "              SAVE.PA           2.0   \n",
      "              TEP.PA            4.0   \n",
      "              VIRP.PA           3.0   \n",
      "\n",
      "                                                                SEQUENCE  \n",
      "OPEN_DATETIME TICKER                                                      \n",
      "1995-05-18    LI.PA    [[-0.44468, -0.57928, -0.59788, 0.1422, 0.0469...  \n",
      "              RE.PA    [[-0.45272, -0.55703, -0.62491, 0.13047, 0.225...  \n",
      "              SAVE.PA  [[-0.44879, -0.57938, -0.62212, 0.04232, -0.00...  \n",
      "              TEP.PA   [[-0.47529, -0.6054, -0.6432, -0.20304, -0.006...  \n",
      "              VIRP.PA  [[-0.44257, -0.56061, -0.74347, 0.358, 0.35239...  \n"
     ]
    }
   ],
   "source": [
    "# df_class_train_csv split into train and val with 0.75/0.25 by open datetime using sm.split_df_by_label_strat\n",
    "df_class_train_csv.reset_index(level='TICKER',inplace=True)\n",
    "\n",
    "df_split=sm.split_df_by_label_strat(\n",
    "    df_in=df_class_train_csv, list_label=[label], split_timeframe=\"D\",random_split=False,split_strat=(80,20,0))\n",
    "df_train_split=df_split['df_'+label+'_train']\n",
    "df_val_split=df_split['df_'+label+'_valid']\n",
    "\n",
    "df_train_split.set_index('TICKER',append=True,inplace=True)\n",
    "df_val_split.set_index('TICKER',append=True,inplace=True)\n",
    "df_train_split.sort_index(inplace=True)\n",
    "df_val_split.sort_index(inplace=True)\n",
    "print(df_train_split[1014:1019])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab_perf_50d\n",
      "0.0    126186\n",
      "1.0    109342\n",
      "2.0    108529\n",
      "3.0    106828\n",
      "4.0    112404\n",
      "Name: count, dtype: int64\n",
      "lab_perf_50d\n",
      "0.0    39851\n",
      "1.0    56438\n",
      "2.0    57376\n",
      "3.0    59085\n",
      "4.0    53401\n",
      "Name: count, dtype: int64\n",
      "lab_perf_50d\n",
      "0.0    56291\n",
      "1.0    48163\n",
      "2.0    44986\n",
      "3.0    40583\n",
      "4.0    39030\n",
      "Name: count, dtype: int64\n",
      "lab_perf_50d\n",
      "0.0    106000\n",
      "1.0    106000\n",
      "2.0    106000\n",
      "3.0    106000\n",
      "4.0    106000\n",
      "Name: count, dtype: int64\n",
      "lab_perf_50d\n",
      "0.0    39000\n",
      "1.0    39000\n",
      "2.0    39000\n",
      "3.0    39000\n",
      "4.0    39000\n",
      "Name: count, dtype: int64\n",
      "lab_perf_50d\n",
      "0.0    39000\n",
      "1.0    39000\n",
      "2.0    39000\n",
      "3.0    39000\n",
      "4.0    39000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benoit\\AppData\\Local\\Temp\\ipykernel_19660\\1437885360.py:18: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x_train_tensor = torch.as_tensor(df_class_train_under[col_sequence], dtype=torch.float)\n",
      "C:\\Users\\Benoit\\AppData\\Local\\Temp\\ipykernel_19660\\1437885360.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  x_train_tensor = torch.as_tensor(df_class_train_under[col_sequence], dtype=torch.float)\n",
      "C:\\Users\\Benoit\\AppData\\Local\\Temp\\ipykernel_19660\\1437885360.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_tensor = torch.tensor(df_class_train_under[label], dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader.dataset.tensors[0].shape=torch.Size([530000, 10, 27]) val_loader.dataset.tensors[0].shape=torch.Size([195000, 10, 27]) test_loader.dataset.tensors[0].shape=torch.Size([195000, 10, 27])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benoit\\AppData\\Local\\Temp\\ipykernel_19660\\1437885360.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x_val_tensor = torch.as_tensor(df_class_val_under[col_sequence], dtype=torch.float)\n",
      "C:\\Users\\Benoit\\AppData\\Local\\Temp\\ipykernel_19660\\1437885360.py:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_val_tensor = torch.tensor(df_class_val_under[label], dtype=torch.int64)\n",
      "C:\\Users\\Benoit\\AppData\\Local\\Temp\\ipykernel_19660\\1437885360.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x_test_tensor = torch.as_tensor(df_class_test_under[col_sequence], dtype=torch.float)\n",
      "C:\\Users\\Benoit\\AppData\\Local\\Temp\\ipykernel_19660\\1437885360.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_tensor = torch.tensor(df_class_test_under[label], dtype=torch.int64)\n",
      "g:\\Python\\MarketDataEnrichment\\.env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557: UserWarning: This DataLoader will create 7 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# print(f\"{df_class_train_csv.shape=} {df_class_val_csv.shape=}\")\n",
    "print(df_train_split[label].value_counts().sort_index()) # undersampling at 109200\n",
    "print(df_val_split[label].value_counts().sort_index()) # undersampling at 43900\n",
    "print(df_class_test_csv[label].value_counts().sort_index()) # undersampling at 41500\n",
    "\n",
    "nb_val=106000#30000 #109200\n",
    "df_class_train_under=balance.class_custom_undersampler(df_train_split,label,nb_val) # undersampling todo\n",
    "\n",
    "nb_val=39000#5000 #41500\n",
    "df_class_val_under=balance.class_custom_undersampler(df_val_split,label,nb_val)\n",
    "df_class_test_under=balance.class_custom_undersampler(df_class_test_csv,label,nb_val)\n",
    "\n",
    "print(df_class_train_under[label].value_counts().sort_index()) \n",
    "print(df_class_val_under[label].value_counts().sort_index()) \n",
    "print(df_class_test_under[label].value_counts().sort_index()) \n",
    "\n",
    "\n",
    "x_train_tensor = torch.as_tensor(df_class_train_under[col_sequence], dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(df_class_train_under[label], dtype=torch.int64)\n",
    "\n",
    "# x_val_tensor = torch.tensor(df_class_val_under['col_sequence_3'], dtype=torch.float)\n",
    "x_val_tensor = torch.as_tensor(df_class_val_under[col_sequence], dtype=torch.float)\n",
    "y_val_tensor = torch.tensor(df_class_val_under[label], dtype=torch.int64)\n",
    "\n",
    "x_test_tensor = torch.as_tensor(df_class_test_under[col_sequence], dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(df_class_test_under[label], dtype=torch.int64)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# initiate a pytorch randomsampler for train data\n",
    "# train_sampler = RandomSampler(train_dataset,num_samples=100000,replacement=True)\n",
    "\n",
    "batch_size=8192#8192\n",
    "num_workers=7\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,drop_last=True,num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,drop_last=True,num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,drop_last=True,num_workers=num_workers)\n",
    " \n",
    "print(f\"{train_loader.dataset.tensors[0].shape=} {val_loader.dataset.tensors[0].shape=} {test_loader.dataset.tensors[0].shape=}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{train_loader.dataset.tensors[0].shape=} {val_loader.dataset.tensors[0].shape=} {test_loader.dataset.tensors[0].shape=}\")\n",
    "#print next(iter(train_loader))\n",
    "pprint(next(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation (Copy from the Tensorflow notebook), not tested here !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_train = df_x_train.corr()\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "corr_train.replace(1,0,inplace=True)\n",
    "corr_train=corr_train.applymap(lambda x : None if x< 0.7 and x>-0.7 else x)\n",
    "corr_train.dropna(axis=0,how='all',inplace=True)\n",
    "corr_train.dropna(axis=1,how='all',inplace=True)\n",
    "\n",
    "# corr_train_check=corr_train[corr_train >0.8]\n",
    "corr_train_check=corr_train\n",
    "sns.heatmap(corr_train_check, annot=False, cmap='coolwarm', vmin=-1, vmax=1, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_class, x='pos_sma200', y='pos_top50', hue='lab_perf_50d', palette='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "###############################################\n",
    "###### REFACTO USING PYTORCH LIGHTNING ########\n",
    "###############################################\n",
    "\n",
    "# Define LSTM model\n",
    "class DynamicLSTMModel(pl.LightningModule):\n",
    "    def __init__(self, layer_configs, lr, criterion):\n",
    "        super(DynamicLSTMModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for config in layer_configs:\n",
    "            # print(f\"{config=}\")\n",
    "            if config['type'] == 'LSTM':\n",
    "                layer = nn.LSTM(input_size=config['input_dim'], hidden_size=config['hidden_dim'], num_layers=config['num_layers'],\n",
    "                                batch_first=True, dropout=config['dropout'], bidirectional=config['bidirectional'])\n",
    "            elif config['type'] == 'Linear':\n",
    "                layer = nn.Linear(config['input_dim'], config['output_dim'])\n",
    "            elif config['type'] == 'Softmax':\n",
    "                layer = nn.Softmax(dim=config['dim'])\n",
    "            elif config['type'] == 'ReLU':\n",
    "                layer = nn.ReLU()\n",
    "            elif config['type'] == 'Sigmoid':\n",
    "                layer = nn.Sigmoid()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported layer type: {config['type']}\")\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.criterion = criterion\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.LSTM):\n",
    "                # LSTM layers require special handling for initial states\n",
    "                batch_size = x.size(0)\n",
    "                hidden_dim = layer.hidden_size\n",
    "                num_layers = layer.num_layers * 2 if layer.bidirectional else layer.num_layers\n",
    "                h0 = torch.zeros(num_layers, batch_size,\n",
    "                                 hidden_dim).to(x.device)\n",
    "                c0 = torch.zeros(num_layers, batch_size,\n",
    "                                 hidden_dim).to(x.device)\n",
    "                x, _ = layer(x, (h0, c0))\n",
    "                x = x[:, -1, :]\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        _, predicted = torch.max(y_hat.data, 1)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        correct = (predicted == y).sum().item()\n",
    "        total = len(y)\n",
    "        self.log(\"train_loss\", loss, on_step=True,\n",
    "                on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc\", correct/total, on_step=True,\n",
    "                on_epoch=True, prog_bar=True, logger=True)\n",
    "        output = {\"loss\": loss, \"train_loss\": loss,\n",
    "                  \"train_correct\": correct, \"train_total\": total}\n",
    "        return output\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        _, predicted = torch.max(y_hat.data, 1)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        correct = (predicted == y).sum().item()\n",
    "        total = len(y)\n",
    "        # output=f\"val_loss: {loss}, val_correct: {correct}, val_total: {y.size(0)}\"\n",
    "        output = {\"loss\": loss, \"val_loss\": loss,\n",
    "                  \"val_correct\": correct, \"val_total\": total}\n",
    "        # self.log(output)\n",
    "        self.log(\"val_loss\", loss, on_step=True,\n",
    "                on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", correct/total, on_step=True,\n",
    "                on_epoch=True, prog_bar=True, logger=True)\n",
    "        return output\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        _, predicted = torch.max(y_hat.data, 1)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        correct = (predicted == y).sum().item()\n",
    "        total = len(y)\n",
    "        # output=f\"val_loss: {loss}, val_correct: {correct}, val_total: {y.size(0)}\"\n",
    "        output = {\"loss\": loss, \"test_loss\": loss, \"test_correct\": correct,\n",
    "                  \"test_total\": total, \"test_acc\": correct/total}\n",
    "        # self.log(output)\n",
    "        self.log(\"test_loss\", loss, on_step=True,\n",
    "                on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_acc\", correct/total, on_step=True,\n",
    "                on_epoch=True, prog_bar=True, logger=True)\n",
    "        return output\n",
    "\n",
    "    # def test_epoch_end(self, outputs):\n",
    "    #     avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "    #     self.log('test_loss_epoch', avg_loss)\n",
    "\n",
    "    # def on_validation_epoch_end(self, outputs):\n",
    "    #     avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "    #     total_correct = sum(x['val_correct'] for x in outputs)\n",
    "    #     total = sum(x['val_total'] for x in outputs)\n",
    "    #     tensorboard_logs = {'val_loss': avg_loss}\n",
    "    #     return {'val_loss': avg_loss, 'progress_bar': tensorboard_logs, 'val_acc': total_correct / total}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=self.lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim = x_train_tensor.shape[2]\n",
    "num_classes = 5\n",
    "epochs = 100  # 350\n",
    "suffix = \"lstm_pytorch_v1\"\n",
    "tb_directory = \"tb_logs\"\n",
    "debug = False\n",
    "verbose=False\n",
    "patience = 5\n",
    "\n",
    "obj_acc = 0.245\n",
    "cpt_param = 0\n",
    "try_limit = 10\n",
    "pct_check_class = 0.28  # check if at least n% of the validation set per class\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "len_val = x_val_tensor.shape[0]\n",
    "check_class_limit = (len_val/num_classes)*pct_check_class\n",
    "check_class = False  # check if at least obj_acc accuracy per class\n",
    "\n",
    "list_param_valid = [\n",
    "    {'layer_configs': [\n",
    "        {'type': 'Linear', 'input_dim': input_dim, 'output_dim': input_dim},\n",
    "        {'type': 'LSTM', 'input_dim': input_dim, 'hidden_dim': 32,         'num_layers': 1, 'dropout': 0.2, 'bidirectional': True},\n",
    "        # Note: LSTM bidirectional output is doubled\n",
    "        {'type': 'Linear', 'input_dim': 32 * 2, 'output_dim': num_classes},\n",
    "        {'type': 'Softmax', 'dim': 1}\n",
    "    ], 'optimizer__lr': 0.01, 'lab': 'lstm_32_1'},\n",
    "    # {'layer_configs': [\n",
    "    #     {'type': 'Linear', 'input_dim': input_dim, 'output_dim': input_dim},\n",
    "    #     {'type': 'LSTM', 'input_dim': input_dim, 'hidden_dim': 32,         'num_layers': 2, 'dropout': 0.2, 'bidirectional': True},\n",
    "    #     # Note: LSTM bidirectional output is doubled\n",
    "    #     {'type': 'Linear', 'input_dim': 32 * 2, 'output_dim': num_classes},\n",
    "    #     {'type': 'Softmax', 'dim': 1}\n",
    "    # ], 'optimizer__lr': 0.01, 'lab': 'lstm_32_2'},\n",
    "    # {'layer_configs': [\n",
    "    #     {'type': 'Linear', 'input_dim': input_dim, 'output_dim': input_dim},\n",
    "    #     {'type': 'LSTM', 'input_dim': input_dim, 'hidden_dim': 64,         'num_layers': 1, 'dropout': 0.2, 'bidirectional': True},\n",
    "    #     # Note: LSTM bidirectional output is doubled\n",
    "    #     {'type': 'Linear', 'input_dim': 64 * 2, 'output_dim': num_classes},\n",
    "    #     {'type': 'Softmax', 'dim': 1}\n",
    "    # ], 'optimizer__lr': 0.01, 'lab': 'lstm_64_1'},\n",
    "    {'layer_configs': [\n",
    "        {'type': 'Linear', 'input_dim': input_dim, 'output_dim': input_dim},\n",
    "        {'type': 'LSTM', 'input_dim': input_dim, 'hidden_dim': 48,         'num_layers': 1, 'dropout': 0.2, 'bidirectional': True},\n",
    "        # Note: LSTM bidirectional output is doubled\n",
    "        {'type': 'Linear', 'input_dim': 48 * 2, 'output_dim': num_classes},\n",
    "        {'type': 'Softmax', 'dim': 1}\n",
    "    ], 'optimizer__lr': 0.01, 'lab': 'lstm_48_1'},\n",
    "    # {'layer_configs': [\n",
    "    #     {'type': 'Linear', 'input_dim': input_dim, 'output_dim': input_dim},\n",
    "    #     {'type': 'LSTM', 'input_dim': input_dim, 'hidden_dim': 48,         'num_layers': 2, 'dropout': 0.2, 'bidirectional': True},\n",
    "    #     # Note: LSTM bidirectional output is doubled\n",
    "    #     {'type': 'Linear', 'input_dim': 48 * 2, 'output_dim': num_classes},\n",
    "    #     {'type': 'Softmax', 'dim': 1}\n",
    "    # ], 'optimizer__lr': 0.01, 'lab': 'lstm_48_2'},\n",
    "    # {'layer_configs': [\n",
    "    #     {'type': 'Linear', 'input_dim': input_dim, 'output_dim': input_dim},\n",
    "    #     {'type': 'LSTM', 'input_dim': input_dim, 'hidden_dim': 64,         'num_layers': 2, 'dropout': 0.2, 'bidirectional': True},\n",
    "    #     # Note: LSTM bidirectional output is doubled\n",
    "    #     {'type': 'Linear', 'input_dim': 64 * 2, 'output_dim': num_classes},\n",
    "    #     {'type': 'Softmax', 'dim': 1}\n",
    "    # ], 'optimizer__lr': 0.01, 'lab': 'lstm_64_2'},\n",
    "    # {'layer_configs': [\n",
    "    #     {'type': 'Linear', 'input_dim': input_dim, 'output_dim': input_dim},\n",
    "    #     {'type': 'LSTM', 'input_dim': input_dim, 'hidden_dim': 80,         'num_layers': 1, 'dropout': 0.2, 'bidirectional': True},\n",
    "    #     # Note: LSTM bidirectional output is doubled\n",
    "    #     {'type': 'Linear', 'input_dim': 80 * 2 , 'output_dim': num_classes},\n",
    "    #     {'type': 'Softmax', 'dim': 1}\n",
    "    # ], 'optimizer__lr': 0.01, 'lab': 'lstm_80_1'},\n",
    "    # {'layer_configs': [\n",
    "    #     {'type': 'Linear', 'input_dim': input_dim, 'output_dim': input_dim},\n",
    "    #     {'type': 'LSTM', 'input_dim': input_dim, 'hidden_dim': 24,         'num_layers': 1, 'dropout': 0.2, 'bidirectional': True},\n",
    "    #     # Note: LSTM bidirectional output is doubled\n",
    "    #     {'type': 'Linear', 'input_dim': 24 * 2 , 'output_dim': num_classes},\n",
    "    #     {'type': 'Softmax', 'dim': 1}\n",
    "    # ], 'optimizer__lr': 0.01, 'lab': 'lstm_24_1'},\n",
    "    # {'layer_configs': [\n",
    "    #     {'type': 'Linear', 'input_dim': input_dim, 'output_dim': input_dim},\n",
    "    #     {'type': 'LSTM', 'input_dim': input_dim, 'hidden_dim': 24,         'num_layers': 2, 'dropout': 0.2, 'bidirectional': True},\n",
    "    #     # Note: LSTM bidirectional output is doubled\n",
    "    #     {'type': 'Linear', 'input_dim': 24 * 2 , 'output_dim': num_classes},\n",
    "    #     {'type': 'Softmax', 'dim': 1}\n",
    "    # ], 'optimizer__lr': 0.01, 'lab': 'lstm_24_2'},\n",
    "\n",
    "]\n",
    "cpt_param = 0\n",
    "while (cpt_param < len(list_param_valid)):#len(list_param_valid) ):  # loop for parameters #and check_class == False1\n",
    "    gc.collect()\n",
    "    param_valid = list_param_valid[cpt_param]  # select the current param line\n",
    "    print(f\"{param_valid=}\")\n",
    "    cpt = 0\n",
    "    \n",
    "\n",
    "    while (cpt < try_limit ):  # loop for train models until good results # and check_class == False\n",
    "        cpt += 1\n",
    "\n",
    "        suffix_comp = f\"{suffix}_{param_valid['lab']}_v{str(cpt)}\" \n",
    "        filename_tmp_model = dts_name+\"_\"+suffix_comp+\".pth\"\n",
    "\n",
    "        model = DynamicLSTMModel(layer_configs=param_valid['layer_configs'], lr=param_valid['optimizer__lr'], criterion=criterion)\n",
    "\n",
    "        if cpt == 1 and debug:\n",
    "            print(model)\n",
    "            print(len(list(model.parameters())))\n",
    "            for i in range(len(list(model.parameters()))):\n",
    "                print(list(model.parameters())[i].size())\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=PATH_DATA+\"\\\\Models\\\\\",  # Specify the directory to save the model\n",
    "            # Specify the filename format\n",
    "            filename=f\"{dts_name}_{suffix}_{datetime.now().strftime('%Y%m%d')}_{cpt_param}_{cpt}\",\n",
    "            save_top_k=1,  # Save only the top k models according to the monitored quantity\n",
    "            verbose=verbose,\n",
    "            monitor='val_loss',  # Specify the metric to monitor\n",
    "            mode='min',  # Mode can be either 'min', 'max', or 'auto'\n",
    "            save_last=False  # Optionally, you can choose to save the last model\n",
    "        )\n",
    "\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\", min_delta=0.001, patience=patience, verbose=verbose, mode=\"min\")\n",
    "        logger = TensorBoardLogger(tb_directory, name=\"my_model\")\n",
    "        logging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.WARNING)\n",
    "        trainer = pl.Trainer(max_epochs=epochs, callbacks=[\n",
    "                             early_stop_callback, checkpoint_callback], logger=logger)\n",
    "\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "        writer = SummaryWriter(log_dir=tb_directory+\"/model_summary\")\n",
    "        model_summary = str(model).replace(\n",
    "            '\\n', '<br/>').replace(' ', '&nbsp;')\n",
    "        writer.add_text(\"model_v\"+str(logger.version), model_summary)\n",
    "        writer.close()\n",
    "\n",
    "        # trainer.test(dataloaders=test_loader)\n",
    "        print(f\"{checkpoint_callback.best_model_path=}\")\n",
    "        best_model = DynamicLSTMModel.load_from_checkpoint(\n",
    "            checkpoint_callback.best_model_path)\n",
    "        result = trainer.test(best_model, dataloaders=test_loader)\n",
    "        # print(f\"{result[0]=}\")\n",
    "        # print(\n",
    "        #     f\"Optim {cpt=} {checkpoint_callback.best_model_path=} {result[0]['test_acc_epoch']=}\")\n",
    "\n",
    "        if result[0]['test_acc_epoch'] > obj_acc:\n",
    "            # calculate the confusion matrix\n",
    "            y_pred = best_model(x_val_tensor)\n",
    "            _, y_pred_classes = torch.max(y_pred, 1)\n",
    "            confusion = metrics.confusion_matrix(y_val_tensor, y_pred_classes)\n",
    "\n",
    "            print(confusion)\n",
    "\n",
    "            check_class = True\n",
    "\n",
    "            for i in range(num_classes):\n",
    "                nb_lab = sum(y_pred_classes == i)\n",
    "                if nb_lab < check_class_limit:\n",
    "                    check_class = False\n",
    "                    print(\n",
    "                        f\"Check class {i=} {nb_lab=} {check_class=} {check_class_limit=}\")\n",
    "\n",
    "            # check saved model, load to check it's OK\n",
    "            if check_class:\n",
    "                torch.save(best_model, filename_tmp_model)\n",
    "                saved_model = torch.load(filename_tmp_model, weights_only=False)\n",
    "                # best_model_state = torch.deepcopy(model.state_dict())\n",
    "                # torch.save(best_model.state_dict(), filename_tmp_model)\n",
    "                # saved_model = DynamicLSTMModel()\n",
    "                # saved_model.load_state_dict(torch.load(filename_tmp_model), weights_only=True)\n",
    "                saved_model.eval()\n",
    "                y_pred = saved_model(x_val_tensor)\n",
    "                _, y_pred_classes = torch.max(y_pred, 1)\n",
    "                confusion = metrics.confusion_matrix(\n",
    "                    y_val_tensor, y_pred_classes)\n",
    "                print(confusion)\n",
    "\n",
    "    if cpt >= try_limit:\n",
    "        cpt_param += 1\n",
    "        print(f\"Optim fail {cpt=} param suivant {cpt_param=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9414  3238 16817  4617  4914]\n",
      " [ 4767  3486 25046  3732  1969]\n",
      " [ 4046  3682 25990  3572  1710]\n",
      " [ 4420  4123 24973  3742  1742]\n",
      " [ 6861  4471 19464  4418  3786]]\n"
     ]
    }
   ],
   "source": [
    "# file_model=\"PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_lstm_48_1_v1.pth\" # >> 0.278\n",
    "# file_model=\"PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_lstm_32_1_v8.pth\" # >> 0.283\n",
    "file_model=\"PARIS_TREND_1D_50D_V2_lstm_pytorch_v1_lstm_48_1_v5.pth\" # >> 23.8\n",
    "saved_model = torch.load(file_model, weights_only=False)\n",
    "saved_model.eval()\n",
    "y_pred = saved_model(x_val_tensor)\n",
    "_, y_pred_classes = torch.max(y_pred, 1)\n",
    "confusion = metrics.confusion_matrix(\n",
    "    y_val_tensor, y_pred_classes)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check with the conf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_class_conf_norm.shape=(244596, 28)\n",
      "                        pos_sma20  pos_sma50  pos_sma200     rsi14  \\\n",
      "OPEN_DATETIME TICKER                                                 \n",
      "2020-08-20    EIFF.PA   -0.468067  -0.608220   -0.661174 -0.224206   \n",
      "              EKI.PA    -0.476917  -0.611072   -0.605117 -0.161947   \n",
      "              EL.PA     -0.473384  -0.617712   -0.657188 -0.207866   \n",
      "              ELEC.PA   -0.450007  -0.575896   -0.575319  0.150287   \n",
      "              ELIOR.PA  -0.459591  -0.620949   -0.786099 -0.104456   \n",
      "\n",
      "                        sma20_rsi14    ret_5d  pos_top20  pos_top50  \\\n",
      "OPEN_DATETIME TICKER                                                  \n",
      "2020-08-20    EIFF.PA     -0.027937 -0.560094   0.908621   0.728005   \n",
      "              EKI.PA      -0.027467 -0.562470   0.815817   0.773077   \n",
      "              EL.PA       -0.159548 -0.554497   0.824120   0.766178   \n",
      "              ELEC.PA      0.177041 -0.543268   0.977868   0.978726   \n",
      "              ELIOR.PA    -0.105494 -0.576762   0.818468   0.523149   \n",
      "\n",
      "                        pos_bot20  pos_bot50  ...   cmf_20     adx14  \\\n",
      "OPEN_DATETIME TICKER                          ...                      \n",
      "2020-08-20    EIFF.PA   -0.990816  -0.992802  ...  0.07656 -0.693080   \n",
      "              EKI.PA    -1.000000  -0.988836  ...  0.08635 -0.699742   \n",
      "              EL.PA     -0.998796  -0.999057  ... -0.17174 -0.708664   \n",
      "              ELEC.PA   -0.988053  -0.974732  ...  0.43956  0.254110   \n",
      "              ELIOR.PA  -0.970715  -0.977049  ...  0.20067 -0.743206   \n",
      "\n",
      "                        adx14_neg  adx14_pos  adx14_dif  pos_avg_vol14  \\\n",
      "OPEN_DATETIME TICKER                                                     \n",
      "2020-08-20    EIFF.PA   -0.566054  -0.664443  -0.049195      -0.912542   \n",
      "              EKI.PA    -0.583001  -0.696549  -0.056774      -0.933283   \n",
      "              EL.PA     -0.485831  -0.668262  -0.091215      -0.920318   \n",
      "              ELEC.PA   -0.899318  -0.354984   0.272167      -0.994255   \n",
      "              ELIOR.PA  -0.540788  -0.616884  -0.038048      -0.945628   \n",
      "\n",
      "                        pos_sma20_200  perf_sma_50_5d  perf_sma_200_5d  \\\n",
      "OPEN_DATETIME TICKER                                                     \n",
      "2020-08-20    EIFF.PA       -0.503910       -0.066517        -0.228329   \n",
      "              EKI.PA        -0.407685       -0.026819        -0.110343   \n",
      "              EL.PA         -0.493029       -0.037971        -0.209590   \n",
      "              ELEC.PA       -0.388701        0.009817        -0.127931   \n",
      "              ELIOR.PA      -0.703469       -0.118585        -0.345373   \n",
      "\n",
      "                        lab_perf_50d  \n",
      "OPEN_DATETIME TICKER                  \n",
      "2020-08-20    EIFF.PA            0.0  \n",
      "              EKI.PA             2.0  \n",
      "              EL.PA              1.0  \n",
      "              ELEC.PA            2.0  \n",
      "              ELIOR.PA           0.0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "dts_name=\"PARIS_TREND_1D_50D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_50d\"\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2405\"\n",
    "scaler_name=file_name+\"_scaler.save\"\n",
    "scaler=joblib.load(PATH_DATA_DTS+scaler_name)\n",
    "\n",
    "\n",
    "df_class_conf=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_CONF,sep=\",\",index_col=[\"OPEN_DATETIME\",\"TICKER\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class_conf=df_class_conf.dropna(subset=[label])\n",
    "df_class_conf=df_class_conf.sort_index()\n",
    "\n",
    "# normalize df_class and df_class_val\n",
    "df_class_conf_norm=balance.normalize_df_scaler(df_in=df_class_conf, str_label=label,scaler=scaler)\n",
    "\n",
    "print(f\"{df_class_conf_norm.shape=}\")\n",
    "print(df_class_conf_norm[10000:10005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 2024-09-27 12:36:26 cnt=20 ticker='ALCRB.PA'\n",
      "time 2024-09-27 12:38:23 cnt=40 ticker='ALHRG.PA'\n",
      "time 2024-09-27 12:39:41 cnt=60 ticker='ALVU.PA'\n",
      "time 2024-09-27 12:40:59 cnt=80 ticker='BLV.PA'\n",
      "time 2024-09-27 12:42:24 cnt=100 ticker='CGM.PA'\n",
      "time 2024-09-27 12:43:44 cnt=120 ticker='DIM.PA'\n",
      "time 2024-09-27 12:45:06 cnt=140 ticker='EUR.PA'\n",
      "time 2024-09-27 12:46:25 cnt=160 ticker='GNE.PA'\n",
      "time 2024-09-27 12:47:47 cnt=180 ticker='JBOG.PA'\n",
      "time 2024-09-27 12:49:07 cnt=200 ticker='MERY.PA'\n",
      "time 2024-09-27 12:50:30 cnt=220 ticker='ODET.PA'\n",
      "time 2024-09-27 12:51:49 cnt=240 ticker='RCO.PA'\n",
      "time 2024-09-27 12:53:16 cnt=260 ticker='SGO.PA'\n",
      "time 2024-09-27 12:54:34 cnt=280 ticker='TRI.PA'\n",
      "df_class_conf_seq.shape=(244596, 29)\n",
      "                        pos_sma20  pos_sma50  pos_sma200     rsi14  \\\n",
      "OPEN_DATETIME TICKER                                                 \n",
      "2021-05-24    ALBFR.PA  -0.474132  -0.567020   -0.463967  0.006233   \n",
      "2021-05-25    ALBFR.PA  -0.469985  -0.567654   -0.463980  0.014897   \n",
      "2021-05-26    ALBFR.PA  -0.454048  -0.557804   -0.453243  0.075652   \n",
      "2021-05-27    ALBFR.PA  -0.461728  -0.569091   -0.464094  0.026937   \n",
      "2021-05-28    ALBFR.PA  -0.460021  -0.573627   -0.467679  0.017016   \n",
      "\n",
      "                        sma20_rsi14    ret_5d  pos_top20  pos_top50  \\\n",
      "OPEN_DATETIME TICKER                                                  \n",
      "2021-05-24    ALBFR.PA     0.072653 -0.546093   0.703757   0.715240   \n",
      "2021-05-25    ALBFR.PA     0.059934 -0.535190   0.710660   0.721875   \n",
      "2021-05-26    ALBFR.PA     0.052429 -0.500846   0.758875   0.768221   \n",
      "2021-05-27    ALBFR.PA     0.037507 -0.520147   0.724439   0.735120   \n",
      "2021-05-28    ALBFR.PA     0.020422 -0.514212   0.717537   0.728486   \n",
      "\n",
      "                        pos_bot20  pos_bot50  ...     adx14  adx14_neg  \\\n",
      "OPEN_DATETIME TICKER                          ...                        \n",
      "2021-05-24    ALBFR.PA  -0.974263  -0.872254  ... -0.564865  -0.567937   \n",
      "2021-05-25    ALBFR.PA  -0.972547  -0.870576  ... -0.583758  -0.599199   \n",
      "2021-05-26    ALBFR.PA  -0.960537  -0.858810  ... -0.594626  -0.617813   \n",
      "2021-05-27    ALBFR.PA  -0.969114  -0.867213  ... -0.604717  -0.636017   \n",
      "2021-05-28    ALBFR.PA  -0.970830  -0.868895  ... -0.614088  -0.653777   \n",
      "\n",
      "                        adx14_pos  adx14_dif  pos_avg_vol14  pos_sma20_200  \\\n",
      "OPEN_DATETIME TICKER                                                         \n",
      "2021-05-24    ALBFR.PA  -0.505505   0.031216      -0.959852      -0.188588   \n",
      "2021-05-25    ALBFR.PA  -0.496062   0.051569      -0.966076      -0.194086   \n",
      "2021-05-26    ALBFR.PA  -0.473027   0.072393      -0.950476      -0.198150   \n",
      "2021-05-27    ALBFR.PA  -0.498123   0.068947      -0.989995      -0.204980   \n",
      "2021-05-28    ALBFR.PA  -0.522606   0.065585      -0.966205      -0.212696   \n",
      "\n",
      "                        perf_sma_50_5d  perf_sma_200_5d  lab_perf_50d  \\\n",
      "OPEN_DATETIME TICKER                                                    \n",
      "2021-05-24    ALBFR.PA        0.064868         0.002198           2.0   \n",
      "2021-05-25    ALBFR.PA        0.069070         0.002827           2.0   \n",
      "2021-05-26    ALBFR.PA        0.074253         0.007014           2.0   \n",
      "2021-05-27    ALBFR.PA        0.076805         0.008480           2.0   \n",
      "2021-05-28    ALBFR.PA        0.082146         0.010678           2.0   \n",
      "\n",
      "                                                                 SEQUENCE  \n",
      "OPEN_DATETIME TICKER                                                       \n",
      "2021-05-24    ALBFR.PA  [[-0.49694, -0.55484, -0.45338, -0.06445, 0.27...  \n",
      "2021-05-25    ALBFR.PA  [[-0.51999, -0.57849, -0.47749, -0.15069, 0.24...  \n",
      "2021-05-26    ALBFR.PA  [[-0.49397, -0.55813, -0.45706, -0.04677, 0.21...  \n",
      "2021-05-27    ALBFR.PA  [[-0.50213, -0.56915, -0.46808, -0.08389, 0.18...  \n",
      "2021-05-28    ALBFR.PA  [[-0.47747, -0.55124, -0.44967, 0.00767, 0.165...  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "list_feat = df_class_conf_norm.columns.values.tolist()\n",
    "list_feat.remove(label)\n",
    "\n",
    "sequence_length = 10\n",
    "col_sequence = \"SEQUENCE\"\n",
    "\n",
    "df_class_conf_norm_sorted = df_class_conf_norm.sort_index(level=['TICKER', 'OPEN_DATETIME'])\n",
    "\n",
    "df_class_conf_seq = pd.DataFrame()\n",
    "cnt=0\n",
    "for ticker in df_class_conf_norm_sorted.index.get_level_values('TICKER').unique():\n",
    "    sub_df=df_class_conf_norm_sorted[df_class_conf_norm_sorted.index.get_level_values('TICKER') == ticker]\n",
    "    sub_df = sm.prepare_sequences_df(\n",
    "        df_in=sub_df, list_features=list_feat, sequence_length=sequence_length, str_new_col=col_sequence)\n",
    "    cnt+=1\n",
    "    if cnt%20==0:\n",
    "        print(f\"time {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} {cnt=} {ticker=}\")\n",
    "        gc.collect()\n",
    "    \n",
    "# concatenate all TICKER data in the same df\n",
    "    df_class_conf_seq = pd.concat([df_class_conf_seq, sub_df])\n",
    "\n",
    "print(f\"{df_class_conf_seq.shape=}\")\n",
    "print(df_class_conf_seq[10000:10005])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "def format_float(x):\n",
    "    return '{:.5f}'.format(x) if x is not None else None\n",
    "\n",
    "def array_to_string(x):\n",
    "    return np.array2string(x,separator='_') if x is not None else None\n",
    "\n",
    "vfunc = np.vectorize(format_float) \n",
    "\n",
    "df_class_conf_seq2=df_class_conf_seq.copy()\n",
    "df_class_conf_seq2[col_sequence] = df_class_conf_seq2[col_sequence].apply(vfunc)\n",
    "df_class_conf_seq2[col_sequence] = df_class_conf_seq2[col_sequence].apply(array_to_string)\n",
    "\n",
    "df_class_conf_seq2.round(5).to_csv(\n",
    "    PATH_DATA_DTS+dts_name+\"_CONF_seq_6\", sep=\",\", float_format='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_class_conf_csv.shape=(241914, 2)\n",
      "                       lab_perf_50d  \\\n",
      "TICKER  OPEN_DATETIME                 \n",
      "ABCA.PA 2021-03-05              1.0   \n",
      "        2021-03-08              1.0   \n",
      "        2021-03-09              1.0   \n",
      "        2021-03-10              0.0   \n",
      "        2021-03-11              0.0   \n",
      "\n",
      "                                                                SEQUENCE  \n",
      "TICKER  OPEN_DATETIME                                                     \n",
      "ABCA.PA 2021-03-05     [[-0.45025, -0.58082, -0.5836, 0.14294, 0.0824...  \n",
      "        2021-03-08     [[-0.45239, -0.58248, -0.58514, 0.10184, 0.081...  \n",
      "        2021-03-09     [[-0.45254, -0.58267, -0.58543, 0.10184, 0.080...  \n",
      "        2021-03-10     [[-0.44821, -0.5792, -0.58267, 0.18664, 0.0836...  \n",
      "        2021-03-11     [[-0.45221, -0.58244, -0.58549, 0.09738, 0.081...  \n",
      "                       lab_perf_50d  \\\n",
      "TICKER  OPEN_DATETIME                 \n",
      "ABCA.PA 2021-03-08              1.0   \n",
      "        2021-03-09              1.0   \n",
      "        2021-03-10              1.0   \n",
      "        2021-03-11              0.0   \n",
      "        2021-03-12              0.0   \n",
      "\n",
      "                                                                SEQUENCE  \n",
      "TICKER  OPEN_DATETIME                                                     \n",
      "ABCA.PA 2021-03-08     [[-0.45239, -0.58248, -0.58514, 0.10184, 0.081...  \n",
      "        2021-03-09     [[-0.45254, -0.58267, -0.58543, 0.10184, 0.080...  \n",
      "        2021-03-10     [[-0.44821, -0.5792, -0.58267, 0.18664, 0.0836...  \n",
      "        2021-03-11     [[-0.45221, -0.58244, -0.58549, 0.09738, 0.081...  \n",
      "        2021-03-12     [[-0.42939, -0.56432, -0.57048, 0.40099, 0.089...  \n"
     ]
    }
   ],
   "source": [
    "dts_name=\"PARIS_TREND_1D_50D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_50d\"\n",
    "col_sequence = \"SEQUENCE\"\n",
    "df_class_conf_csv=pd.read_csv(PATH_DATA_DTS+dts_name+\"_CONF_seq_6.zip\",sep=\",\",index_col=[\"TICKER\",\"OPEN_DATETIME\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class_conf_csv=df_class_conf_csv.dropna(subset=[col_sequence])\n",
    "df_class_conf_csv=df_class_conf_csv.sort_index()\n",
    "gc.collect()\n",
    "# keep only index, label and sequence\n",
    "\n",
    "df_class_conf_csv=df_class_conf_csv[[label,col_sequence]]\n",
    "df_class_conf_csv[col_sequence] = df_class_conf_csv[col_sequence].str.replace(\"_\", \",\").apply(ast.literal_eval)\n",
    "df_class_conf_csv[col_sequence]  = df_class_conf_csv[col_sequence] .apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "print(f\"{df_class_conf_csv.shape=}\")\n",
    "print(df_class_conf_csv[1015:1020])\n",
    "\n",
    "# decision is made between market sessions so we have shift the label of 1 day for each ticker\n",
    "df_class_conf_csv[label] = df_class_conf_csv.groupby(level='TICKER')[label].shift(1)\n",
    "df_class_conf_csv=df_class_conf_csv.dropna(subset=[label])\n",
    "print(df_class_conf_csv[1014:1019])\n",
    "df_class_conf_csv.to_pickle(PATH_DATA_DTS+dts_name+\"_CONF_seq_6.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benoit\\AppData\\Local\\Temp\\ipykernel_19660\\3892677125.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x_conf_tensor = torch.as_tensor(df_class_conf_csv[col_sequence], dtype=torch.float)\n",
      "C:\\Users\\Benoit\\AppData\\Local\\Temp\\ipykernel_19660\\3892677125.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_conf_tensor = torch.tensor(df_class_conf_csv[label], dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19162  3185 17629  8087 12365]\n",
      " [11303  4028 24980  6903  4221]\n",
      " [ 8634  3634 22104  5537  3348]\n",
      " [ 9375  3543 18740  5573  3537]\n",
      " [13558  3623 14491  4747  9309]]\n"
     ]
    }
   ],
   "source": [
    "dts_name=\"PARIS_TREND_1D_50D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_50d\"\n",
    "col_sequence = \"SEQUENCE\"\n",
    "\n",
    "df_class_conf_csv=pd.read_pickle(PATH_DATA_DTS+dts_name+\"_CONF_seq_6.pckl\") \n",
    "\n",
    "x_conf_tensor = torch.as_tensor(df_class_conf_csv[col_sequence], dtype=torch.float)\n",
    "y_conf_tensor = torch.tensor(df_class_conf_csv[label], dtype=torch.int64)\n",
    "\n",
    "file_model=\"PARIS_TREND_1D_50D_V2_lstm_pytorch_v1_lstm_48_1_v5.pth\" # >> 0.\n",
    "saved_model = torch.load(file_model, weights_only=False)\n",
    "saved_model.eval()\n",
    "y_pred = saved_model(x_conf_tensor)\n",
    "_, y_pred_classes = torch.max(y_pred, 1)\n",
    "df_class_conf_csv['predicted_score'] = y_pred_classes.numpy()\n",
    "\n",
    "confusion = metrics.confusion_matrix(\n",
    "    y_conf_tensor, y_pred_classes)\n",
    "print(confusion)\n",
    "\n",
    "df_class_conf_csv.to_pickle(PATH_DATA_DTS+dts_name+\"_CONF_seq_6_predict.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-95f7905441debbf6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-95f7905441debbf6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "###### SAVE CODE FOR BASIC PYTORCH #####\n",
    "###### BEFORE PYTORCH LIGHTNING ########\n",
    "\n",
    "list_param_valid = [\n",
    "                    {'model__dropout': 0.05, 'model__hidden_dim': 16, 'model__num_layers': 2, 'optimizer__lr': 0.1},\n",
    "                    # {'fit__batch_size': 256, 'model__dropout': 0.05, 'model__layers': [64, 10], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.9},\n",
    "                    # {'fit__batch_size': 32, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.7},\n",
    "                    # {'fit__batch_size': 32, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.5},\n",
    "                    # {'fit__batch_size': 64, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.9},\n",
    "                    # {'fit__batch_size': 64, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.7},\n",
    "                    # {'fit__batch_size': 64, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.5},\n",
    "]\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_dim = x_train_tensor.shape[2]\n",
    "num_classes = 5\n",
    "epochs = 6#350\n",
    "suffix=\"lstm_pytorch_v1\"\n",
    "filename_tmp_model = dts_name+\"_\"+suffix+\".pckl\"\n",
    "patience = 3\n",
    "\n",
    "val_accuracy=0.0\n",
    "obj_acc=0.25\n",
    "cpt_param=0 \n",
    "try_limit=5\n",
    "pct_check_class=0.4 # check if at least n% of the validation set per class\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "len_val=x_val_tensor.shape[0]\n",
    "check_class_limit=(len_val/num_classes)*pct_check_class\n",
    "check_class=False # check if at least obj_acc accuracy per class\n",
    "\n",
    "while(cpt_param<len(list_param_valid) and check_class==False):\n",
    "    param_valid=list_param_valid[cpt_param] #select the current param line\n",
    "    print(param_valid)\n",
    "    cpt=0\n",
    "\n",
    "    while(cpt<try_limit and check_class==False):\n",
    "        cpt+=1\n",
    "        \n",
    "        model = LSTMModel(input_dim=input_dim, hidden_dim=param_valid['model__hidden_dim'], num_layers=param_valid['model__num_layers'], num_classes=num_classes, dropout=param_valid['model__dropout'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = Adam(model.parameters(), lr=param_valid['optimizer__lr'])\n",
    "\n",
    "        if cpt==1:\n",
    "            print(model)\n",
    "            print(len(list(model.parameters())))\n",
    "            for i in range(len(list(model.parameters()))):\n",
    "                print(list(model.parameters())[i].size())\n",
    "\n",
    "        # Training loop\n",
    "        hist = np.zeros(epochs)\n",
    "        for epoch in range(epochs):\n",
    "            for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if epoch % 1 == 0 :   #change % \n",
    "                print(f\"Epoch {epoch+1} CrossEntropyLoss: {loss.item()}\")\n",
    "            hist[epoch] = loss.item()\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                outputs = model(x_batch)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        val_accuracy = correct / total\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "        if val_accuracy>obj_acc:\n",
    "            print(f\"Optim success {cpt=} {val_accuracy=}\")\n",
    "            check_class=True #exit directly\n",
    "\n",
    "            # calculate the confusion matrix\n",
    "            y_pred = model(x_val_tensor)\n",
    "            _, y_pred_classes = torch.max(y_pred, 1)\n",
    "            confusion = metrics.confusion_matrix(y_val_tensor, y_pred_classes)\n",
    "            print(confusion)\n",
    "\n",
    "            for i in range(num_classes):\n",
    "                nb_lab=sum(y_pred_classes == i)\n",
    "                if nb_lab<check_class_limit  :\n",
    "                    check_class=False\n",
    "                    print(f\"Check class {i=} {nb_lab=} {check_class=} {check_class_limit=}\")\n",
    "                # print(f\"Categ {i}: real {sum(y_val_tensor == i)} predict {sum(y_pred_classes == i)}\")\n",
    "\n",
    "\n",
    "            #check saved model, load to check it's OK\n",
    "            if check_class:\n",
    "                torch.save(model, filename_tmp_model)\n",
    "                saved_model = torch.load(filename_tmp_model)\n",
    "                saved_model.eval()\n",
    "                y_pred = saved_model(x_val_tensor)\n",
    "                _, y_pred_classes = torch.max(y_pred, 1)\n",
    "                confusion = metrics.confusion_matrix(y_val_tensor, y_pred_classes)\n",
    "                print(confusion)\n",
    "\n",
    "    if cpt>=try_limit :\n",
    "        cpt_param+=1\n",
    "        print(f\"Optim fail {cpt=} param suivant {cpt_param=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[-1]\n",
    "window_size = sequence_length\n",
    "dropout = 0.2\n",
    "num_classes = 4\n",
    "\n",
    "# cat_y_train = keras.utils.to_categorical(col_y_train, num_classes)\n",
    "# cat_y_valid = keras.utils.to_categorical(col_y_valid, num_classes)\n",
    "\n",
    "# df_x_train_exp = np.expand_dims(df_x_train, axis=2)\n",
    "# df_x_valid_exp = np.expand_dims(df_x_valid, axis=2)\n",
    "\n",
    "\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(LSTM(units=20, return_sequences=False,#True\n",
    "               input_shape=(window_size, input_dim)))\n",
    "#,kernel_regularizer=l2(0.1), recurrent_regularizer=l2(0.1), bias_regularizer=l2(0.1)\n",
    "model_LSTM.add(Dropout(rate=dropout))   \n",
    "# model_LSTM.add(Dropout(rate=dropout))\n",
    "# model_LSTM.add(Bidirectional(LSTM((window_size * 2), return_sequences=True)))\n",
    "# model_LSTM.add(Dropout(rate=dropout))\n",
    "# model_LSTM.add(Bidirectional(LSTM(window_size, return_sequences=False)))\n",
    "model_LSTM.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "model_LSTM.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model_LSTM.fit(x_train_lstm, y_train_lstm, batch_size=1024,\n",
    "                         shuffle=False, epochs=20, validation_data=(x_val_lstm, y_val_lstm))#,verbose=0\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Plot loss\n",
    "epochs = range(1, len(train_accuracy) + 1)\n",
    "plt.plot(epochs, train_accuracy, 'bo-', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'ro-', label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print if keras can use the gpu to train the model\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
