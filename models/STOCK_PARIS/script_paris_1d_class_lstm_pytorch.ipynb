{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'balance' from 'C:\\\\Projets\\\\MarketDataEnrichment\\\\dataset_mngr\\\\balance.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import importlib\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset,RandomSampler\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import sqlite_io as sio\n",
    "import add_indicators as indic\n",
    "import split_merge as sm\n",
    "import balance  # wait for new release https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1081\n",
    "import model_mngr as modmgr\n",
    "\n",
    "importlib.reload(sio)\n",
    "importlib.reload(modmgr)\n",
    "importlib.reload(sm)\n",
    "importlib.reload(balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = \"C:\\\\Projets\\\\Data\"\n",
    "PATH_DB_FWK=\"C:\\\\Projets\\\\Data\\\\sqlite\\\\dataset_market.db\"\n",
    "PATH_DB_STOCK=\"C:\\\\Projets\\\\Data\\\\sqlite\\\\dataset_paris_stock_adjusted.db\"\n",
    "PATH_DATA_DTS=PATH_DATA+\"\\\\DTS_FULL\\\\\"\n",
    "\n",
    "SUFFIX_TRAIN=\"_TRAIN.zip\"\n",
    "SUFFIX_VAL=\"_VAL.zip\"\n",
    "SUFFIX_CONF=\"_CONF.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONNECTION TO SQLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"con_stock\" in locals():\n",
    "        sio.close_connection(con_stock)\n",
    "con_stock = sio.get_connection(str_db_path=PATH_DB_STOCK)\n",
    "\n",
    "if \"con_fwk\" in locals():\n",
    "        sio.close_connection(con_fwk)\n",
    "con_fwk = sio.get_connection(str_db_path=PATH_DB_FWK)\n",
    "\n",
    "my_session_maker = sessionmaker(bind=con_fwk)\n",
    "session=my_session_maker()\n",
    "\n",
    "table_stock=\"DS_PARIS_1D_ADJ_CLEAN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "\n",
    "df_base=sio.get_candles_to_df(session=session,con=con_stock, target_table=table_stock,tradable=True)\n",
    "df_work=pd.DataFrame()\n",
    "for code_value in df_base.index.get_level_values('CODE').unique():\n",
    "    sub_df=df_base[df_base.index.get_level_values('CODE') == code_value]\n",
    "    df_work_tmp = indic.add_indicators_to_df(con=con_fwk, df_in=sub_df, dts_name=dts_name,symbol=multi_symbol)\n",
    "    df_work = pd.concat([df_work, df_work_tmp])\n",
    "    \n",
    "df_work.sort_index(inplace=True)\n",
    "df_work.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_work[10000:10010]\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# print(df_work.describe())\n",
    "\n",
    "df_work.round(5).to_csv(\n",
    "    PATH_DATA_DTS+dts_name+\"_BASE.zip\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START HERE FOR BASE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>OPEN</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>VOLUME</th>\n",
       "      <th>sma20</th>\n",
       "      <th>pos_sma20</th>\n",
       "      <th>sma50</th>\n",
       "      <th>sma200</th>\n",
       "      <th>pos_sma50</th>\n",
       "      <th>...</th>\n",
       "      <th>adx14</th>\n",
       "      <th>adx14_neg</th>\n",
       "      <th>adx14_pos</th>\n",
       "      <th>adx14_dif</th>\n",
       "      <th>avg_vol14</th>\n",
       "      <th>pos_avg_vol14</th>\n",
       "      <th>pos_sma20_200</th>\n",
       "      <th>williamsr_14</th>\n",
       "      <th>perf_sma_50_5d</th>\n",
       "      <th>perf_sma_200_5d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPEN_DATETIME</th>\n",
       "      <th>CODE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-04-26</th>\n",
       "      <th>AB.PA</th>\n",
       "      <td>12.98</td>\n",
       "      <td>12.98</td>\n",
       "      <td>12.20</td>\n",
       "      <td>12.68</td>\n",
       "      <td>62866.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.68000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-27</th>\n",
       "      <th>AB.PA</th>\n",
       "      <td>12.74</td>\n",
       "      <td>12.83</td>\n",
       "      <td>12.61</td>\n",
       "      <td>12.70</td>\n",
       "      <td>22370.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.69000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-28</th>\n",
       "      <th>AB.PA</th>\n",
       "      <td>12.70</td>\n",
       "      <td>12.70</td>\n",
       "      <td>12.41</td>\n",
       "      <td>12.50</td>\n",
       "      <td>8211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.62667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.01003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-29</th>\n",
       "      <th>AB.PA</th>\n",
       "      <td>12.60</td>\n",
       "      <td>12.65</td>\n",
       "      <td>12.46</td>\n",
       "      <td>12.64</td>\n",
       "      <td>4676.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.63000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-30</th>\n",
       "      <th>AB.PA</th>\n",
       "      <td>12.63</td>\n",
       "      <td>12.71</td>\n",
       "      <td>12.55</td>\n",
       "      <td>12.65</td>\n",
       "      <td>4470.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.63400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      OPEN   HIGH    LOW  CLOSE   VOLUME  sma20  pos_sma20  \\\n",
       "OPEN_DATETIME CODE                                                           \n",
       "2010-04-26    AB.PA  12.98  12.98  12.20  12.68  62866.0    NaN        NaN   \n",
       "2010-04-27    AB.PA  12.74  12.83  12.61  12.70  22370.0    NaN        NaN   \n",
       "2010-04-28    AB.PA  12.70  12.70  12.41  12.50   8211.0    NaN        NaN   \n",
       "2010-04-29    AB.PA  12.60  12.65  12.46  12.64   4676.0    NaN        NaN   \n",
       "2010-04-30    AB.PA  12.63  12.71  12.55  12.65   4470.0    NaN        NaN   \n",
       "\n",
       "                        sma50  sma200  pos_sma50  ...  adx14  adx14_neg  \\\n",
       "OPEN_DATETIME CODE                                ...                     \n",
       "2010-04-26    AB.PA  12.68000     NaN    0.00000  ...    0.0        0.0   \n",
       "2010-04-27    AB.PA  12.69000     NaN    0.00079  ...    0.0        0.0   \n",
       "2010-04-28    AB.PA  12.62667     NaN   -0.01003  ...    0.0        0.0   \n",
       "2010-04-29    AB.PA  12.63000     NaN    0.00079  ...    0.0        0.0   \n",
       "2010-04-30    AB.PA  12.63400     NaN    0.00127  ...    0.0        0.0   \n",
       "\n",
       "                     adx14_pos  adx14_dif  avg_vol14  pos_avg_vol14  \\\n",
       "OPEN_DATETIME CODE                                                    \n",
       "2010-04-26    AB.PA        0.0        0.0        NaN            NaN   \n",
       "2010-04-27    AB.PA        0.0        0.0        NaN            NaN   \n",
       "2010-04-28    AB.PA        0.0        0.0        NaN            NaN   \n",
       "2010-04-29    AB.PA        0.0        0.0        NaN            NaN   \n",
       "2010-04-30    AB.PA        0.0        0.0        NaN            NaN   \n",
       "\n",
       "                     pos_sma20_200  williamsr_14  perf_sma_50_5d  \\\n",
       "OPEN_DATETIME CODE                                                 \n",
       "2010-04-26    AB.PA            NaN           NaN             NaN   \n",
       "2010-04-27    AB.PA            NaN           NaN             NaN   \n",
       "2010-04-28    AB.PA            NaN           NaN             NaN   \n",
       "2010-04-29    AB.PA            NaN           NaN             NaN   \n",
       "2010-04-30    AB.PA            NaN           NaN             NaN   \n",
       "\n",
       "                     perf_sma_200_5d  \n",
       "OPEN_DATETIME CODE                    \n",
       "2010-04-26    AB.PA              NaN  \n",
       "2010-04-27    AB.PA              NaN  \n",
       "2010-04-28    AB.PA              NaN  \n",
       "2010-04-29    AB.PA              NaN  \n",
       "2010-04-30    AB.PA              NaN  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "# dts_name=\"PARIS_TREND_1D_50D_V1\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "\n",
    "df_work=pd.read_csv(PATH_DATA_DTS+dts_name+\"_BASE.zip\",sep=\",\",index_col=[\"OPEN_DATETIME\",\"CODE\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_work.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with no pos_sma200 \n",
    "df_work=df_work.dropna(subset=['pos_sma200'])\n",
    "\n",
    "# if williamsr_14 >0 =0 if williamsr_14<-100 = -100\n",
    "# df_work['williamsr_14']=df_work['williamsr_14'].apply(lambda x: 0 if x>0 else x)\n",
    "# df_work['williamsr_14']=df_work['williamsr_14'].apply(lambda x: -100 if x<-100 else x)\n",
    "\n",
    "# if williamsr_14 >0 =0 if williamsr_14<-100 = -100\n",
    "df_work.loc[df_work['williamsr_14'] > 0, 'williamsr_14'] = 0\n",
    "df_work.loc[df_work['williamsr_14'] < -100, 'williamsr_14'] = -100\n",
    "\n",
    "# print min and max of the columns williamsr_14, perf_sma_50_5d, perf_sma_200_5d\n",
    "# print(f\"{df_work['williamsr_14'].min()=}\")  inf-100\n",
    "# print(f\"{df_work['williamsr_14'].max()=}\") sup 0\n",
    "\n",
    "# df_check=df_work[df_work['perf_sma_50_5d'] > 1]\n",
    "# df_check=df_check[df_check['ret_1d'] <= 2]\n",
    "# print(df_check.index.get_level_values('CODE').unique())\n",
    "# df_check[df_check.index.get_level_values('CODE')=='AI.PA']\n",
    "# df_check.head(5)\n",
    "# df_check=df_work[df_work.index.get_level_values('CODE')=='AI.PA']\n",
    "# CATG\n",
    "# mask = df_work['stdev20_1d'] > 1000\n",
    "# df_work.drop(df_work[mask].index, inplace=True)\n",
    "# df_check[6000:6010]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>OPEN</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>VOLUME</th>\n",
       "      <th>pos_sma20</th>\n",
       "      <th>pos_sma50</th>\n",
       "      <th>pos_sma200</th>\n",
       "      <th>pos_sma50_200</th>\n",
       "      <th>pos_sma20_50</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_donchian20_lo</th>\n",
       "      <th>adx14</th>\n",
       "      <th>adx14_neg</th>\n",
       "      <th>adx14_pos</th>\n",
       "      <th>adx14_dif</th>\n",
       "      <th>pos_avg_vol14</th>\n",
       "      <th>pos_sma20_200</th>\n",
       "      <th>williamsr_14</th>\n",
       "      <th>perf_sma_50_5d</th>\n",
       "      <th>perf_sma_200_5d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPEN_DATETIME</th>\n",
       "      <th>CODE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-06-02</th>\n",
       "      <th>ABEO.PA</th>\n",
       "      <td>7.6136</td>\n",
       "      <td>7.9789</td>\n",
       "      <td>7.6136</td>\n",
       "      <td>7.9789</td>\n",
       "      <td>6332.0</td>\n",
       "      <td>0.09139</td>\n",
       "      <td>-0.03555</td>\n",
       "      <td>-0.53194</td>\n",
       "      <td>-0.51468</td>\n",
       "      <td>-0.11631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26911</td>\n",
       "      <td>26.15567</td>\n",
       "      <td>22.77986</td>\n",
       "      <td>24.12433</td>\n",
       "      <td>1.34447</td>\n",
       "      <td>0.93774</td>\n",
       "      <td>-0.57113</td>\n",
       "      <td>-31.81630</td>\n",
       "      <td>-0.03229</td>\n",
       "      <td>-0.02455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-03</th>\n",
       "      <th>ABEO.PA</th>\n",
       "      <td>8.1711</td>\n",
       "      <td>8.6710</td>\n",
       "      <td>8.1519</td>\n",
       "      <td>8.4595</td>\n",
       "      <td>8982.0</td>\n",
       "      <td>0.14359</td>\n",
       "      <td>0.02936</td>\n",
       "      <td>-0.50131</td>\n",
       "      <td>-0.51553</td>\n",
       "      <td>-0.09989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.34555</td>\n",
       "      <td>25.89215</td>\n",
       "      <td>20.36515</td>\n",
       "      <td>32.16729</td>\n",
       "      <td>11.80214</td>\n",
       "      <td>1.50382</td>\n",
       "      <td>-0.56393</td>\n",
       "      <td>-11.45907</td>\n",
       "      <td>-0.03301</td>\n",
       "      <td>-0.02453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-04</th>\n",
       "      <th>ABEO.PA</th>\n",
       "      <td>8.5557</td>\n",
       "      <td>8.5557</td>\n",
       "      <td>7.7866</td>\n",
       "      <td>8.4019</td>\n",
       "      <td>7735.0</td>\n",
       "      <td>0.12282</td>\n",
       "      <td>0.02657</td>\n",
       "      <td>-0.50225</td>\n",
       "      <td>-0.51513</td>\n",
       "      <td>-0.08572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.28909</td>\n",
       "      <td>24.74737</td>\n",
       "      <td>23.41954</td>\n",
       "      <td>28.54604</td>\n",
       "      <td>5.12651</td>\n",
       "      <td>1.39693</td>\n",
       "      <td>-0.55669</td>\n",
       "      <td>-16.46677</td>\n",
       "      <td>-0.03314</td>\n",
       "      <td>-0.02459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-05</th>\n",
       "      <th>ABEO.PA</th>\n",
       "      <td>8.4211</td>\n",
       "      <td>8.7864</td>\n",
       "      <td>8.2480</td>\n",
       "      <td>8.5749</td>\n",
       "      <td>6423.0</td>\n",
       "      <td>0.13198</td>\n",
       "      <td>0.05137</td>\n",
       "      <td>-0.48943</td>\n",
       "      <td>-0.51437</td>\n",
       "      <td>-0.07121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30793</td>\n",
       "      <td>24.10544</td>\n",
       "      <td>21.58743</td>\n",
       "      <td>29.66498</td>\n",
       "      <td>8.07755</td>\n",
       "      <td>1.15302</td>\n",
       "      <td>-0.54895</td>\n",
       "      <td>-12.08848</td>\n",
       "      <td>-0.03011</td>\n",
       "      <td>-0.02463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-08</th>\n",
       "      <th>ABEO.PA</th>\n",
       "      <td>8.7672</td>\n",
       "      <td>9.0363</td>\n",
       "      <td>8.5941</td>\n",
       "      <td>8.7479</td>\n",
       "      <td>19545.0</td>\n",
       "      <td>0.14221</td>\n",
       "      <td>0.07638</td>\n",
       "      <td>-0.47650</td>\n",
       "      <td>-0.51365</td>\n",
       "      <td>-0.05763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.33431</td>\n",
       "      <td>23.93590</td>\n",
       "      <td>20.13379</td>\n",
       "      <td>31.31449</td>\n",
       "      <td>11.18069</td>\n",
       "      <td>3.09019</td>\n",
       "      <td>-0.54168</td>\n",
       "      <td>-14.42361</td>\n",
       "      <td>-0.02444</td>\n",
       "      <td>-0.02464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-09</th>\n",
       "      <th>ABEO.PA</th>\n",
       "      <td>8.7479</td>\n",
       "      <td>9.0363</td>\n",
       "      <td>8.3826</td>\n",
       "      <td>8.8056</td>\n",
       "      <td>8807.0</td>\n",
       "      <td>0.13831</td>\n",
       "      <td>0.08706</td>\n",
       "      <td>-0.47041</td>\n",
       "      <td>-0.51282</td>\n",
       "      <td>-0.04502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.34312</td>\n",
       "      <td>23.25077</td>\n",
       "      <td>21.27235</td>\n",
       "      <td>28.39698</td>\n",
       "      <td>7.12462</td>\n",
       "      <td>1.37399</td>\n",
       "      <td>-0.53476</td>\n",
       "      <td>-11.53788</td>\n",
       "      <td>-0.02087</td>\n",
       "      <td>-0.02461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-10</th>\n",
       "      <th>ABEO.PA</th>\n",
       "      <td>9.0363</td>\n",
       "      <td>9.0363</td>\n",
       "      <td>8.2673</td>\n",
       "      <td>8.6518</td>\n",
       "      <td>6484.0</td>\n",
       "      <td>0.10674</td>\n",
       "      <td>0.06883</td>\n",
       "      <td>-0.47704</td>\n",
       "      <td>-0.51072</td>\n",
       "      <td>-0.03425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26761</td>\n",
       "      <td>22.33359</td>\n",
       "      <td>20.60949</td>\n",
       "      <td>25.39907</td>\n",
       "      <td>4.78959</td>\n",
       "      <td>1.06259</td>\n",
       "      <td>-0.52748</td>\n",
       "      <td>-19.22981</td>\n",
       "      <td>-0.01504</td>\n",
       "      <td>-0.02473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-11</th>\n",
       "      <th>ABEO.PA</th>\n",
       "      <td>8.5557</td>\n",
       "      <td>8.5557</td>\n",
       "      <td>8.0173</td>\n",
       "      <td>8.1711</td>\n",
       "      <td>5301.0</td>\n",
       "      <td>0.03645</td>\n",
       "      <td>0.01229</td>\n",
       "      <td>-0.50358</td>\n",
       "      <td>-0.50960</td>\n",
       "      <td>-0.02332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19718</td>\n",
       "      <td>20.89547</td>\n",
       "      <td>22.22109</td>\n",
       "      <td>23.22080</td>\n",
       "      <td>0.99971</td>\n",
       "      <td>0.83871</td>\n",
       "      <td>-0.52104</td>\n",
       "      <td>-50.00289</td>\n",
       "      <td>-0.01374</td>\n",
       "      <td>-0.02486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-12</th>\n",
       "      <th>ABEO.PA</th>\n",
       "      <td>7.9981</td>\n",
       "      <td>8.0750</td>\n",
       "      <td>7.5175</td>\n",
       "      <td>7.6328</td>\n",
       "      <td>11060.0</td>\n",
       "      <td>-0.03547</td>\n",
       "      <td>-0.05092</td>\n",
       "      <td>-0.53389</td>\n",
       "      <td>-0.50888</td>\n",
       "      <td>-0.01602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08470</td>\n",
       "      <td>20.25323</td>\n",
       "      <td>26.93381</td>\n",
       "      <td>21.20351</td>\n",
       "      <td>-5.73030</td>\n",
       "      <td>1.63685</td>\n",
       "      <td>-0.51675</td>\n",
       "      <td>-81.11310</td>\n",
       "      <td>-0.01393</td>\n",
       "      <td>-0.02496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-15</th>\n",
       "      <th>ABEO.PA</th>\n",
       "      <td>7.9789</td>\n",
       "      <td>7.9789</td>\n",
       "      <td>7.5944</td>\n",
       "      <td>7.5944</td>\n",
       "      <td>5996.0</td>\n",
       "      <td>-0.04161</td>\n",
       "      <td>-0.05262</td>\n",
       "      <td>-0.53395</td>\n",
       "      <td>-0.50807</td>\n",
       "      <td>-0.01149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07924</td>\n",
       "      <td>19.65686</td>\n",
       "      <td>25.52877</td>\n",
       "      <td>20.09740</td>\n",
       "      <td>-5.43137</td>\n",
       "      <td>0.87697</td>\n",
       "      <td>-0.51372</td>\n",
       "      <td>-91.46210</td>\n",
       "      <td>-0.01365</td>\n",
       "      <td>-0.02485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         OPEN    HIGH     LOW   CLOSE   VOLUME  pos_sma20  \\\n",
       "OPEN_DATETIME CODE                                                          \n",
       "2020-06-02    ABEO.PA  7.6136  7.9789  7.6136  7.9789   6332.0    0.09139   \n",
       "2020-06-03    ABEO.PA  8.1711  8.6710  8.1519  8.4595   8982.0    0.14359   \n",
       "2020-06-04    ABEO.PA  8.5557  8.5557  7.7866  8.4019   7735.0    0.12282   \n",
       "2020-06-05    ABEO.PA  8.4211  8.7864  8.2480  8.5749   6423.0    0.13198   \n",
       "2020-06-08    ABEO.PA  8.7672  9.0363  8.5941  8.7479  19545.0    0.14221   \n",
       "2020-06-09    ABEO.PA  8.7479  9.0363  8.3826  8.8056   8807.0    0.13831   \n",
       "2020-06-10    ABEO.PA  9.0363  9.0363  8.2673  8.6518   6484.0    0.10674   \n",
       "2020-06-11    ABEO.PA  8.5557  8.5557  8.0173  8.1711   5301.0    0.03645   \n",
       "2020-06-12    ABEO.PA  7.9981  8.0750  7.5175  7.6328  11060.0   -0.03547   \n",
       "2020-06-15    ABEO.PA  7.9789  7.9789  7.5944  7.5944   5996.0   -0.04161   \n",
       "\n",
       "                       pos_sma50  pos_sma200  pos_sma50_200  pos_sma20_50  \\\n",
       "OPEN_DATETIME CODE                                                          \n",
       "2020-06-02    ABEO.PA   -0.03555    -0.53194       -0.51468      -0.11631   \n",
       "2020-06-03    ABEO.PA    0.02936    -0.50131       -0.51553      -0.09989   \n",
       "2020-06-04    ABEO.PA    0.02657    -0.50225       -0.51513      -0.08572   \n",
       "2020-06-05    ABEO.PA    0.05137    -0.48943       -0.51437      -0.07121   \n",
       "2020-06-08    ABEO.PA    0.07638    -0.47650       -0.51365      -0.05763   \n",
       "2020-06-09    ABEO.PA    0.08706    -0.47041       -0.51282      -0.04502   \n",
       "2020-06-10    ABEO.PA    0.06883    -0.47704       -0.51072      -0.03425   \n",
       "2020-06-11    ABEO.PA    0.01229    -0.50358       -0.50960      -0.02332   \n",
       "2020-06-12    ABEO.PA   -0.05092    -0.53389       -0.50888      -0.01602   \n",
       "2020-06-15    ABEO.PA   -0.05262    -0.53395       -0.50807      -0.01149   \n",
       "\n",
       "                       ...  pos_donchian20_lo     adx14  adx14_neg  adx14_pos  \\\n",
       "OPEN_DATETIME CODE     ...                                                      \n",
       "2020-06-02    ABEO.PA  ...            0.26911  26.15567   22.77986   24.12433   \n",
       "2020-06-03    ABEO.PA  ...            0.34555  25.89215   20.36515   32.16729   \n",
       "2020-06-04    ABEO.PA  ...            0.28909  24.74737   23.41954   28.54604   \n",
       "2020-06-05    ABEO.PA  ...            0.30793  24.10544   21.58743   29.66498   \n",
       "2020-06-08    ABEO.PA  ...            0.33431  23.93590   20.13379   31.31449   \n",
       "2020-06-09    ABEO.PA  ...            0.34312  23.25077   21.27235   28.39698   \n",
       "2020-06-10    ABEO.PA  ...            0.26761  22.33359   20.60949   25.39907   \n",
       "2020-06-11    ABEO.PA  ...            0.19718  20.89547   22.22109   23.22080   \n",
       "2020-06-12    ABEO.PA  ...            0.08470  20.25323   26.93381   21.20351   \n",
       "2020-06-15    ABEO.PA  ...            0.07924  19.65686   25.52877   20.09740   \n",
       "\n",
       "                       adx14_dif  pos_avg_vol14  pos_sma20_200  williamsr_14  \\\n",
       "OPEN_DATETIME CODE                                                             \n",
       "2020-06-02    ABEO.PA    1.34447        0.93774       -0.57113     -31.81630   \n",
       "2020-06-03    ABEO.PA   11.80214        1.50382       -0.56393     -11.45907   \n",
       "2020-06-04    ABEO.PA    5.12651        1.39693       -0.55669     -16.46677   \n",
       "2020-06-05    ABEO.PA    8.07755        1.15302       -0.54895     -12.08848   \n",
       "2020-06-08    ABEO.PA   11.18069        3.09019       -0.54168     -14.42361   \n",
       "2020-06-09    ABEO.PA    7.12462        1.37399       -0.53476     -11.53788   \n",
       "2020-06-10    ABEO.PA    4.78959        1.06259       -0.52748     -19.22981   \n",
       "2020-06-11    ABEO.PA    0.99971        0.83871       -0.52104     -50.00289   \n",
       "2020-06-12    ABEO.PA   -5.73030        1.63685       -0.51675     -81.11310   \n",
       "2020-06-15    ABEO.PA   -5.43137        0.87697       -0.51372     -91.46210   \n",
       "\n",
       "                       perf_sma_50_5d  perf_sma_200_5d  \n",
       "OPEN_DATETIME CODE                                      \n",
       "2020-06-02    ABEO.PA        -0.03229         -0.02455  \n",
       "2020-06-03    ABEO.PA        -0.03301         -0.02453  \n",
       "2020-06-04    ABEO.PA        -0.03314         -0.02459  \n",
       "2020-06-05    ABEO.PA        -0.03011         -0.02463  \n",
       "2020-06-08    ABEO.PA        -0.02444         -0.02464  \n",
       "2020-06-09    ABEO.PA        -0.02087         -0.02461  \n",
       "2020-06-10    ABEO.PA        -0.01504         -0.02473  \n",
       "2020-06-11    ABEO.PA        -0.01374         -0.02486  \n",
       "2020-06-12    ABEO.PA        -0.01393         -0.02496  \n",
       "2020-06-15    ABEO.PA        -0.01365         -0.02485  \n",
       "\n",
       "[10 rows x 53 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_work[10000:10010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           LABEL\n",
      "0   lab_perf_20d\n",
      "1   lab_perf_50d\n",
      "2  lab_perf_125d\n"
     ]
    }
   ],
   "source": [
    "df_work = indic.drop_indicators_by_type(\n",
    "    con=con_fwk, df_in=df_work, dts_name=dts_name, symbol=multi_symbol, ind_type=0)\n",
    "list_label = indic.get_ind_list_by_type_for_dts(\n",
    "    con=con_fwk, dts_name=dts_name, symbol_code=multi_symbol, ind_type=2)\n",
    "print(list_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>OPEN</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>VOLUME</th>\n",
       "      <th>pos_sma20</th>\n",
       "      <th>pos_sma50</th>\n",
       "      <th>pos_sma200</th>\n",
       "      <th>pos_sma50_200</th>\n",
       "      <th>pos_sma20_50</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_donchian20_lo</th>\n",
       "      <th>adx14</th>\n",
       "      <th>adx14_neg</th>\n",
       "      <th>adx14_pos</th>\n",
       "      <th>adx14_dif</th>\n",
       "      <th>pos_avg_vol14</th>\n",
       "      <th>pos_sma20_200</th>\n",
       "      <th>williamsr_14</th>\n",
       "      <th>perf_sma_50_5d</th>\n",
       "      <th>perf_sma_200_5d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPEN_DATETIME</th>\n",
       "      <th>CODE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">1995-11-24</th>\n",
       "      <th>BN.PA</th>\n",
       "      <td>5.3121</td>\n",
       "      <td>5.3292</td>\n",
       "      <td>5.3035</td>\n",
       "      <td>5.3121</td>\n",
       "      <td>1112239.0</td>\n",
       "      <td>0.01177</td>\n",
       "      <td>0.01509</td>\n",
       "      <td>0.03347</td>\n",
       "      <td>0.01811</td>\n",
       "      <td>0.00328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07840</td>\n",
       "      <td>20.28870</td>\n",
       "      <td>17.98263</td>\n",
       "      <td>24.68813</td>\n",
       "      <td>6.70549</td>\n",
       "      <td>0.57971</td>\n",
       "      <td>0.02145</td>\n",
       "      <td>-43.92655</td>\n",
       "      <td>-0.00423</td>\n",
       "      <td>0.00475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOI.PA</th>\n",
       "      <td>2.4317</td>\n",
       "      <td>2.4317</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>3392.0</td>\n",
       "      <td>-0.03791</td>\n",
       "      <td>-0.04912</td>\n",
       "      <td>0.10459</td>\n",
       "      <td>0.16165</td>\n",
       "      <td>-0.01165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>40.68240</td>\n",
       "      <td>27.60340</td>\n",
       "      <td>14.71410</td>\n",
       "      <td>-12.88930</td>\n",
       "      <td>0.08447</td>\n",
       "      <td>0.14812</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-0.00193</td>\n",
       "      <td>0.00800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDI.PA</th>\n",
       "      <td>5.5472</td>\n",
       "      <td>5.5958</td>\n",
       "      <td>5.5472</td>\n",
       "      <td>5.5958</td>\n",
       "      <td>85024.0</td>\n",
       "      <td>0.01444</td>\n",
       "      <td>0.04278</td>\n",
       "      <td>0.15177</td>\n",
       "      <td>0.10453</td>\n",
       "      <td>0.02793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>14.69556</td>\n",
       "      <td>19.13562</td>\n",
       "      <td>20.95757</td>\n",
       "      <td>1.82195</td>\n",
       "      <td>0.11627</td>\n",
       "      <td>0.13538</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00866</td>\n",
       "      <td>0.00701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEC.PA</th>\n",
       "      <td>3.9661</td>\n",
       "      <td>3.9661</td>\n",
       "      <td>3.9661</td>\n",
       "      <td>3.9661</td>\n",
       "      <td>162.0</td>\n",
       "      <td>-0.02968</td>\n",
       "      <td>-0.05170</td>\n",
       "      <td>-0.03635</td>\n",
       "      <td>0.01619</td>\n",
       "      <td>-0.02270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02507</td>\n",
       "      <td>14.74654</td>\n",
       "      <td>55.61796</td>\n",
       "      <td>43.88082</td>\n",
       "      <td>-11.73714</td>\n",
       "      <td>0.63943</td>\n",
       "      <td>-0.00688</td>\n",
       "      <td>-71.42857</td>\n",
       "      <td>-0.00965</td>\n",
       "      <td>0.00166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GFC.PA</th>\n",
       "      <td>3.3486</td>\n",
       "      <td>3.3486</td>\n",
       "      <td>3.2076</td>\n",
       "      <td>3.2093</td>\n",
       "      <td>32614.0</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>0.03218</td>\n",
       "      <td>0.24682</td>\n",
       "      <td>0.20795</td>\n",
       "      <td>0.02969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06339</td>\n",
       "      <td>33.09500</td>\n",
       "      <td>12.06264</td>\n",
       "      <td>26.76666</td>\n",
       "      <td>14.70403</td>\n",
       "      <td>2.22395</td>\n",
       "      <td>0.24381</td>\n",
       "      <td>-61.60998</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.01102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAT.PA</th>\n",
       "      <td>24.8529</td>\n",
       "      <td>24.8529</td>\n",
       "      <td>24.8529</td>\n",
       "      <td>24.8529</td>\n",
       "      <td>154.0</td>\n",
       "      <td>-0.00517</td>\n",
       "      <td>-0.01589</td>\n",
       "      <td>-0.05827</td>\n",
       "      <td>-0.04307</td>\n",
       "      <td>-0.01078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05672</td>\n",
       "      <td>11.41274</td>\n",
       "      <td>49.78475</td>\n",
       "      <td>43.88114</td>\n",
       "      <td>-5.90361</td>\n",
       "      <td>0.26857</td>\n",
       "      <td>-0.05338</td>\n",
       "      <td>-31.34328</td>\n",
       "      <td>-0.00242</td>\n",
       "      <td>-0.00235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LI.PA</th>\n",
       "      <td>2.0554</td>\n",
       "      <td>2.0554</td>\n",
       "      <td>2.0435</td>\n",
       "      <td>2.0435</td>\n",
       "      <td>143309.0</td>\n",
       "      <td>0.01303</td>\n",
       "      <td>-0.00332</td>\n",
       "      <td>-0.02706</td>\n",
       "      <td>-0.02382</td>\n",
       "      <td>-0.01615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03726</td>\n",
       "      <td>36.83981</td>\n",
       "      <td>9.74307</td>\n",
       "      <td>17.22198</td>\n",
       "      <td>7.47891</td>\n",
       "      <td>2.38002</td>\n",
       "      <td>-0.03958</td>\n",
       "      <td>-41.56051</td>\n",
       "      <td>-0.00162</td>\n",
       "      <td>-0.00054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RE.PA</th>\n",
       "      <td>4.8710</td>\n",
       "      <td>4.8710</td>\n",
       "      <td>4.7188</td>\n",
       "      <td>4.7188</td>\n",
       "      <td>33953.0</td>\n",
       "      <td>0.00203</td>\n",
       "      <td>-0.01415</td>\n",
       "      <td>-0.00043</td>\n",
       "      <td>0.01391</td>\n",
       "      <td>-0.01614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06898</td>\n",
       "      <td>12.87952</td>\n",
       "      <td>19.22271</td>\n",
       "      <td>23.88377</td>\n",
       "      <td>4.66107</td>\n",
       "      <td>0.72411</td>\n",
       "      <td>-0.00245</td>\n",
       "      <td>-44.43796</td>\n",
       "      <td>-0.01148</td>\n",
       "      <td>0.00182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAVE.PA</th>\n",
       "      <td>15.3615</td>\n",
       "      <td>15.6143</td>\n",
       "      <td>15.3615</td>\n",
       "      <td>15.3615</td>\n",
       "      <td>28928.0</td>\n",
       "      <td>0.03593</td>\n",
       "      <td>0.05608</td>\n",
       "      <td>0.05257</td>\n",
       "      <td>-0.00332</td>\n",
       "      <td>0.01945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08714</td>\n",
       "      <td>48.09569</td>\n",
       "      <td>4.50842</td>\n",
       "      <td>30.13034</td>\n",
       "      <td>25.62192</td>\n",
       "      <td>0.41972</td>\n",
       "      <td>0.01606</td>\n",
       "      <td>-36.89894</td>\n",
       "      <td>0.00149</td>\n",
       "      <td>0.00358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEP.PA</th>\n",
       "      <td>1.4524</td>\n",
       "      <td>1.4524</td>\n",
       "      <td>1.3325</td>\n",
       "      <td>1.3325</td>\n",
       "      <td>83775.0</td>\n",
       "      <td>-0.13757</td>\n",
       "      <td>-0.15298</td>\n",
       "      <td>-0.16537</td>\n",
       "      <td>-0.01463</td>\n",
       "      <td>-0.01787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>26.08714</td>\n",
       "      <td>65.25452</td>\n",
       "      <td>17.32163</td>\n",
       "      <td>-47.93289</td>\n",
       "      <td>3.00736</td>\n",
       "      <td>-0.03224</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-0.00313</td>\n",
       "      <td>-0.00357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          OPEN     HIGH      LOW    CLOSE     VOLUME  \\\n",
       "OPEN_DATETIME CODE                                                     \n",
       "1995-11-24    BN.PA     5.3121   5.3292   5.3035   5.3121  1112239.0   \n",
       "              BOI.PA    2.4317   2.4317   2.3817   2.3817     3392.0   \n",
       "              CDI.PA    5.5472   5.5958   5.5472   5.5958    85024.0   \n",
       "              ELEC.PA   3.9661   3.9661   3.9661   3.9661      162.0   \n",
       "              GFC.PA    3.3486   3.3486   3.2076   3.2093    32614.0   \n",
       "              LAT.PA   24.8529  24.8529  24.8529  24.8529      154.0   \n",
       "              LI.PA     2.0554   2.0554   2.0435   2.0435   143309.0   \n",
       "              RE.PA     4.8710   4.8710   4.7188   4.7188    33953.0   \n",
       "              SAVE.PA  15.3615  15.6143  15.3615  15.3615    28928.0   \n",
       "              TEP.PA    1.4524   1.4524   1.3325   1.3325    83775.0   \n",
       "\n",
       "                       pos_sma20  pos_sma50  pos_sma200  pos_sma50_200  \\\n",
       "OPEN_DATETIME CODE                                                       \n",
       "1995-11-24    BN.PA      0.01177    0.01509     0.03347        0.01811   \n",
       "              BOI.PA    -0.03791   -0.04912     0.10459        0.16165   \n",
       "              CDI.PA     0.01444    0.04278     0.15177        0.10453   \n",
       "              ELEC.PA   -0.02968   -0.05170    -0.03635        0.01619   \n",
       "              GFC.PA     0.00242    0.03218     0.24682        0.20795   \n",
       "              LAT.PA    -0.00517   -0.01589    -0.05827       -0.04307   \n",
       "              LI.PA      0.01303   -0.00332    -0.02706       -0.02382   \n",
       "              RE.PA      0.00203   -0.01415    -0.00043        0.01391   \n",
       "              SAVE.PA    0.03593    0.05608     0.05257       -0.00332   \n",
       "              TEP.PA    -0.13757   -0.15298    -0.16537       -0.01463   \n",
       "\n",
       "                       pos_sma20_50  ...  pos_donchian20_lo     adx14  \\\n",
       "OPEN_DATETIME CODE                   ...                                \n",
       "1995-11-24    BN.PA         0.00328  ...            0.07840  20.28870   \n",
       "              BOI.PA       -0.01165  ...            0.00000  40.68240   \n",
       "              CDI.PA        0.02793  ...            0.04362  14.69556   \n",
       "              ELEC.PA      -0.02270  ...            0.02507  14.74654   \n",
       "              GFC.PA        0.02969  ...            0.06339  33.09500   \n",
       "              LAT.PA       -0.01078  ...            0.05672  11.41274   \n",
       "              LI.PA        -0.01615  ...            0.03726  36.83981   \n",
       "              RE.PA        -0.01614  ...            0.06898  12.87952   \n",
       "              SAVE.PA       0.01945  ...            0.08714  48.09569   \n",
       "              TEP.PA       -0.01787  ...            0.00000  26.08714   \n",
       "\n",
       "                       adx14_neg  adx14_pos  adx14_dif  pos_avg_vol14  \\\n",
       "OPEN_DATETIME CODE                                                      \n",
       "1995-11-24    BN.PA     17.98263   24.68813    6.70549        0.57971   \n",
       "              BOI.PA    27.60340   14.71410  -12.88930        0.08447   \n",
       "              CDI.PA    19.13562   20.95757    1.82195        0.11627   \n",
       "              ELEC.PA   55.61796   43.88082  -11.73714        0.63943   \n",
       "              GFC.PA    12.06264   26.76666   14.70403        2.22395   \n",
       "              LAT.PA    49.78475   43.88114   -5.90361        0.26857   \n",
       "              LI.PA      9.74307   17.22198    7.47891        2.38002   \n",
       "              RE.PA     19.22271   23.88377    4.66107        0.72411   \n",
       "              SAVE.PA    4.50842   30.13034   25.62192        0.41972   \n",
       "              TEP.PA    65.25452   17.32163  -47.93289        3.00736   \n",
       "\n",
       "                       pos_sma20_200  williamsr_14  perf_sma_50_5d  \\\n",
       "OPEN_DATETIME CODE                                                   \n",
       "1995-11-24    BN.PA          0.02145     -43.92655        -0.00423   \n",
       "              BOI.PA         0.14812    -100.00000        -0.00193   \n",
       "              CDI.PA         0.13538      -0.00000         0.00866   \n",
       "              ELEC.PA       -0.00688     -71.42857        -0.00965   \n",
       "              GFC.PA         0.24381     -61.60998         0.00240   \n",
       "              LAT.PA        -0.05338     -31.34328        -0.00242   \n",
       "              LI.PA         -0.03958     -41.56051        -0.00162   \n",
       "              RE.PA         -0.00245     -44.43796        -0.01148   \n",
       "              SAVE.PA        0.01606     -36.89894         0.00149   \n",
       "              TEP.PA        -0.03224    -100.00000        -0.00313   \n",
       "\n",
       "                       perf_sma_200_5d  \n",
       "OPEN_DATETIME CODE                      \n",
       "1995-11-24    BN.PA            0.00475  \n",
       "              BOI.PA           0.00800  \n",
       "              CDI.PA           0.00701  \n",
       "              ELEC.PA          0.00166  \n",
       "              GFC.PA           0.01102  \n",
       "              LAT.PA          -0.00235  \n",
       "              LI.PA           -0.00054  \n",
       "              RE.PA            0.00182  \n",
       "              SAVE.PA          0.00358  \n",
       "              TEP.PA          -0.00357  \n",
       "\n",
       "[10 rows x 53 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_work=df_work.droplevel('CODE') !!!!!!\n",
    "df_work.sort_index(inplace=True)\n",
    "df_work[10000:10010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected: df_selected.shape=(844051, 29) valid: df_valid.shape=(233081, 29) confirm: df_confirm.shape=(247146, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_sma20</th>\n",
       "      <th>pos_sma50</th>\n",
       "      <th>pos_sma200</th>\n",
       "      <th>rsi14</th>\n",
       "      <th>sma20_rsi14</th>\n",
       "      <th>ret_5d</th>\n",
       "      <th>pos_top20</th>\n",
       "      <th>pos_top50</th>\n",
       "      <th>pos_bot20</th>\n",
       "      <th>pos_bot50</th>\n",
       "      <th>...</th>\n",
       "      <th>cmf_20</th>\n",
       "      <th>adx14</th>\n",
       "      <th>adx14_neg</th>\n",
       "      <th>adx14_pos</th>\n",
       "      <th>adx14_dif</th>\n",
       "      <th>pos_avg_vol14</th>\n",
       "      <th>pos_sma20_200</th>\n",
       "      <th>perf_sma_50_5d</th>\n",
       "      <th>perf_sma_200_5d</th>\n",
       "      <th>TICKER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPEN_DATETIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-01-04</th>\n",
       "      <td>0.03032</td>\n",
       "      <td>0.04450</td>\n",
       "      <td>0.04734</td>\n",
       "      <td>68.61368</td>\n",
       "      <td>57.45887</td>\n",
       "      <td>0.02203</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06381</td>\n",
       "      <td>0.10456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.68106</td>\n",
       "      <td>34.55900</td>\n",
       "      <td>16.30154</td>\n",
       "      <td>32.80825</td>\n",
       "      <td>16.50671</td>\n",
       "      <td>1.43364</td>\n",
       "      <td>0.01652</td>\n",
       "      <td>0.01085</td>\n",
       "      <td>0.00350</td>\n",
       "      <td>SAVE.PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-04</th>\n",
       "      <td>0.00911</td>\n",
       "      <td>-0.01411</td>\n",
       "      <td>-0.06019</td>\n",
       "      <td>51.10742</td>\n",
       "      <td>48.38847</td>\n",
       "      <td>0.01840</td>\n",
       "      <td>-0.03493</td>\n",
       "      <td>-0.11954</td>\n",
       "      <td>0.05738</td>\n",
       "      <td>0.10499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00654</td>\n",
       "      <td>21.08814</td>\n",
       "      <td>30.49097</td>\n",
       "      <td>26.14443</td>\n",
       "      <td>-4.34654</td>\n",
       "      <td>0.10186</td>\n",
       "      <td>-0.06867</td>\n",
       "      <td>-0.01641</td>\n",
       "      <td>-0.00343</td>\n",
       "      <td>TEP.PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-04</th>\n",
       "      <td>0.05060</td>\n",
       "      <td>0.03251</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>58.77622</td>\n",
       "      <td>45.64184</td>\n",
       "      <td>0.03184</td>\n",
       "      <td>-0.02624</td>\n",
       "      <td>-0.02624</td>\n",
       "      <td>0.10498</td>\n",
       "      <td>0.10498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11007</td>\n",
       "      <td>19.47531</td>\n",
       "      <td>27.82168</td>\n",
       "      <td>32.68546</td>\n",
       "      <td>4.86378</td>\n",
       "      <td>1.73709</td>\n",
       "      <td>0.11336</td>\n",
       "      <td>0.00850</td>\n",
       "      <td>0.01067</td>\n",
       "      <td>TFI.PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-04</th>\n",
       "      <td>0.01749</td>\n",
       "      <td>-0.03933</td>\n",
       "      <td>0.04489</td>\n",
       "      <td>49.29749</td>\n",
       "      <td>38.37917</td>\n",
       "      <td>0.04334</td>\n",
       "      <td>-0.04469</td>\n",
       "      <td>-0.10454</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.24571</td>\n",
       "      <td>22.52874</td>\n",
       "      <td>21.82438</td>\n",
       "      <td>25.90113</td>\n",
       "      <td>4.07675</td>\n",
       "      <td>1.35506</td>\n",
       "      <td>0.02694</td>\n",
       "      <td>-0.00896</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>VIRP.PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>0.01285</td>\n",
       "      <td>0.00631</td>\n",
       "      <td>-0.00376</td>\n",
       "      <td>52.83213</td>\n",
       "      <td>49.86578</td>\n",
       "      <td>-0.00893</td>\n",
       "      <td>-0.02243</td>\n",
       "      <td>-0.03173</td>\n",
       "      <td>0.04537</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14429</td>\n",
       "      <td>14.92596</td>\n",
       "      <td>28.76524</td>\n",
       "      <td>30.40562</td>\n",
       "      <td>1.64038</td>\n",
       "      <td>0.50658</td>\n",
       "      <td>-0.01641</td>\n",
       "      <td>0.00690</td>\n",
       "      <td>0.00293</td>\n",
       "      <td>BN.PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>0.03801</td>\n",
       "      <td>0.02653</td>\n",
       "      <td>0.10581</td>\n",
       "      <td>61.28371</td>\n",
       "      <td>49.09160</td>\n",
       "      <td>0.03733</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.03275</td>\n",
       "      <td>0.05942</td>\n",
       "      <td>0.07388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50814</td>\n",
       "      <td>29.47521</td>\n",
       "      <td>17.54691</td>\n",
       "      <td>16.77613</td>\n",
       "      <td>-0.77078</td>\n",
       "      <td>0.12624</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>-0.00504</td>\n",
       "      <td>0.00746</td>\n",
       "      <td>BOI.PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>0.03567</td>\n",
       "      <td>0.12029</td>\n",
       "      <td>0.28952</td>\n",
       "      <td>66.79495</td>\n",
       "      <td>71.57781</td>\n",
       "      <td>0.00596</td>\n",
       "      <td>-0.01651</td>\n",
       "      <td>-0.01651</td>\n",
       "      <td>0.11823</td>\n",
       "      <td>0.22153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.42051</td>\n",
       "      <td>41.67504</td>\n",
       "      <td>14.69324</td>\n",
       "      <td>43.10116</td>\n",
       "      <td>28.40792</td>\n",
       "      <td>0.71424</td>\n",
       "      <td>0.24511</td>\n",
       "      <td>0.02108</td>\n",
       "      <td>0.01041</td>\n",
       "      <td>CDI.PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>-0.00497</td>\n",
       "      <td>-0.04962</td>\n",
       "      <td>-0.10276</td>\n",
       "      <td>44.68482</td>\n",
       "      <td>41.48714</td>\n",
       "      <td>0.00438</td>\n",
       "      <td>-0.04037</td>\n",
       "      <td>-0.11778</td>\n",
       "      <td>0.00438</td>\n",
       "      <td>0.00438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05534</td>\n",
       "      <td>10.20240</td>\n",
       "      <td>38.68822</td>\n",
       "      <td>47.13757</td>\n",
       "      <td>8.44935</td>\n",
       "      <td>1.14001</td>\n",
       "      <td>-0.09827</td>\n",
       "      <td>-0.00897</td>\n",
       "      <td>0.00085</td>\n",
       "      <td>ELEC.PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>0.01013</td>\n",
       "      <td>0.02410</td>\n",
       "      <td>0.19591</td>\n",
       "      <td>57.52017</td>\n",
       "      <td>55.21953</td>\n",
       "      <td>0.01443</td>\n",
       "      <td>-0.01506</td>\n",
       "      <td>-0.05496</td>\n",
       "      <td>0.04269</td>\n",
       "      <td>0.07580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01669</td>\n",
       "      <td>26.80769</td>\n",
       "      <td>14.80556</td>\n",
       "      <td>34.98566</td>\n",
       "      <td>20.18010</td>\n",
       "      <td>0.10965</td>\n",
       "      <td>0.18391</td>\n",
       "      <td>0.00753</td>\n",
       "      <td>0.01116</td>\n",
       "      <td>GFC.PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>0.06142</td>\n",
       "      <td>0.06319</td>\n",
       "      <td>0.01671</td>\n",
       "      <td>63.83981</td>\n",
       "      <td>49.25421</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.17008</td>\n",
       "      <td>0.17008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08836</td>\n",
       "      <td>15.60680</td>\n",
       "      <td>35.58655</td>\n",
       "      <td>52.39946</td>\n",
       "      <td>16.81291</td>\n",
       "      <td>0.95004</td>\n",
       "      <td>-0.04213</td>\n",
       "      <td>0.00509</td>\n",
       "      <td>-0.00040</td>\n",
       "      <td>LAT.PA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pos_sma20  pos_sma50  pos_sma200     rsi14  sma20_rsi14  \\\n",
       "OPEN_DATETIME                                                            \n",
       "1996-01-04       0.03032    0.04450     0.04734  68.61368     57.45887   \n",
       "1996-01-04       0.00911   -0.01411    -0.06019  51.10742     48.38847   \n",
       "1996-01-04       0.05060    0.03251     0.16970  58.77622     45.64184   \n",
       "1996-01-04       0.01749   -0.03933     0.04489  49.29749     38.37917   \n",
       "1996-01-05       0.01285    0.00631    -0.00376  52.83213     49.86578   \n",
       "1996-01-05       0.03801    0.02653     0.10581  61.28371     49.09160   \n",
       "1996-01-05       0.03567    0.12029     0.28952  66.79495     71.57781   \n",
       "1996-01-05      -0.00497   -0.04962    -0.10276  44.68482     41.48714   \n",
       "1996-01-05       0.01013    0.02410     0.19591  57.52017     55.21953   \n",
       "1996-01-05       0.06142    0.06319     0.01671  63.83981     49.25421   \n",
       "\n",
       "                ret_5d  pos_top20  pos_top50  pos_bot20  pos_bot50  ...  \\\n",
       "OPEN_DATETIME                                                       ...   \n",
       "1996-01-04     0.02203    0.00000    0.00000    0.06381    0.10456  ...   \n",
       "1996-01-04     0.01840   -0.03493   -0.11954    0.05738    0.10499  ...   \n",
       "1996-01-04     0.03184   -0.02624   -0.02624    0.10498    0.10498  ...   \n",
       "1996-01-04     0.04334   -0.04469   -0.10454    0.06570    0.06570  ...   \n",
       "1996-01-05    -0.00893   -0.02243   -0.03173    0.04537    0.05623  ...   \n",
       "1996-01-05     0.03733    0.00000   -0.03275    0.05942    0.07388  ...   \n",
       "1996-01-05     0.00596   -0.01651   -0.01651    0.11823    0.22153  ...   \n",
       "1996-01-05     0.00438   -0.04037   -0.11778    0.00438    0.00438  ...   \n",
       "1996-01-05     0.01443   -0.01506   -0.05496    0.04269    0.07580  ...   \n",
       "1996-01-05     0.00000    0.00000    0.00000    0.17008    0.17008  ...   \n",
       "\n",
       "                cmf_20     adx14  adx14_neg  adx14_pos  adx14_dif  \\\n",
       "OPEN_DATETIME                                                       \n",
       "1996-01-04     0.68106  34.55900   16.30154   32.80825   16.50671   \n",
       "1996-01-04    -0.00654  21.08814   30.49097   26.14443   -4.34654   \n",
       "1996-01-04     0.11007  19.47531   27.82168   32.68546    4.86378   \n",
       "1996-01-04     0.24571  22.52874   21.82438   25.90113    4.07675   \n",
       "1996-01-05     0.14429  14.92596   28.76524   30.40562    1.64038   \n",
       "1996-01-05     0.50814  29.47521   17.54691   16.77613   -0.77078   \n",
       "1996-01-05     0.42051  41.67504   14.69324   43.10116   28.40792   \n",
       "1996-01-05     0.05534  10.20240   38.68822   47.13757    8.44935   \n",
       "1996-01-05    -0.01669  26.80769   14.80556   34.98566   20.18010   \n",
       "1996-01-05     0.08836  15.60680   35.58655   52.39946   16.81291   \n",
       "\n",
       "               pos_avg_vol14  pos_sma20_200  perf_sma_50_5d  perf_sma_200_5d  \\\n",
       "OPEN_DATETIME                                                                  \n",
       "1996-01-04           1.43364        0.01652         0.01085          0.00350   \n",
       "1996-01-04           0.10186       -0.06867        -0.01641         -0.00343   \n",
       "1996-01-04           1.73709        0.11336         0.00850          0.01067   \n",
       "1996-01-04           1.35506        0.02694        -0.00896          0.00673   \n",
       "1996-01-05           0.50658       -0.01641         0.00690          0.00293   \n",
       "1996-01-05           0.12624        0.06531        -0.00504          0.00746   \n",
       "1996-01-05           0.71424        0.24511         0.02108          0.01041   \n",
       "1996-01-05           1.14001       -0.09827        -0.00897          0.00085   \n",
       "1996-01-05           0.10965        0.18391         0.00753          0.01116   \n",
       "1996-01-05           0.95004       -0.04213         0.00509         -0.00040   \n",
       "\n",
       "                TICKER  \n",
       "OPEN_DATETIME           \n",
       "1996-01-04     SAVE.PA  \n",
       "1996-01-04      TEP.PA  \n",
       "1996-01-04      TFI.PA  \n",
       "1996-01-04     VIRP.PA  \n",
       "1996-01-05       BN.PA  \n",
       "1996-01-05      BOI.PA  \n",
       "1996-01-05      CDI.PA  \n",
       "1996-01-05     ELEC.PA  \n",
       "1996-01-05      GFC.PA  \n",
       "1996-01-05      LAT.PA  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_studied = \"lab_perf_20d\"\n",
    "algo_studied = \"LSTM_CLASS\"\n",
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "\n",
    "df_work_lab = indic.drop_indicators_not_selected(con=con_fwk, df_in=df_work, dts_name=dts_name, symbol=multi_symbol,label=lab_studied,algo=algo_studied)\n",
    "# print(df_work_lab.head(5))\n",
    "\n",
    "# move CODE to column to be able to slit the dataset\n",
    "df_work_lab['TICKER'] = df_work_lab.index.get_level_values('CODE')\n",
    "df_work_lab=df_work_lab.droplevel('CODE')\n",
    "\n",
    "df_split=sm.split_df_by_label_strat(\n",
    "    df_in=df_work_lab, list_label=[lab_studied], split_timeframe=\"M\",random_split=False,split_strat=(80,10,10))\n",
    "df_selected = df_split['df_'+lab_studied+'_train']\n",
    "df_valid = df_split['df_'+lab_studied+'_valid']\n",
    "df_confirm = df_split['df_'+lab_studied+'_confirm']\n",
    "df_selected.sort_index(inplace=True)\n",
    "df_valid.sort_index(inplace=True)\n",
    "df_confirm.sort_index(inplace=True)\n",
    "\n",
    "print(f\"selected: {df_selected.shape=} valid: {df_valid.shape=} confirm: {df_confirm.shape=}\")\n",
    "df_selected[10000:10010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        min      max\n",
      "lab_perf_20d_class                  \n",
      "0                  -0.80165 -0.05202\n",
      "1                  -0.05201 -0.00892\n",
      "2                  -0.00891  0.02358\n",
      "3                   0.02359  0.07135\n",
      "4                   0.07136  3.82176\n"
     ]
    }
   ],
   "source": [
    "label=lab_studied\n",
    "df_class=balance.add_class_by_lab_nb_lines(df_in=df_selected,str_label=lab_studied,nb_class=5,bool_replace_label=False)\n",
    "min_max_lab_by_class = df_class.groupby(label+'_class')[label].agg(['min', 'max'])\n",
    "print(min_max_lab_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPEN_DATETIME\n",
      "1989-10-27    4\n",
      "2017-02-28    4\n",
      "Name: lab_perf_20d, dtype: int64\n",
      "OPEN_DATETIME\n",
      "2017-03-01    4.0\n",
      "2020-07-31    4.0\n",
      "Name: lab_perf_20d, dtype: float64\n",
      "OPEN_DATETIME\n",
      "2020-08-03    3.0\n",
      "2023-12-13    2.0\n",
      "Name: lab_perf_20d, dtype: float64\n",
      "lab_perf_20d\n",
      "0    168828\n",
      "1    168821\n",
      "2    168810\n",
      "3    168787\n",
      "4    168805\n",
      "Name: count, dtype: int64\n",
      "lab_perf_20d\n",
      "0.0    52773\n",
      "1.0    48096\n",
      "2.0    45225\n",
      "3.0    44833\n",
      "4.0    42132\n",
      "Name: count, dtype: int64\n",
      "lab_perf_20d\n",
      "0.0    61234\n",
      "1.0    48511\n",
      "2.0    42878\n",
      "3.0    44301\n",
      "4.0    50215\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_sma20</th>\n",
       "      <th>pos_sma50</th>\n",
       "      <th>pos_sma200</th>\n",
       "      <th>rsi14</th>\n",
       "      <th>sma20_rsi14</th>\n",
       "      <th>ret_5d</th>\n",
       "      <th>pos_top20</th>\n",
       "      <th>pos_top50</th>\n",
       "      <th>pos_bot20</th>\n",
       "      <th>pos_bot50</th>\n",
       "      <th>...</th>\n",
       "      <th>adx14</th>\n",
       "      <th>adx14_neg</th>\n",
       "      <th>adx14_pos</th>\n",
       "      <th>adx14_dif</th>\n",
       "      <th>pos_avg_vol14</th>\n",
       "      <th>pos_sma20_200</th>\n",
       "      <th>perf_sma_50_5d</th>\n",
       "      <th>perf_sma_200_5d</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>lab_perf_20d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPEN_DATETIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-01-04</th>\n",
       "      <td>0.03032</td>\n",
       "      <td>0.04450</td>\n",
       "      <td>0.04734</td>\n",
       "      <td>68.61368</td>\n",
       "      <td>57.45887</td>\n",
       "      <td>0.02203</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06381</td>\n",
       "      <td>0.10456</td>\n",
       "      <td>...</td>\n",
       "      <td>34.55900</td>\n",
       "      <td>16.30154</td>\n",
       "      <td>32.80825</td>\n",
       "      <td>16.50671</td>\n",
       "      <td>1.43364</td>\n",
       "      <td>0.01652</td>\n",
       "      <td>0.01085</td>\n",
       "      <td>0.00350</td>\n",
       "      <td>SAVE.PA</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-04</th>\n",
       "      <td>0.00911</td>\n",
       "      <td>-0.01411</td>\n",
       "      <td>-0.06019</td>\n",
       "      <td>51.10742</td>\n",
       "      <td>48.38847</td>\n",
       "      <td>0.01840</td>\n",
       "      <td>-0.03493</td>\n",
       "      <td>-0.11954</td>\n",
       "      <td>0.05738</td>\n",
       "      <td>0.10499</td>\n",
       "      <td>...</td>\n",
       "      <td>21.08814</td>\n",
       "      <td>30.49097</td>\n",
       "      <td>26.14443</td>\n",
       "      <td>-4.34654</td>\n",
       "      <td>0.10186</td>\n",
       "      <td>-0.06867</td>\n",
       "      <td>-0.01641</td>\n",
       "      <td>-0.00343</td>\n",
       "      <td>TEP.PA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-04</th>\n",
       "      <td>0.05060</td>\n",
       "      <td>0.03251</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>58.77622</td>\n",
       "      <td>45.64184</td>\n",
       "      <td>0.03184</td>\n",
       "      <td>-0.02624</td>\n",
       "      <td>-0.02624</td>\n",
       "      <td>0.10498</td>\n",
       "      <td>0.10498</td>\n",
       "      <td>...</td>\n",
       "      <td>19.47531</td>\n",
       "      <td>27.82168</td>\n",
       "      <td>32.68546</td>\n",
       "      <td>4.86378</td>\n",
       "      <td>1.73709</td>\n",
       "      <td>0.11336</td>\n",
       "      <td>0.00850</td>\n",
       "      <td>0.01067</td>\n",
       "      <td>TFI.PA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-04</th>\n",
       "      <td>0.01749</td>\n",
       "      <td>-0.03933</td>\n",
       "      <td>0.04489</td>\n",
       "      <td>49.29749</td>\n",
       "      <td>38.37917</td>\n",
       "      <td>0.04334</td>\n",
       "      <td>-0.04469</td>\n",
       "      <td>-0.10454</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>...</td>\n",
       "      <td>22.52874</td>\n",
       "      <td>21.82438</td>\n",
       "      <td>25.90113</td>\n",
       "      <td>4.07675</td>\n",
       "      <td>1.35506</td>\n",
       "      <td>0.02694</td>\n",
       "      <td>-0.00896</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>VIRP.PA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>0.01285</td>\n",
       "      <td>0.00631</td>\n",
       "      <td>-0.00376</td>\n",
       "      <td>52.83213</td>\n",
       "      <td>49.86578</td>\n",
       "      <td>-0.00893</td>\n",
       "      <td>-0.02243</td>\n",
       "      <td>-0.03173</td>\n",
       "      <td>0.04537</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>14.92596</td>\n",
       "      <td>28.76524</td>\n",
       "      <td>30.40562</td>\n",
       "      <td>1.64038</td>\n",
       "      <td>0.50658</td>\n",
       "      <td>-0.01641</td>\n",
       "      <td>0.00690</td>\n",
       "      <td>0.00293</td>\n",
       "      <td>BN.PA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>0.03801</td>\n",
       "      <td>0.02653</td>\n",
       "      <td>0.10581</td>\n",
       "      <td>61.28371</td>\n",
       "      <td>49.09160</td>\n",
       "      <td>0.03733</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.03275</td>\n",
       "      <td>0.05942</td>\n",
       "      <td>0.07388</td>\n",
       "      <td>...</td>\n",
       "      <td>29.47521</td>\n",
       "      <td>17.54691</td>\n",
       "      <td>16.77613</td>\n",
       "      <td>-0.77078</td>\n",
       "      <td>0.12624</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>-0.00504</td>\n",
       "      <td>0.00746</td>\n",
       "      <td>BOI.PA</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>0.03567</td>\n",
       "      <td>0.12029</td>\n",
       "      <td>0.28952</td>\n",
       "      <td>66.79495</td>\n",
       "      <td>71.57781</td>\n",
       "      <td>0.00596</td>\n",
       "      <td>-0.01651</td>\n",
       "      <td>-0.01651</td>\n",
       "      <td>0.11823</td>\n",
       "      <td>0.22153</td>\n",
       "      <td>...</td>\n",
       "      <td>41.67504</td>\n",
       "      <td>14.69324</td>\n",
       "      <td>43.10116</td>\n",
       "      <td>28.40792</td>\n",
       "      <td>0.71424</td>\n",
       "      <td>0.24511</td>\n",
       "      <td>0.02108</td>\n",
       "      <td>0.01041</td>\n",
       "      <td>CDI.PA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>-0.00497</td>\n",
       "      <td>-0.04962</td>\n",
       "      <td>-0.10276</td>\n",
       "      <td>44.68482</td>\n",
       "      <td>41.48714</td>\n",
       "      <td>0.00438</td>\n",
       "      <td>-0.04037</td>\n",
       "      <td>-0.11778</td>\n",
       "      <td>0.00438</td>\n",
       "      <td>0.00438</td>\n",
       "      <td>...</td>\n",
       "      <td>10.20240</td>\n",
       "      <td>38.68822</td>\n",
       "      <td>47.13757</td>\n",
       "      <td>8.44935</td>\n",
       "      <td>1.14001</td>\n",
       "      <td>-0.09827</td>\n",
       "      <td>-0.00897</td>\n",
       "      <td>0.00085</td>\n",
       "      <td>ELEC.PA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>0.01013</td>\n",
       "      <td>0.02410</td>\n",
       "      <td>0.19591</td>\n",
       "      <td>57.52017</td>\n",
       "      <td>55.21953</td>\n",
       "      <td>0.01443</td>\n",
       "      <td>-0.01506</td>\n",
       "      <td>-0.05496</td>\n",
       "      <td>0.04269</td>\n",
       "      <td>0.07580</td>\n",
       "      <td>...</td>\n",
       "      <td>26.80769</td>\n",
       "      <td>14.80556</td>\n",
       "      <td>34.98566</td>\n",
       "      <td>20.18010</td>\n",
       "      <td>0.10965</td>\n",
       "      <td>0.18391</td>\n",
       "      <td>0.00753</td>\n",
       "      <td>0.01116</td>\n",
       "      <td>GFC.PA</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>0.06142</td>\n",
       "      <td>0.06319</td>\n",
       "      <td>0.01671</td>\n",
       "      <td>63.83981</td>\n",
       "      <td>49.25421</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.17008</td>\n",
       "      <td>0.17008</td>\n",
       "      <td>...</td>\n",
       "      <td>15.60680</td>\n",
       "      <td>35.58655</td>\n",
       "      <td>52.39946</td>\n",
       "      <td>16.81291</td>\n",
       "      <td>0.95004</td>\n",
       "      <td>-0.04213</td>\n",
       "      <td>0.00509</td>\n",
       "      <td>-0.00040</td>\n",
       "      <td>LAT.PA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pos_sma20  pos_sma50  pos_sma200     rsi14  sma20_rsi14  \\\n",
       "OPEN_DATETIME                                                            \n",
       "1996-01-04       0.03032    0.04450     0.04734  68.61368     57.45887   \n",
       "1996-01-04       0.00911   -0.01411    -0.06019  51.10742     48.38847   \n",
       "1996-01-04       0.05060    0.03251     0.16970  58.77622     45.64184   \n",
       "1996-01-04       0.01749   -0.03933     0.04489  49.29749     38.37917   \n",
       "1996-01-05       0.01285    0.00631    -0.00376  52.83213     49.86578   \n",
       "1996-01-05       0.03801    0.02653     0.10581  61.28371     49.09160   \n",
       "1996-01-05       0.03567    0.12029     0.28952  66.79495     71.57781   \n",
       "1996-01-05      -0.00497   -0.04962    -0.10276  44.68482     41.48714   \n",
       "1996-01-05       0.01013    0.02410     0.19591  57.52017     55.21953   \n",
       "1996-01-05       0.06142    0.06319     0.01671  63.83981     49.25421   \n",
       "\n",
       "                ret_5d  pos_top20  pos_top50  pos_bot20  pos_bot50  ...  \\\n",
       "OPEN_DATETIME                                                       ...   \n",
       "1996-01-04     0.02203    0.00000    0.00000    0.06381    0.10456  ...   \n",
       "1996-01-04     0.01840   -0.03493   -0.11954    0.05738    0.10499  ...   \n",
       "1996-01-04     0.03184   -0.02624   -0.02624    0.10498    0.10498  ...   \n",
       "1996-01-04     0.04334   -0.04469   -0.10454    0.06570    0.06570  ...   \n",
       "1996-01-05    -0.00893   -0.02243   -0.03173    0.04537    0.05623  ...   \n",
       "1996-01-05     0.03733    0.00000   -0.03275    0.05942    0.07388  ...   \n",
       "1996-01-05     0.00596   -0.01651   -0.01651    0.11823    0.22153  ...   \n",
       "1996-01-05     0.00438   -0.04037   -0.11778    0.00438    0.00438  ...   \n",
       "1996-01-05     0.01443   -0.01506   -0.05496    0.04269    0.07580  ...   \n",
       "1996-01-05     0.00000    0.00000    0.00000    0.17008    0.17008  ...   \n",
       "\n",
       "                  adx14  adx14_neg  adx14_pos  adx14_dif  pos_avg_vol14  \\\n",
       "OPEN_DATETIME                                                             \n",
       "1996-01-04     34.55900   16.30154   32.80825   16.50671        1.43364   \n",
       "1996-01-04     21.08814   30.49097   26.14443   -4.34654        0.10186   \n",
       "1996-01-04     19.47531   27.82168   32.68546    4.86378        1.73709   \n",
       "1996-01-04     22.52874   21.82438   25.90113    4.07675        1.35506   \n",
       "1996-01-05     14.92596   28.76524   30.40562    1.64038        0.50658   \n",
       "1996-01-05     29.47521   17.54691   16.77613   -0.77078        0.12624   \n",
       "1996-01-05     41.67504   14.69324   43.10116   28.40792        0.71424   \n",
       "1996-01-05     10.20240   38.68822   47.13757    8.44935        1.14001   \n",
       "1996-01-05     26.80769   14.80556   34.98566   20.18010        0.10965   \n",
       "1996-01-05     15.60680   35.58655   52.39946   16.81291        0.95004   \n",
       "\n",
       "               pos_sma20_200  perf_sma_50_5d  perf_sma_200_5d   TICKER  \\\n",
       "OPEN_DATETIME                                                            \n",
       "1996-01-04           0.01652         0.01085          0.00350  SAVE.PA   \n",
       "1996-01-04          -0.06867        -0.01641         -0.00343   TEP.PA   \n",
       "1996-01-04           0.11336         0.00850          0.01067   TFI.PA   \n",
       "1996-01-04           0.02694        -0.00896          0.00673  VIRP.PA   \n",
       "1996-01-05          -0.01641         0.00690          0.00293    BN.PA   \n",
       "1996-01-05           0.06531        -0.00504          0.00746   BOI.PA   \n",
       "1996-01-05           0.24511         0.02108          0.01041   CDI.PA   \n",
       "1996-01-05          -0.09827        -0.00897          0.00085  ELEC.PA   \n",
       "1996-01-05           0.18391         0.00753          0.01116   GFC.PA   \n",
       "1996-01-05          -0.04213         0.00509         -0.00040   LAT.PA   \n",
       "\n",
       "               lab_perf_20d  \n",
       "OPEN_DATETIME                \n",
       "1996-01-04                3  \n",
       "1996-01-04                2  \n",
       "1996-01-04                2  \n",
       "1996-01-04                4  \n",
       "1996-01-05                2  \n",
       "1996-01-05                3  \n",
       "1996-01-05                4  \n",
       "1996-01-05                2  \n",
       "1996-01-05                3  \n",
       "1996-01-05                4  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label=lab_studied\n",
    "df_class=balance.add_class_by_lab_nb_lines(df_in=df_selected,str_label=lab_studied,nb_class=5,bool_replace_label=True)\n",
    "df_class.sort_index(inplace=True)\n",
    "# categ_50={0:[-1,-0.05456],1:[-0.07876,-0.00783],2:[-0.00783,0.04790],3:[0.04790,0.12406],4:[0.12406,6]}\n",
    "categ_20={0:[-1,-0.0520],1:[-0.0520,-0.0089],2:[-0.0089,0.0235],3:[0.0235,0.0713],4:[0.0713,4]}\n",
    "df_class_val=balance.add_lab_by_class(df_in=df_valid,str_label=lab_studied, categ=categ_20,bool_replace_label=True) # categ\n",
    "df_class_val.sort_index(inplace=True)\n",
    "df_class_conf=balance.add_lab_by_class(df_in=df_confirm,str_label=lab_studied, categ=categ_20,bool_replace_label=True) # categ\n",
    "df_class_conf.sort_index(inplace=True)\n",
    "print(df_class.loc[:, label].dropna().iloc[[0, -1]])\n",
    "print(df_class_val.loc[:, label].dropna().iloc[[0, -1]])\n",
    "print(df_class_conf.loc[:, label].dropna().iloc[[0, -1]])\n",
    "# df_class_clean=df_class.drop(['OPEN','HIGH','LOW','CLOSE','VOLUME','lab_perf_125d','lab_perf_20d','lab_perf_50d'],axis=1)\n",
    "data = df_class[label]\n",
    "print(data.value_counts().sort_index())\n",
    "data_val = df_class_val[label]\n",
    "print(data_val.value_counts().sort_index())\n",
    "data_conf = df_class_conf[label]\n",
    "print(data_conf.value_counts().sort_index())\n",
    "df_class[10000:10010]\n",
    "# min_max_lab_by_class = df_class.groupby(label+'_class')[label].agg(['min', 'max'])\n",
    "# print(min_max_lab_by_class)\n",
    "\n",
    "# lab_perf_20d : train min nb rows 211000 validation 53000 confirm 55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SAVE DATASETS\n",
    "\n",
    "df_class.round(5).to_csv(\n",
    "    PATH_DATA_DTS+dts_name+SUFFIX_TRAIN, sep=\",\")\n",
    "df_class_val.round(5).to_csv(\n",
    "    PATH_DATA_DTS+dts_name+SUFFIX_VAL, sep=\",\")\n",
    "df_class_conf.round(5).to_csv(\n",
    "    PATH_DATA_DTS+dts_name+SUFFIX_CONF, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and save scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Projets\\\\Data\\\\DTS_FULL\\\\PARIS_TREND_1D_20D_V2_train_colab_lstm_norm_2405_scaler.save']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_20d\"\n",
    "\n",
    "df_class=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_TRAIN,sep=\",\",index_col=[\"OPEN_DATETIME\",\"TICKER\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class=df_class.dropna(subset=[label])\n",
    "df_class=df_class.sort_index()\n",
    "\n",
    "df_norm,norm_scaler= balance.normalize_df(df_in=df_class,str_label=label,tuple_ft_range=(-1,1))\n",
    "\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2405\"\n",
    "scaler_name=file_name+\"_scaler.save\"\n",
    "joblib.dump(norm_scaler,filename=PATH_DATA_DTS+scaler_name)\n",
    "\n",
    "# df_class_val=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_VAL,sep=\",\",index_col=[\"OPEN_DATETIME\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "# df_class_val.dropna(subset=[label], inplace=True)\n",
    "# df_class_val.sort_index(inplace=True)\n",
    "\n",
    "# list_feat = df_class.columns.values.tolist()\n",
    "# list_feat.remove(label)\n",
    "# X, y = sm.split_df_x_y(\n",
    "#     df_in=df_class, list_features=list_feat, str_label=label, drop_na=True)\n",
    "# nb_val=211000\n",
    "# method = RandomUnderSampler(sampling_strategy={0:nb_val,1:nb_val,2:nb_val,3:nb_val}) \n",
    "# df_x_train, col_y_train=  method.fit_resample(X, y)\n",
    "# print(col_y_train.value_counts().sort_index())\n",
    "\n",
    "# X, y = sm.split_df_x_y(\n",
    "#     df_in=df_class_val, list_features=list_feat, str_label=label, drop_na=True)\n",
    "# nb_val=53000\n",
    "# method = RandomUnderSampler(sampling_strategy={0:nb_val,1:nb_val,2:nb_val,3:nb_val}) # 53000 pour lab 20 et nn pour lab 50\n",
    "# df_x_val, col_y_val=  method.fit_resample(X, y)\n",
    "# print(col_y_val.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train et val df, normalize,  undersample  and preparation for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########  SAVE   ##########\n",
    "#############################\n",
    "\n",
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_20d\"\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2405\"\n",
    "scaler_name=file_name+\"_scaler.save\"\n",
    "scaler=joblib.load(PATH_DATA_DTS+scaler_name)\n",
    "\n",
    "\n",
    "df_class=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_TRAIN,sep=\",\",index_col=[\"OPEN_DATETIME\",\"TICKER\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class=df_class.dropna(subset=[label])\n",
    "df_class=df_class.loc['1995-01-01':] # drop rows < 1995-01-01\n",
    "df_class=df_class.sort_index()\n",
    "df_class_val=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_VAL,sep=\",\",index_col=[\"OPEN_DATETIME\",\"TICKER\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class_val=df_class_val.dropna(subset=[label])\n",
    "df_class_val=df_class_val.sort_index()\n",
    "\n",
    "# normalize df_class and df_class_val\n",
    "df_class_train_norm=balance.normalize_df_scaler(df_in=df_class, str_label=label,scaler=scaler)\n",
    "df_class_val_norm=balance.normalize_df_scaler(df_in=df_class_val, str_label=label,scaler=scaler)\n",
    "\n",
    "list_feat = df_class.columns.values.tolist()\n",
    "list_feat.remove(label)\n",
    "\n",
    "df_x_train, col_y_train = sm.split_df_x_y(\n",
    "    df_in=df_class_train_norm, list_features=list_feat, str_label=label, drop_na=True)\n",
    "# nb_val=100000#208000\n",
    "# method = RandomUnderSampler(sampling_strategy={0:nb_val,1:nb_val,2:nb_val,3:nb_val}) \n",
    "# df_x_train, col_y_train=  method.fit_resample(X, y)\n",
    "# print(col_y_train.value_counts().sort_index())\n",
    "\n",
    "df_x_val, col_y_val = sm.split_df_x_y(\n",
    "    df_in=df_class_val_norm, list_features=list_feat, str_label=label, drop_na=True)\n",
    "# nb_val=25000#53000\n",
    "# method = RandomUnderSampler(sampling_strategy={0:nb_val,1:nb_val,2:nb_val,3:nb_val}) # 53000 pour lab 20 et nn pour lab 50\n",
    "# df_x_val, col_y_val=  method.fit_resample(X, y)\n",
    "# print(col_y_val.value_counts().sort_index())\n",
    "\n",
    "sequence_length = 10\n",
    "\n",
    "x_train=df_x_train.values\n",
    "y_train=col_y_train.values\n",
    "x_val=df_x_val.values\n",
    "y_val=col_y_val.values\n",
    "x_train_lstm,y_train_lstm=sm.prepare_sequences(x_train,y_train,sequence_length)\n",
    "x_val_lstm,y_val_lstm=sm.prepare_sequences(x_val,y_val,sequence_length)\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train_lstm, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train_lstm, dtype=torch.float)\n",
    "x_val_tensor = torch.tensor(x_val_lstm, dtype=torch.float)\n",
    "y_val_tensor = torch.tensor(y_val_lstm, dtype=torch.float)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "# initiate a pytorch randomsampler for train data\n",
    "train_sampler = RandomSampler(train_dataset,num_samples=100000,replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False,sampler=train_sampler,drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512,drop_last=True)\n",
    "\n",
    "#######################################\n",
    "### SAVE SEQUENCE MANIPULATION ########\n",
    "\n",
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_20d\"\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2405\"\n",
    "col_sequence = \"SEQUENCE\"\n",
    "df_class_train_csv=pd.read_csv(PATH_DATA_DTS+dts_name+\"_TRAIN_seq_6.zip\",sep=\",\",index_col=[\"TICKER\",\"OPEN_DATETIME\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class_train_csv=df_class_train_csv.dropna(subset=[col_sequence])\n",
    "df_class_train_csv=df_class_train_csv.sort_index()\n",
    "df_class_val_csv=pd.read_csv(PATH_DATA_DTS+dts_name+\"_VAL_seq_6.zip\",sep=\",\",index_col=[\"TICKER\",\"OPEN_DATETIME\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class_val_csv=df_class_val_csv.dropna(subset=[col_sequence])\n",
    "df_class_val_csv=df_class_val_csv.sort_index()\n",
    "\n",
    "# keep only index, label and sequence\n",
    "df_class_train_csv=df_class_train_csv[[label,col_sequence]]\n",
    "df_class_val_csv=df_class_val_csv[[label,col_sequence]]\n",
    "\n",
    "# TODO !!!!!!!!!!!\n",
    "# !!!!!!!!!!!!!\n",
    "# df_class_train_csv['col_sequence_2'] = df_class_train_csv[col_sequence].str.replace(\"_\", \",\").apply(ast.literal_eval)\n",
    "# df_class_train_csv['col_sequence_3']  = df_class_train_csv['col_sequence_2'] .apply(lambda x: np.array(x, dtype=np.float32))\n",
    "# df_class_train_csv['col_sequence_2'] = df_class_train_csv[col_sequence].apply(lambda x: np.fromstring(x.strip('[]'), sep='_'))\n",
    "\n",
    "df_class_val_csv['col_sequence_2'] = df_class_val_csv[col_sequence].str.replace(\"_\", \",\").apply(ast.literal_eval)\n",
    "df_class_val_csv['col_sequence_3']  = df_class_val_csv['col_sequence_2'] .apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "\n",
    "# print(f\"{df_class_train_csv.shape=}\")\n",
    "# print(df_class_train_csv[1005:1010])\n",
    "print(f\"{df_class_val_csv.shape=}\")\n",
    "print(df_class_val_csv[1015:1020])\n",
    "\n",
    "# decision is made between market sessions so we have shift the label of 1 day for each ticker\n",
    "df_class_val_csv[label] = df_class_val_csv.groupby(level='TICKER')[label].shift(1)\n",
    "df_class_val_csv=df_class_val_csv.dropna(subset=[label])\n",
    "print(df_class_val_csv[1014:1019])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_class_train_norm.shape=(837054, 28) df_class_val_norm.shape=(233059, 28)\n",
      "                       pos_sma20  pos_sma50  pos_sma200     rsi14  \\\n",
      "OPEN_DATETIME TICKER                                                \n",
      "1998-01-20    RE.PA    -0.401871  -0.530231   -0.504369  0.387174   \n",
      "              SAVE.PA  -0.429049  -0.551279   -0.530901  0.333374   \n",
      "              TEP.PA   -0.379269  -0.510884   -0.525588  0.509715   \n",
      "              TFI.PA   -0.396483  -0.499134   -0.470829  0.504751   \n",
      "              VIRP.PA  -0.414437  -0.515018   -0.598517  0.253438   \n",
      "\n",
      "                       sma20_rsi14    ret_5d  pos_top20  pos_top50  pos_bot20  \\\n",
      "OPEN_DATETIME TICKER                                                            \n",
      "1998-01-20    RE.PA       0.182813 -0.503245   0.983320   0.983966  -0.918870   \n",
      "              SAVE.PA     0.195811 -0.515616   0.980994   0.981731  -0.967591   \n",
      "              TEP.PA      0.198386 -0.495371   0.984695   0.985288  -0.902783   \n",
      "              TFI.PA      0.370092 -0.505919   1.000000   1.000000  -0.917227   \n",
      "              VIRP.PA     0.222042 -0.520613   0.919375   0.922500  -0.900381   \n",
      "\n",
      "                       pos_bot50  ...   cmf_20     adx14  adx14_neg  \\\n",
      "OPEN_DATETIME TICKER              ...                                 \n",
      "1998-01-20    RE.PA    -0.928958  ...  0.35220 -0.385375  -0.698540   \n",
      "              SAVE.PA  -0.917901  ...  0.05869 -0.586076  -0.813125   \n",
      "              TEP.PA   -0.905425  ...  0.28968 -0.469373  -0.726042   \n",
      "              TFI.PA   -0.858916  ...  0.16207 -0.148957  -0.779130   \n",
      "              VIRP.PA  -0.899170  ...  0.46392 -0.387973  -0.813607   \n",
      "\n",
      "                       adx14_pos  adx14_dif  pos_avg_vol14  pos_sma20_200  \\\n",
      "OPEN_DATETIME TICKER                                                        \n",
      "1998-01-20    RE.PA    -0.278402   0.210069      -0.924964      -0.332989   \n",
      "              SAVE.PA  -0.421942   0.195591      -0.911933      -0.343584   \n",
      "              TEP.PA   -0.260655   0.232694      -0.953953      -0.384536   \n",
      "              TFI.PA   -0.141195   0.318968      -0.855588      -0.291030   \n",
      "              VIRP.PA  -0.461689   0.175959      -0.954981      -0.456234   \n",
      "\n",
      "                       perf_sma_50_5d  perf_sma_200_5d  lab_perf_20d  \n",
      "OPEN_DATETIME TICKER                                                  \n",
      "1998-01-20    RE.PA          0.056033        -0.048995             3  \n",
      "              SAVE.PA        0.064044        -0.059569             3  \n",
      "              TEP.PA         0.058978        -0.082391             2  \n",
      "              TFI.PA         0.098127        -0.050670             0  \n",
      "              VIRP.PA        0.061845        -0.163107             4  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_20d\"\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2405\"\n",
    "scaler_name=file_name+\"_scaler.save\"\n",
    "scaler=joblib.load(PATH_DATA_DTS+scaler_name)\n",
    "\n",
    "\n",
    "df_class=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_TRAIN,sep=\",\",index_col=[\"OPEN_DATETIME\",\"TICKER\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class=df_class.dropna(subset=[label])\n",
    "df_class=df_class.loc['1995-01-01':] # drop rows < 1995-01-01\n",
    "df_class=df_class.sort_index()\n",
    "df_class_val=pd.read_csv(PATH_DATA_DTS+dts_name+SUFFIX_VAL,sep=\",\",index_col=[\"OPEN_DATETIME\",\"TICKER\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class_val=df_class_val.dropna(subset=[label])\n",
    "df_class_val=df_class_val.sort_index()\n",
    "\n",
    "# normalize df_class and df_class_val\n",
    "df_class_train_norm=balance.normalize_df_scaler(df_in=df_class, str_label=label,scaler=scaler)\n",
    "df_class_val_norm=balance.normalize_df_scaler(df_in=df_class_val, str_label=label,scaler=scaler)\n",
    "\n",
    "print(f\"{df_class_train_norm.shape=} {df_class_val_norm.shape=}\")\n",
    "print(df_class_train_norm[10000:10005])\n",
    "# print type of index of df_class_train_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(df_class_train_norm.index[0])= <class 'tuple'> type(df_class_train_norm.index[1])= <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"{type(df_class_train_norm.index[0])= } {type(df_class_train_norm.index[1])= }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 2024-07-05 17:04:48 cnt=20 ticker='ALDEL.PA'\n",
      "time 2024-07-05 17:08:38 cnt=40 ticker='ALNOV.PA'\n",
      "time 2024-07-05 17:13:25 cnt=60 ticker='BEN.PA'\n",
      "time 2024-07-05 17:19:47 cnt=80 ticker='CDI.PA'\n",
      "time 2024-07-05 17:25:08 cnt=100 ticker='DG.PA'\n",
      "time 2024-07-05 17:29:26 cnt=120 ticker='ETL.PA'\n",
      "time 2024-07-05 17:33:04 cnt=140 ticker='GTT.PA'\n",
      "time 2024-07-05 17:36:51 cnt=160 ticker='LBIRD.PA'\n",
      "time 2024-07-05 17:42:34 cnt=180 ticker='MRN.PA'\n",
      "time 2024-07-05 17:46:25 cnt=200 ticker='POXEL.PA'\n",
      "time 2024-07-05 17:52:45 cnt=220 ticker='SCR.PA'\n",
      "time 2024-07-05 17:59:31 cnt=240 ticker='TRI.PA'\n",
      "2024-07-05 18:02:44 train seq ok\n",
      "time 2024-07-05 18:03:53 cnt=20 ticker='ALCYB.PA'\n",
      "time 2024-07-05 18:04:52 cnt=40 ticker='ALLDL.PA'\n",
      "time 2024-07-05 18:05:56 cnt=60 ticker='ATO.PA'\n",
      "time 2024-07-05 18:07:13 cnt=80 ticker='CAP.PA'\n",
      "time 2024-07-05 18:08:45 cnt=100 ticker='CRLA.PA'\n",
      "time 2024-07-05 18:09:58 cnt=120 ticker='ENGI.PA'\n",
      "time 2024-07-05 18:11:19 cnt=140 ticker='GDS.PA'\n",
      "time 2024-07-05 18:12:53 cnt=160 ticker='IPN.PA'\n",
      "time 2024-07-05 18:13:54 cnt=180 ticker='MC.PA'\n",
      "time 2024-07-05 18:15:07 cnt=200 ticker='NXI.PA'\n",
      "time 2024-07-05 18:16:15 cnt=220 ticker='RCO.PA'\n",
      "time 2024-07-05 18:17:26 cnt=240 ticker='SGO.PA'\n",
      "time 2024-07-05 18:18:31 cnt=260 ticker='UBI.PA'\n",
      "df_class_train_seq.shape=(837054, 29) df_class_val_seq.shape=(233059, 29)\n",
      "                      pos_sma20  pos_sma50  pos_sma200   rsi14  sma20_rsi14  \\\n",
      "OPEN_DATETIME TICKER                                                          \n",
      "2016-07-25    AC.PA    -0.43466   -0.58989    -0.61448 0.05453     -0.02354   \n",
      "2016-07-26    AC.PA    -0.44159   -0.59259    -0.61640 0.03113     -0.01071   \n",
      "2016-07-27    AC.PA    -0.44466   -0.59275    -0.61658 0.02611     -0.00138   \n",
      "2016-07-28    AC.PA    -0.44885   -0.59380    -0.61739 0.01413      0.00816   \n",
      "2016-07-29    AC.PA    -0.44650   -0.59052    -0.61491 0.04028      0.01379   \n",
      "\n",
      "                       ret_5d  pos_top20  pos_top50  pos_bot20  pos_bot50  \\\n",
      "OPEN_DATETIME TICKER                                                        \n",
      "2016-07-25    AC.PA  -0.53983    0.91540    0.81889   -0.94169   -0.94176   \n",
      "2016-07-26    AC.PA  -0.54340    0.90284    0.80736   -0.94471   -0.94419   \n",
      "2016-07-27    AC.PA  -0.54533    0.90027    0.80498   -0.94533   -0.94469   \n",
      "2016-07-28    AC.PA  -0.54387    0.89449    0.79966   -0.94672   -0.94581   \n",
      "2016-07-29    AC.PA  -0.53959    0.90702    0.81120   -0.94370   -0.94338   \n",
      "\n",
      "                      ...    adx14  adx14_neg  adx14_pos  adx14_dif  \\\n",
      "OPEN_DATETIME TICKER  ...                                             \n",
      "2016-07-25    AC.PA   ... -0.79292   -0.54385   -0.42461    0.05962   \n",
      "2016-07-26    AC.PA   ... -0.79818   -0.54391   -0.44704    0.04844   \n",
      "2016-07-27    AC.PA   ... -0.79946   -0.56255   -0.44280    0.05988   \n",
      "2016-07-28    AC.PA   ... -0.81273   -0.47415   -0.51301   -0.01943   \n",
      "2016-07-29    AC.PA   ... -0.82505   -0.49114   -0.52875   -0.01880   \n",
      "\n",
      "                      pos_avg_vol14  pos_sma20_200  perf_sma_50_5d  \\\n",
      "OPEN_DATETIME TICKER                                                 \n",
      "2016-07-25    AC.PA        -0.93641       -0.46200        -0.01720   \n",
      "2016-07-26    AC.PA        -0.92346       -0.45871        -0.01496   \n",
      "2016-07-27    AC.PA        -0.93478       -0.45623        -0.01555   \n",
      "2016-07-28    AC.PA        -0.89073       -0.45365        -0.01445   \n",
      "2016-07-29    AC.PA        -0.92344       -0.45204        -0.01590   \n",
      "\n",
      "                      perf_sma_200_5d  lab_perf_20d  \\\n",
      "OPEN_DATETIME TICKER                                  \n",
      "2016-07-25    AC.PA          -0.19881             0   \n",
      "2016-07-26    AC.PA          -0.19776             0   \n",
      "2016-07-27    AC.PA          -0.19619             0   \n",
      "2016-07-28    AC.PA          -0.19253             0   \n",
      "2016-07-29    AC.PA          -0.18959             0   \n",
      "\n",
      "                                                               SEQUENCE  \n",
      "OPEN_DATETIME TICKER                                                     \n",
      "2016-07-25    AC.PA   [[-0.42446, -0.59085, -0.61649, 0.08634, -0.10...  \n",
      "2016-07-26    AC.PA   [[-0.40231, -0.57259, -0.60201, 0.16833, -0.09...  \n",
      "2016-07-27    AC.PA   [[-0.41091, -0.57664, -0.60492, 0.14463, -0.06...  \n",
      "2016-07-28    AC.PA   [[-0.43487, -0.5937, -0.61808, 0.04526, -0.057...  \n",
      "2016-07-29    AC.PA   [[-0.43305, -0.5923, -0.61688, 0.05092, -0.053...  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "list_feat = df_class.columns.values.tolist()\n",
    "list_feat.remove(label)\n",
    "\n",
    "sequence_length = 10\n",
    "col_sequence = \"SEQUENCE\"\n",
    "\n",
    "# for each TICKER in index of df_class_train_norm, sort data with index and prepare sequences\n",
    "df_class_train_norm_sorted = df_class_train_norm.sort_index(level=['TICKER', 'OPEN_DATETIME'])\n",
    "df_class_val_norm_sorted = df_class_val_norm.sort_index(level=['TICKER', 'OPEN_DATETIME'])\n",
    "\n",
    "# Prepare sequences for each TICKER\n",
    "df_class_train_seq = pd.DataFrame()\n",
    "cnt=0\n",
    "for ticker in df_class_train_norm_sorted.index.get_level_values('TICKER').unique():\n",
    "    sub_df=df_class_train_norm_sorted[df_class_train_norm_sorted.index.get_level_values('TICKER') == ticker]\n",
    "    sub_df = sm.prepare_sequences_df(\n",
    "        df_in=sub_df, list_features=list_feat, sequence_length=sequence_length, str_new_col=col_sequence)\n",
    "    cnt+=1\n",
    "    if cnt%20==0:\n",
    "        print(f\"time {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} {cnt=} {ticker=}\")\n",
    "        gc.collect()\n",
    "    # if cnt==3:\n",
    "    #     break\n",
    "    \n",
    "# concatenate all TICKER data in the same df\n",
    "    df_class_train_seq = pd.concat([df_class_train_seq, sub_df])\n",
    "\n",
    "print((f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} train seq ok\"))\n",
    "\n",
    "df_class_val_seq = pd.DataFrame()\n",
    "cnt=0\n",
    "for ticker in df_class_val_norm_sorted.index.get_level_values('TICKER').unique():\n",
    "    sub_df=df_class_val_norm_sorted[df_class_val_norm_sorted.index.get_level_values('TICKER') == ticker]\n",
    "    sub_df = sm.prepare_sequences_df(\n",
    "        df_in=sub_df, list_features=list_feat, sequence_length=sequence_length, str_new_col=col_sequence)\n",
    "    cnt+=1\n",
    "    if cnt%20==0:\n",
    "        print(f\"time {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} {cnt=} {ticker=}\")\n",
    "        gc.collect()\n",
    "    # if cnt==3:\n",
    "    #     break\n",
    "    \n",
    "# concatenate all TICKER data in the same df\n",
    "    df_class_val_seq = pd.concat([df_class_val_seq, sub_df])\n",
    "\n",
    "print(f\"{df_class_train_seq.shape=} {df_class_val_seq.shape=}\")\n",
    "print(df_class_train_seq[10000:10005])\n",
    "\n",
    "# df_class_train_norm=sm.prepare_sequences_df(df_in=df_class_train_norm,list_features=list_feat,sequence_length=sequence_length,str_new_col=col_sequence)\n",
    "# df_class_val_norm=sm.prepare_sequences_df(df_in=df_class_val_norm,list_features=list_feat,sequence_length=sequence_length,str_new_col=col_sequence)\n",
    "\n",
    "# df_x_train, col_y_train = sm.split_df_x_y(\n",
    "#     df_in=df_class_train_norm, list_features=list_feat, str_label=label, drop_na=True)\n",
    "\n",
    "# df_x_val, col_y_val = sm.split_df_x_y(\n",
    "#     df_in=df_class_val_norm, list_features=list_feat, str_label=label, drop_na=True)\n",
    "\n",
    "\n",
    "\n",
    "# x_train=df_x_train.values\n",
    "# y_train=col_y_train.values\n",
    "# x_val=df_x_val.values\n",
    "# y_val=col_y_val.values\n",
    "# x_train_lstm,y_train_lstm=sm.prepare_sequences(x_train,y_train,sequence_length)\n",
    "# x_val_lstm,y_val_lstm=sm.prepare_sequences(x_val,y_val,sequence_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO put this in a function ??\n",
    "gc.collect()\n",
    "def format_float(x):\n",
    "    return '{:.5f}'.format(x) if x is not None else None\n",
    "\n",
    "def array_to_string(x):\n",
    "    return np.array2string(x,separator='_') if x is not None else None\n",
    "\n",
    "\n",
    "vfunc = np.vectorize(format_float) \n",
    "\n",
    "df_class_train_seq2=df_class_train_seq.copy()\n",
    "df_class_val_seq2=df_class_val_seq.copy()\n",
    "\n",
    "df_class_train_seq2[col_sequence] = df_class_train_seq2[col_sequence].apply(vfunc)\n",
    "df_class_val_seq2[col_sequence] = df_class_val_seq2[col_sequence].apply(vfunc)\n",
    "\n",
    "df_class_train_seq2[col_sequence] = df_class_train_seq2[col_sequence].apply(array_to_string)\n",
    "df_class_val_seq2[col_sequence] = df_class_val_seq2[col_sequence].apply(array_to_string)\n",
    "\n",
    "df_class_train_seq2.round(5).to_csv(\n",
    "    PATH_DATA_DTS+dts_name+\"_TRAIN_seq_6\", sep=\",\", float_format='%.5f')\n",
    "df_class_val_seq2.round(5).to_csv(\n",
    "    PATH_DATA_DTS+dts_name+\"_VAL_seq_6\", sep=\",\", float_format='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START HERE TO LOAD DATASETS WITH SEQUENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_class_train_csv.shape=(834759, 2)\n",
      "                      lab_perf_20d  \\\n",
      "TICKER OPEN_DATETIME                 \n",
      "AB.PA  2015-02-11                0   \n",
      "       2015-02-12                0   \n",
      "       2015-02-13                1   \n",
      "       2015-02-16                1   \n",
      "       2015-02-17                0   \n",
      "\n",
      "                                                               SEQUENCE  \n",
      "TICKER OPEN_DATETIME                                                     \n",
      "AB.PA  2015-02-11     [[-0.39987, -0.49388, -0.45321, 0.33119, 0.245...  \n",
      "       2015-02-12     [[-0.40479, -0.49565, -0.45233, 0.33764, 0.256...  \n",
      "       2015-02-13     [[-0.39282, -0.4837, -0.43821, 0.39571, 0.2664...  \n",
      "       2015-02-16     [[-0.35487, -0.4481, -0.40128, 0.51362, 0.2836...  \n",
      "       2015-02-17     [[-0.35342, -0.4441, -0.39371, 0.5362, 0.29874...  \n",
      "df_class_val_csv.shape=(230586, 2)\n",
      "                       lab_perf_20d  \\\n",
      "TICKER  OPEN_DATETIME                 \n",
      "ABCA.PA 2017-10-16              2.0   \n",
      "        2017-10-17              1.0   \n",
      "        2017-10-18              2.0   \n",
      "        2017-10-19              2.0   \n",
      "        2017-10-20              2.0   \n",
      "\n",
      "                                                                SEQUENCE  \n",
      "TICKER  OPEN_DATETIME                                                     \n",
      "ABCA.PA 2017-10-16     [[-0.4251, -0.57107, -0.62103, 0.32662, -0.155...  \n",
      "        2017-10-17     [[-0.4225, -0.56767, -0.61805, 0.36406, -0.118...  \n",
      "        2017-10-18     [[-0.41547, -0.5608, -0.61241, 0.43196, -0.087...  \n",
      "        2017-10-19     [[-0.42011, -0.563, -0.6135, 0.39188, -0.05539...  \n",
      "        2017-10-20     [[-0.42119, -0.56264, -0.61259, 0.40091, -0.02...  \n",
      "                      lab_perf_20d  \\\n",
      "TICKER OPEN_DATETIME                 \n",
      "AB.PA  2015-02-11              0.0   \n",
      "       2015-02-12              0.0   \n",
      "       2015-02-13              0.0   \n",
      "       2015-02-16              1.0   \n",
      "       2015-02-17              1.0   \n",
      "\n",
      "                                                               SEQUENCE  \n",
      "TICKER OPEN_DATETIME                                                     \n",
      "AB.PA  2015-02-11     [[-0.39987, -0.49388, -0.45321, 0.33119, 0.245...  \n",
      "       2015-02-12     [[-0.40479, -0.49565, -0.45233, 0.33764, 0.256...  \n",
      "       2015-02-13     [[-0.39282, -0.4837, -0.43821, 0.39571, 0.2664...  \n",
      "       2015-02-16     [[-0.35487, -0.4481, -0.40128, 0.51362, 0.2836...  \n",
      "       2015-02-17     [[-0.35342, -0.4441, -0.39371, 0.5362, 0.29874...  \n",
      "                       lab_perf_20d  \\\n",
      "TICKER  OPEN_DATETIME                 \n",
      "ABCA.PA 2017-10-17              2.0   \n",
      "        2017-10-18              1.0   \n",
      "        2017-10-19              2.0   \n",
      "        2017-10-20              2.0   \n",
      "        2017-10-23              2.0   \n",
      "\n",
      "                                                                SEQUENCE  \n",
      "TICKER  OPEN_DATETIME                                                     \n",
      "ABCA.PA 2017-10-17     [[-0.4225, -0.56767, -0.61805, 0.36406, -0.118...  \n",
      "        2017-10-18     [[-0.41547, -0.5608, -0.61241, 0.43196, -0.087...  \n",
      "        2017-10-19     [[-0.42011, -0.563, -0.6135, 0.39188, -0.05539...  \n",
      "        2017-10-20     [[-0.42119, -0.56264, -0.61259, 0.40091, -0.02...  \n",
      "        2017-10-23     [[-0.43434, -0.57196, -0.61907, 0.20803, -0.00...  \n"
     ]
    }
   ],
   "source": [
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_20d\"\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2405\"\n",
    "col_sequence = \"SEQUENCE\"\n",
    "df_class_train_csv=pd.read_csv(PATH_DATA_DTS+dts_name+\"_TRAIN_seq_6.zip\",sep=\",\",index_col=[\"TICKER\",\"OPEN_DATETIME\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class_train_csv=df_class_train_csv.dropna(subset=[col_sequence])\n",
    "df_class_train_csv=df_class_train_csv.sort_index()\n",
    "df_class_val_csv=pd.read_csv(PATH_DATA_DTS+dts_name+\"_VAL_seq_6.zip\",sep=\",\",index_col=[\"TICKER\",\"OPEN_DATETIME\"],parse_dates=[\"OPEN_DATETIME\"])\n",
    "df_class_val_csv=df_class_val_csv.dropna(subset=[col_sequence])\n",
    "df_class_val_csv=df_class_val_csv.sort_index()\n",
    "\n",
    "# keep only index, label and sequence\n",
    "df_class_train_csv=df_class_train_csv[[label,col_sequence]]\n",
    "df_class_val_csv=df_class_val_csv[[label,col_sequence]]\n",
    "\n",
    "df_class_train_csv[col_sequence] = df_class_train_csv[col_sequence].str.replace(\"_\", \",\").apply(ast.literal_eval)\n",
    "df_class_train_csv[col_sequence]  = df_class_train_csv[col_sequence] .apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "df_class_val_csv[col_sequence] = df_class_val_csv[col_sequence].str.replace(\"_\", \",\").apply(ast.literal_eval)\n",
    "df_class_val_csv[col_sequence]  = df_class_val_csv[col_sequence] .apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "\n",
    "print(f\"{df_class_train_csv.shape=}\")\n",
    "print(df_class_train_csv[1015:1020])\n",
    "print(f\"{df_class_val_csv.shape=}\")\n",
    "print(df_class_val_csv[1015:1020])\n",
    "\n",
    "# decision is made between market sessions so we have shift the label of 1 day for each ticker\n",
    "df_class_train_csv[label] = df_class_train_csv.groupby(level='TICKER')[label].shift(1)\n",
    "df_class_train_csv=df_class_train_csv.dropna(subset=[label])\n",
    "df_class_val_csv[label] = df_class_val_csv.groupby(level='TICKER')[label].shift(1)\n",
    "df_class_val_csv=df_class_val_csv.dropna(subset=[label])\n",
    "print(df_class_train_csv[1014:1019])\n",
    "print(df_class_val_csv[1014:1019])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_train_csv.to_pickle(PATH_DATA_DTS+dts_name+\"_TRAIN_seq_6.pckl\")\n",
    "df_class_val_csv.to_pickle(PATH_DATA_DTS+dts_name+\"_VAL_seq_6.pckl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START HERE TO DIRECTLY LOAD THE PICKLE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      lab_perf_20d  \\\n",
      "TICKER OPEN_DATETIME                 \n",
      "AB.PA  2015-02-11              0.0   \n",
      "       2015-02-12              0.0   \n",
      "       2015-02-13              0.0   \n",
      "       2015-02-16              1.0   \n",
      "       2015-02-17              1.0   \n",
      "\n",
      "                                                               SEQUENCE  \n",
      "TICKER OPEN_DATETIME                                                     \n",
      "AB.PA  2015-02-11     [[-0.39987, -0.49388, -0.45321, 0.33119, 0.245...  \n",
      "       2015-02-12     [[-0.40479, -0.49565, -0.45233, 0.33764, 0.256...  \n",
      "       2015-02-13     [[-0.39282, -0.4837, -0.43821, 0.39571, 0.2664...  \n",
      "       2015-02-16     [[-0.35487, -0.4481, -0.40128, 0.51362, 0.2836...  \n",
      "       2015-02-17     [[-0.35342, -0.4441, -0.39371, 0.5362, 0.29874...  \n"
     ]
    }
   ],
   "source": [
    "dts_name=\"PARIS_TREND_1D_20D_V2\"\n",
    "multi_symbol=\"PARIS_STOCK\"\n",
    "label = \"lab_perf_20d\"\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2405\"\n",
    "col_sequence = \"SEQUENCE\"\n",
    "\n",
    "df_class_train_csv=pd.read_pickle(PATH_DATA_DTS+dts_name+\"_TRAIN_seq_6.pckl\")  #the train will be split in train + val\n",
    "df_class_test_csv=pd.read_pickle(PATH_DATA_DTS+dts_name+\"_VAL_seq_6.pckl\") #the val is finally used as a test dataset\n",
    "print(df_class_train_csv[1014:1019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       lab_perf_20d  \\\n",
      "OPEN_DATETIME TICKER                  \n",
      "1995-05-18    LI.PA             1.0   \n",
      "              RE.PA             2.0   \n",
      "              SAVE.PA           2.0   \n",
      "              TEP.PA            2.0   \n",
      "              VIRP.PA           1.0   \n",
      "\n",
      "                                                                SEQUENCE  \n",
      "OPEN_DATETIME TICKER                                                      \n",
      "1995-05-18    LI.PA    [[-0.44468, -0.57928, -0.59788, 0.1422, 0.0469...  \n",
      "              RE.PA    [[-0.45272, -0.55703, -0.62491, 0.13047, 0.225...  \n",
      "              SAVE.PA  [[-0.44879, -0.57938, -0.62212, 0.04232, -0.00...  \n",
      "              TEP.PA   [[-0.47529, -0.6054, -0.6432, -0.20304, -0.006...  \n",
      "              VIRP.PA  [[-0.44257, -0.56061, -0.74347, 0.358, 0.35239...  \n"
     ]
    }
   ],
   "source": [
    "# df_class_train_csv split into train and val with 0.75/0.25 by open datetime using sm.split_df_by_label_strat\n",
    "df_class_train_csv.reset_index(level='TICKER',inplace=True)\n",
    "\n",
    "df_split=sm.split_df_by_label_strat(\n",
    "    df_in=df_class_train_csv, list_label=[label], split_timeframe=\"D\",random_split=False,split_strat=(80,20,0))\n",
    "df_train_split=df_split['df_'+label+'_train']\n",
    "df_val_split=df_split['df_'+label+'_valid']\n",
    "\n",
    "df_train_split.set_index('TICKER',append=True,inplace=True)\n",
    "df_val_split.set_index('TICKER',append=True,inplace=True)\n",
    "df_train_split.sort_index(inplace=True)\n",
    "df_val_split.sort_index(inplace=True)\n",
    "print(df_train_split[1014:1019])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab_perf_20d\n",
      "0.0    123176\n",
      "1.0    110279\n",
      "2.0    109418\n",
      "3.0    109278\n",
      "4.0    114613\n",
      "Name: count, dtype: int64\n",
      "lab_perf_20d\n",
      "0.0    43973\n",
      "1.0    56382\n",
      "2.0    57460\n",
      "3.0    57611\n",
      "4.0    52314\n",
      "Name: count, dtype: int64\n",
      "lab_perf_20d\n",
      "0.0    52460\n",
      "1.0    47659\n",
      "2.0    44551\n",
      "3.0    44057\n",
      "4.0    41585\n",
      "Name: count, dtype: int64\n",
      "lab_perf_20d\n",
      "0.0    30000\n",
      "1.0    30000\n",
      "2.0    30000\n",
      "3.0    30000\n",
      "4.0    30000\n",
      "Name: count, dtype: int64\n",
      "lab_perf_20d\n",
      "0.0    5000\n",
      "1.0    5000\n",
      "2.0    5000\n",
      "3.0    5000\n",
      "4.0    5000\n",
      "Name: count, dtype: int64\n",
      "lab_perf_20d\n",
      "0.0    5000\n",
      "1.0    5000\n",
      "2.0    5000\n",
      "3.0    5000\n",
      "4.0    5000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hebus\\AppData\\Local\\Temp\\ipykernel_15892\\1789223486.py:18: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x_train_tensor = torch.as_tensor(df_class_train_under[col_sequence], dtype=torch.float)\n",
      "C:\\Users\\hebus\\AppData\\Local\\Temp\\ipykernel_15892\\1789223486.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  x_train_tensor = torch.as_tensor(df_class_train_under[col_sequence], dtype=torch.float)\n",
      "C:\\Users\\hebus\\AppData\\Local\\Temp\\ipykernel_15892\\1789223486.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_tensor = torch.tensor(df_class_train_under[label], dtype=torch.int64)\n",
      "C:\\Users\\hebus\\AppData\\Local\\Temp\\ipykernel_15892\\1789223486.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x_val_tensor = torch.as_tensor(df_class_val_under[col_sequence], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader.dataset.tensors[0].shape=torch.Size([150000, 10, 27]) val_loader.dataset.tensors[0].shape=torch.Size([25000, 10, 27]) test_loader.dataset.tensors[0].shape=torch.Size([25000, 10, 27])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hebus\\AppData\\Local\\Temp\\ipykernel_15892\\1789223486.py:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_val_tensor = torch.tensor(df_class_val_under[label], dtype=torch.int64)\n",
      "C:\\Users\\hebus\\AppData\\Local\\Temp\\ipykernel_15892\\1789223486.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x_test_tensor = torch.as_tensor(df_class_test_under[col_sequence], dtype=torch.float)\n",
      "C:\\Users\\hebus\\AppData\\Local\\Temp\\ipykernel_15892\\1789223486.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_tensor = torch.tensor(df_class_test_under[label], dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "# print(f\"{df_class_train_csv.shape=} {df_class_val_csv.shape=}\")\n",
    "print(df_train_split[label].value_counts().sort_index()) # undersampling at 109200\n",
    "print(df_val_split[label].value_counts().sort_index()) # undersampling at 43900\n",
    "print(df_class_test_csv[label].value_counts().sort_index()) # undersampling at 41500\n",
    "\n",
    "nb_val=30000 #109200\n",
    "df_class_train_under=balance.class_custom_undersampler(df_train_split,label,nb_val) # undersampling todo\n",
    "\n",
    "nb_val=5000 #41500\n",
    "df_class_val_under=balance.class_custom_undersampler(df_val_split,label,nb_val)\n",
    "df_class_test_under=balance.class_custom_undersampler(df_class_test_csv,label,nb_val)\n",
    "\n",
    "print(df_class_train_under[label].value_counts().sort_index()) \n",
    "print(df_class_val_under[label].value_counts().sort_index()) \n",
    "print(df_class_test_under[label].value_counts().sort_index()) \n",
    "\n",
    "\n",
    "x_train_tensor = torch.as_tensor(df_class_train_under[col_sequence], dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(df_class_train_under[label], dtype=torch.int64)\n",
    "\n",
    "# x_val_tensor = torch.tensor(df_class_val_under['col_sequence_3'], dtype=torch.float)\n",
    "x_val_tensor = torch.as_tensor(df_class_val_under[col_sequence], dtype=torch.float)\n",
    "y_val_tensor = torch.tensor(df_class_val_under[label], dtype=torch.int64)\n",
    "\n",
    "x_test_tensor = torch.as_tensor(df_class_test_under[col_sequence], dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(df_class_test_under[label], dtype=torch.int64)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# initiate a pytorch randomsampler for train data\n",
    "# train_sampler = RandomSampler(train_dataset,num_samples=100000,replacement=True)\n",
    "\n",
    "batch_size=512\n",
    "num_workers=7\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,drop_last=True,num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,drop_last=True,num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,drop_last=True,num_workers=num_workers)\n",
    " \n",
    "print(f\"{train_loader.dataset.tensors[0].shape=} {val_loader.dataset.tensors[0].shape=} {test_loader.dataset.tensors[0].shape=}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{train_loader.dataset.tensors[0].shape=} {val_loader.dataset.tensors[0].shape=} {test_loader.dataset.tensors[0].shape=}\")\n",
    "#print next(iter(train_loader))\n",
    "pprint(next(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation (Copy from the Tensorflow notebook), not tested here !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_train = df_x_train.corr()\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "corr_train.replace(1,0,inplace=True)\n",
    "corr_train=corr_train.applymap(lambda x : None if x< 0.7 and x>-0.7 else x)\n",
    "corr_train.dropna(axis=0,how='all',inplace=True)\n",
    "corr_train.dropna(axis=1,how='all',inplace=True)\n",
    "\n",
    "# corr_train_check=corr_train[corr_train >0.8]\n",
    "corr_train_check=corr_train\n",
    "sns.heatmap(corr_train_check, annot=False, cmap='coolwarm', vmin=-1, vmax=1, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_class, x='pos_sma200', y='pos_top50', hue='lab_perf_20d', palette='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | layers    | ModuleList       | 49.0 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "49.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "49.0 K    Total params\n",
      "0.196     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_valid={'layer_configs': [{'type': 'Linear', 'input_dim': 27, 'output_dim': 27}, {'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.0, 'bidirectional': True}, {'type': 'Linear', 'input_dim': 128, 'output_dim': 5}, {'type': 'Softmax', 'dim': 1}], 'optimizer__lr': 0.01}\n",
      "config={'type': 'Linear', 'input_dim': 27, 'output_dim': 27}\n",
      "config={'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.0, 'bidirectional': True}\n",
      "config={'type': 'Linear', 'input_dim': 128, 'output_dim': 5}\n",
      "config={'type': 'Softmax', 'dim': 1}\n",
      "Epoch 0: 100%|██████████| 292/292 [00:50<00:00,  5.81it/s, v_num=52, train_loss_step=1.690, train_acc_step=0.213, val_loss_step=1.750, val_acc_step=0.158, val_loss_epoch=1.700, val_acc_epoch=0.200, train_loss_epoch=1.640, train_acc_epoch=0.214]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.704\n",
      "Epoch 0, global step 292: 'val_loss' reached 1.70439 (best 1.70439), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_1-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 292/292 [00:50<00:00,  5.83it/s, v_num=52, train_loss_step=1.580, train_acc_step=0.254, val_loss_step=1.620, val_acc_step=0.221, val_loss_epoch=1.650, val_acc_epoch=0.200, train_loss_epoch=1.680, train_acc_epoch=0.204] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.058 >= min_delta = 0.001. New best score: 1.647\n",
      "Epoch 1, global step 584: 'val_loss' reached 1.64658 (best 1.64658), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_1-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 292/292 [00:51<00:00,  5.71it/s, v_num=52, train_loss_step=1.580, train_acc_step=0.271, val_loss_step=1.630, val_acc_step=0.215, val_loss_epoch=1.650, val_acc_epoch=0.200, train_loss_epoch=1.640, train_acc_epoch=0.201] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 876: 'val_loss' reached 1.64645 (best 1.64645), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_1-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 292/292 [01:19<00:00,  3.65it/s, v_num=52, train_loss_step=1.850, train_acc_step=0.0527, val_loss_step=1.730, val_acc_step=0.180, val_loss_epoch=1.700, val_acc_epoch=0.201, train_loss_epoch=1.660, train_acc_epoch=0.204]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1168: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 292/292 [01:54<00:00,  2.55it/s, v_num=52, train_loss_step=1.850, train_acc_step=0.0527, val_loss_step=1.730, val_acc_step=0.180, val_loss_epoch=1.700, val_acc_epoch=0.201, train_loss_epoch=1.700, train_acc_epoch=0.200]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1460: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 292/292 [01:55<00:00,  2.53it/s, v_num=52, train_loss_step=1.850, train_acc_step=0.0527, val_loss_step=1.730, val_acc_step=0.180, val_loss_epoch=1.700, val_acc_epoch=0.201, train_loss_epoch=1.700, train_acc_epoch=0.200]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1752: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 292/292 [01:43<00:00,  2.81it/s, v_num=52, train_loss_step=1.850, train_acc_step=0.0527, val_loss_step=1.730, val_acc_step=0.180, val_loss_epoch=1.700, val_acc_epoch=0.201, train_loss_epoch=1.700, train_acc_epoch=0.200]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 1.647. Signaling Trainer to stop.\n",
      "Epoch 6, global step 2044: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 292/292 [01:43<00:00,  2.81it/s, v_num=52, train_loss_step=1.850, train_acc_step=0.0527, val_loss_step=1.730, val_acc_step=0.180, val_loss_epoch=1.700, val_acc_epoch=0.201, train_loss_epoch=1.700, train_acc_epoch=0.200]\n",
      "checkpoint_callback.best_model_path='C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_1-v2.ckpt'\n",
      "config={'type': 'Linear', 'input_dim': 27, 'output_dim': 27}\n",
      "config={'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.0, 'bidirectional': True}\n",
      "config={'type': 'Linear', 'input_dim': 128, 'output_dim': 5}\n",
      "config={'type': 'Softmax', 'dim': 1}\n",
      "Testing DataLoader 0: 100%|██████████| 48/48 [00:01<00:00, 39.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.2010498046875      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.6465201377868652     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.2010498046875     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.6465201377868652    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | layers    | ModuleList       | 49.0 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "49.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "49.0 K    Total params\n",
      "0.196     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result[0]={'test_loss_epoch': 1.6465201377868652, 'test_acc_epoch': 0.2010498046875}\n",
      "Optim cpt=1 checkpoint_callback.best_model_path='C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_1-v2.ckpt' result[0]['test_acc_epoch']=0.2010498046875\n",
      "config={'type': 'Linear', 'input_dim': 27, 'output_dim': 27}\n",
      "config={'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.0, 'bidirectional': True}\n",
      "config={'type': 'Linear', 'input_dim': 128, 'output_dim': 5}\n",
      "config={'type': 'Softmax', 'dim': 1}\n",
      "Epoch 0: 100%|██████████| 292/292 [00:44<00:00,  6.55it/s, v_num=53, train_loss_step=1.610, train_acc_step=0.213, val_loss_step=1.610, val_acc_step=0.158, val_loss_epoch=1.610, val_acc_epoch=0.201, train_loss_epoch=1.620, train_acc_epoch=0.212]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.609\n",
      "Epoch 0, global step 292: 'val_loss' reached 1.60866 (best 1.60866), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_2-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 292/292 [01:22<00:00,  3.55it/s, v_num=53, train_loss_step=1.600, train_acc_step=0.211, val_loss_step=1.600, val_acc_step=0.211, val_loss_epoch=1.600, val_acc_epoch=0.219, train_loss_epoch=1.610, train_acc_epoch=0.217] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.001. New best score: 1.605\n",
      "Epoch 1, global step 584: 'val_loss' reached 1.60472 (best 1.60472), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_2-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 292/292 [01:19<00:00,  3.68it/s, v_num=53, train_loss_step=1.590, train_acc_step=0.234, val_loss_step=1.600, val_acc_step=0.209, val_loss_epoch=1.600, val_acc_epoch=0.223, train_loss_epoch=1.610, train_acc_epoch=0.230] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.602\n",
      "Epoch 2, global step 876: 'val_loss' reached 1.60233 (best 1.60233), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_2-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 292/292 [00:57<00:00,  5.09it/s, v_num=53, train_loss_step=1.590, train_acc_step=0.250, val_loss_step=1.590, val_acc_step=0.266, val_loss_epoch=1.600, val_acc_epoch=0.226, train_loss_epoch=1.600, train_acc_epoch=0.239] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1168: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 292/292 [01:09<00:00,  4.21it/s, v_num=53, train_loss_step=1.590, train_acc_step=0.232, val_loss_step=1.590, val_acc_step=0.262, val_loss_epoch=1.600, val_acc_epoch=0.233, train_loss_epoch=1.600, train_acc_epoch=0.242] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.600\n",
      "Epoch 4, global step 1460: 'val_loss' reached 1.60023 (best 1.60023), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_2-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 292/292 [01:05<00:00,  4.44it/s, v_num=53, train_loss_step=1.580, train_acc_step=0.262, val_loss_step=1.600, val_acc_step=0.242, val_loss_epoch=1.620, val_acc_epoch=0.206, train_loss_epoch=1.600, train_acc_epoch=0.240] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1752: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 292/292 [01:50<00:00,  2.65it/s, v_num=53, train_loss_step=1.590, train_acc_step=0.229, val_loss_step=1.590, val_acc_step=0.225, val_loss_epoch=1.600, val_acc_epoch=0.222, train_loss_epoch=1.600, train_acc_epoch=0.242] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 2044: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 292/292 [01:50<00:00,  2.65it/s, v_num=53, train_loss_step=1.580, train_acc_step=0.219, val_loss_step=1.590, val_acc_step=0.225, val_loss_epoch=1.610, val_acc_epoch=0.222, train_loss_epoch=1.600, train_acc_epoch=0.243] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 2336: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 292/292 [01:42<00:00,  2.84it/s, v_num=53, train_loss_step=1.580, train_acc_step=0.213, val_loss_step=1.590, val_acc_step=0.238, val_loss_epoch=1.600, val_acc_epoch=0.221, train_loss_epoch=1.600, train_acc_epoch=0.240] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 2628: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 292/292 [01:40<00:00,  2.90it/s, v_num=53, train_loss_step=1.590, train_acc_step=0.207, val_loss_step=1.590, val_acc_step=0.242, val_loss_epoch=1.600, val_acc_epoch=0.240, train_loss_epoch=1.590, train_acc_epoch=0.239] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 1.600. Signaling Trainer to stop.\n",
      "Epoch 9, global step 2920: 'val_loss' reached 1.59940 (best 1.59940), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_2-v1.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 292/292 [01:40<00:00,  2.90it/s, v_num=53, train_loss_step=1.590, train_acc_step=0.207, val_loss_step=1.590, val_acc_step=0.242, val_loss_epoch=1.600, val_acc_epoch=0.240, train_loss_epoch=1.590, train_acc_epoch=0.239]\n",
      "checkpoint_callback.best_model_path='C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_2-v1.ckpt'\n",
      "config={'type': 'Linear', 'input_dim': 27, 'output_dim': 27}\n",
      "config={'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.0, 'bidirectional': True}\n",
      "config={'type': 'Linear', 'input_dim': 128, 'output_dim': 5}\n",
      "config={'type': 'Softmax', 'dim': 1}\n",
      "Testing DataLoader 0: 100%|██████████| 48/48 [00:03<00:00, 13.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2444254606962204     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.5895849466323853     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2444254606962204    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.5895849466323853    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | layers    | ModuleList       | 49.0 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "49.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "49.0 K    Total params\n",
      "0.196     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result[0]={'test_loss_epoch': 1.5895849466323853, 'test_acc_epoch': 0.2444254606962204}\n",
      "Optim cpt=2 checkpoint_callback.best_model_path='C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_2-v1.ckpt' result[0]['test_acc_epoch']=0.2444254606962204\n",
      "config={'type': 'Linear', 'input_dim': 27, 'output_dim': 27}\n",
      "config={'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.0, 'bidirectional': True}\n",
      "config={'type': 'Linear', 'input_dim': 128, 'output_dim': 5}\n",
      "config={'type': 'Softmax', 'dim': 1}\n",
      "Epoch 0: 100%|██████████| 292/292 [01:07<00:00,  4.30it/s, v_num=54, train_loss_step=1.590, train_acc_step=0.254, val_loss_step=1.600, val_acc_step=0.244, val_loss_epoch=1.610, val_acc_epoch=0.199, train_loss_epoch=1.620, train_acc_epoch=0.217]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.615\n",
      "Epoch 0, global step 292: 'val_loss' reached 1.61464 (best 1.61464), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_3-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 292/292 [00:51<00:00,  5.65it/s, v_num=54, train_loss_step=1.570, train_acc_step=0.277, val_loss_step=1.610, val_acc_step=0.223, val_loss_epoch=1.630, val_acc_epoch=0.198, train_loss_epoch=1.630, train_acc_epoch=0.215] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 584: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 292/292 [00:59<00:00,  4.88it/s, v_num=54, train_loss_step=1.630, train_acc_step=0.0762, val_loss_step=1.600, val_acc_step=0.227, val_loss_epoch=1.610, val_acc_epoch=0.199, train_loss_epoch=1.630, train_acc_epoch=0.201]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 876: 'val_loss' reached 1.61392 (best 1.61392), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_3-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 292/292 [01:05<00:00,  4.46it/s, v_num=54, train_loss_step=1.620, train_acc_step=0.195, val_loss_step=1.600, val_acc_step=0.268, val_loss_epoch=1.610, val_acc_epoch=0.224, train_loss_epoch=1.620, train_acc_epoch=0.213] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.001. New best score: 1.610\n",
      "Epoch 3, global step 1168: 'val_loss' reached 1.61025 (best 1.61025), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_3-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 292/292 [01:16<00:00,  3.81it/s, v_num=54, train_loss_step=1.610, train_acc_step=0.248, val_loss_step=1.590, val_acc_step=0.264, val_loss_epoch=1.610, val_acc_epoch=0.220, train_loss_epoch=1.610, train_acc_epoch=0.228] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.608\n",
      "Epoch 4, global step 1460: 'val_loss' reached 1.60797 (best 1.60797), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_3-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 292/292 [01:16<00:00,  3.83it/s, v_num=54, train_loss_step=1.590, train_acc_step=0.242, val_loss_step=1.590, val_acc_step=0.262, val_loss_epoch=1.610, val_acc_epoch=0.224, train_loss_epoch=1.610, train_acc_epoch=0.232] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.606\n",
      "Epoch 5, global step 1752: 'val_loss' reached 1.60628 (best 1.60628), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_3-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 292/292 [01:27<00:00,  3.34it/s, v_num=54, train_loss_step=1.590, train_acc_step=0.234, val_loss_step=1.590, val_acc_step=0.262, val_loss_epoch=1.600, val_acc_epoch=0.227, train_loss_epoch=1.600, train_acc_epoch=0.236] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.604\n",
      "Epoch 6, global step 2044: 'val_loss' reached 1.60414 (best 1.60414), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_3-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 292/292 [01:08<00:00,  4.28it/s, v_num=54, train_loss_step=1.590, train_acc_step=0.250, val_loss_step=1.580, val_acc_step=0.266, val_loss_epoch=1.610, val_acc_epoch=0.226, train_loss_epoch=1.600, train_acc_epoch=0.236] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 2336: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 292/292 [01:22<00:00,  3.56it/s, v_num=54, train_loss_step=1.590, train_acc_step=0.232, val_loss_step=1.580, val_acc_step=0.275, val_loss_epoch=1.600, val_acc_epoch=0.231, train_loss_epoch=1.600, train_acc_epoch=0.243] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.602\n",
      "Epoch 8, global step 2628: 'val_loss' reached 1.60205 (best 1.60205), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_3-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 292/292 [01:07<00:00,  4.35it/s, v_num=54, train_loss_step=1.600, train_acc_step=0.227, val_loss_step=1.590, val_acc_step=0.217, val_loss_epoch=1.610, val_acc_epoch=0.230, train_loss_epoch=1.600, train_acc_epoch=0.242] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 2920: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 292/292 [01:07<00:00,  4.35it/s, v_num=54, train_loss_step=1.600, train_acc_step=0.227, val_loss_step=1.590, val_acc_step=0.217, val_loss_epoch=1.610, val_acc_epoch=0.230, train_loss_epoch=1.600, train_acc_epoch=0.242]\n",
      "checkpoint_callback.best_model_path='C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_3-v1.ckpt'\n",
      "config={'type': 'Linear', 'input_dim': 27, 'output_dim': 27}\n",
      "config={'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.0, 'bidirectional': True}\n",
      "config={'type': 'Linear', 'input_dim': 128, 'output_dim': 5}\n",
      "config={'type': 'Softmax', 'dim': 1}\n",
      "Testing DataLoader 0: 100%|██████████| 48/48 [00:01<00:00, 26.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2432454377412796     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.5952256917953491     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2432454377412796    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.5952256917953491    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result[0]={'test_loss_epoch': 1.5952256917953491, 'test_acc_epoch': 0.2432454377412796}\n",
      "Optim cpt=3 checkpoint_callback.best_model_path='C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_0_3-v1.ckpt' result[0]['test_acc_epoch']=0.2432454377412796\n",
      "Optim fail cpt=3 param suivant cpt_param=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | layers    | ModuleList       | 17.2 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "17.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.2 K    Total params\n",
      "0.069     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_valid={'layer_configs': [{'type': 'Linear', 'input_dim': 27, 'output_dim': 27}, {'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.2, 'bidirectional': False}, {'type': 'Linear', 'input_dim': 32, 'output_dim': 5}, {'type': 'Softmax', 'dim': 1}], 'optimizer__lr': 0.01}\n",
      "config={'type': 'Linear', 'input_dim': 27, 'output_dim': 27}\n",
      "config={'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.2, 'bidirectional': False}\n",
      "config={'type': 'Linear', 'input_dim': 32, 'output_dim': 5}\n",
      "config={'type': 'Softmax', 'dim': 1}\n",
      "Epoch 0: 100%|██████████| 292/292 [00:30<00:00,  9.55it/s, v_num=55, train_loss_step=1.660, train_acc_step=0.213, val_loss_step=1.650, val_acc_step=0.158, val_loss_epoch=1.630, val_acc_epoch=0.200, train_loss_epoch=1.640, train_acc_epoch=0.215]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.629\n",
      "Epoch 0, global step 292: 'val_loss' reached 1.62876 (best 1.62876), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 292/292 [00:31<00:00,  9.27it/s, v_num=55, train_loss_step=1.600, train_acc_step=0.271, val_loss_step=1.650, val_acc_step=0.215, val_loss_epoch=1.650, val_acc_epoch=0.200, train_loss_epoch=1.660, train_acc_epoch=0.204]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 584: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 292/292 [00:30<00:00,  9.56it/s, v_num=55, train_loss_step=1.600, train_acc_step=0.209, val_loss_step=1.630, val_acc_step=0.203, val_loss_epoch=1.630, val_acc_epoch=0.200, train_loss_epoch=1.640, train_acc_epoch=0.207]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 876: 'val_loss' reached 1.62858 (best 1.62858), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 292/292 [00:31<00:00,  9.18it/s, v_num=55, train_loss_step=1.590, train_acc_step=0.271, val_loss_step=1.630, val_acc_step=0.215, val_loss_epoch=1.620, val_acc_epoch=0.200, train_loss_epoch=1.640, train_acc_epoch=0.212]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.001. New best score: 1.624\n",
      "Epoch 3, global step 1168: 'val_loss' reached 1.62427 (best 1.62427), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 292/292 [00:33<00:00,  8.63it/s, v_num=55, train_loss_step=1.580, train_acc_step=0.254, val_loss_step=1.600, val_acc_step=0.244, val_loss_epoch=1.620, val_acc_epoch=0.199, train_loss_epoch=1.630, train_acc_epoch=0.208] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.001. New best score: 1.616\n",
      "Epoch 4, global step 1460: 'val_loss' reached 1.61572 (best 1.61572), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 292/292 [00:35<00:00,  8.16it/s, v_num=55, train_loss_step=1.600, train_acc_step=0.189, val_loss_step=1.610, val_acc_step=0.229, val_loss_epoch=1.610, val_acc_epoch=0.202, train_loss_epoch=1.620, train_acc_epoch=0.210] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.001. New best score: 1.610\n",
      "Epoch 5, global step 1752: 'val_loss' reached 1.61035 (best 1.61035), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 292/292 [00:49<00:00,  5.90it/s, v_num=55, train_loss_step=1.610, train_acc_step=0.207, val_loss_step=1.610, val_acc_step=0.199, val_loss_epoch=1.610, val_acc_epoch=0.201, train_loss_epoch=1.610, train_acc_epoch=0.190]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 2044: 'val_loss' reached 1.60966 (best 1.60966), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 292/292 [00:46<00:00,  6.29it/s, v_num=55, train_loss_step=1.600, train_acc_step=0.242, val_loss_step=1.610, val_acc_step=0.232, val_loss_epoch=1.610, val_acc_epoch=0.199, train_loss_epoch=1.610, train_acc_epoch=0.193] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 2336: 'val_loss' reached 1.60950 (best 1.60950), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 292/292 [00:48<00:00,  5.99it/s, v_num=55, train_loss_step=1.610, train_acc_step=0.211, val_loss_step=1.610, val_acc_step=0.201, val_loss_epoch=1.610, val_acc_epoch=0.203, train_loss_epoch=1.610, train_acc_epoch=0.191]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 1.609\n",
      "Epoch 8, global step 2628: 'val_loss' reached 1.60923 (best 1.60923), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 292/292 [00:49<00:00,  5.88it/s, v_num=55, train_loss_step=1.610, train_acc_step=0.213, val_loss_step=1.610, val_acc_step=0.207, val_loss_epoch=1.610, val_acc_epoch=0.202, train_loss_epoch=1.610, train_acc_epoch=0.184]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 2920: 'val_loss' reached 1.60885 (best 1.60885), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_1.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 292/292 [00:49<00:00,  5.88it/s, v_num=55, train_loss_step=1.610, train_acc_step=0.213, val_loss_step=1.610, val_acc_step=0.207, val_loss_epoch=1.610, val_acc_epoch=0.202, train_loss_epoch=1.610, train_acc_epoch=0.184]\n",
      "checkpoint_callback.best_model_path='C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_1.ckpt'\n",
      "config={'type': 'Linear', 'input_dim': 27, 'output_dim': 27}\n",
      "config={'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.2, 'bidirectional': False}\n",
      "config={'type': 'Linear', 'input_dim': 32, 'output_dim': 5}\n",
      "config={'type': 'Softmax', 'dim': 1}\n",
      "Testing DataLoader 0: 100%|██████████| 48/48 [00:01<00:00, 26.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2014973908662796     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.6089013814926147     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2014973908662796    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.6089013814926147    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | layers    | ModuleList       | 17.2 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "17.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.2 K    Total params\n",
      "0.069     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result[0]={'test_loss_epoch': 1.6089013814926147, 'test_acc_epoch': 0.2014973908662796}\n",
      "Optim cpt=1 checkpoint_callback.best_model_path='C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_1.ckpt' result[0]['test_acc_epoch']=0.2014973908662796\n",
      "config={'type': 'Linear', 'input_dim': 27, 'output_dim': 27}\n",
      "config={'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.2, 'bidirectional': False}\n",
      "config={'type': 'Linear', 'input_dim': 32, 'output_dim': 5}\n",
      "config={'type': 'Softmax', 'dim': 1}\n",
      "Epoch 0: 100%|██████████| 292/292 [00:38<00:00,  7.63it/s, v_num=56, train_loss_step=1.670, train_acc_step=0.213, val_loss_step=1.710, val_acc_step=0.158, val_loss_epoch=1.670, val_acc_epoch=0.200, train_loss_epoch=1.640, train_acc_epoch=0.209]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.668\n",
      "Epoch 0, global step 292: 'val_loss' reached 1.66812 (best 1.66812), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 292/292 [00:45<00:00,  6.45it/s, v_num=56, train_loss_step=1.570, train_acc_step=0.271, val_loss_step=1.600, val_acc_step=0.215, val_loss_epoch=1.620, val_acc_epoch=0.200, train_loss_epoch=1.630, train_acc_epoch=0.212] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.051 >= min_delta = 0.001. New best score: 1.617\n",
      "Epoch 1, global step 584: 'val_loss' reached 1.61696 (best 1.61696), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 292/292 [00:43<00:00,  6.66it/s, v_num=56, train_loss_step=1.610, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.200]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.001. New best score: 1.610\n",
      "Epoch 2, global step 876: 'val_loss' reached 1.60954 (best 1.60954), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 292/292 [00:44<00:00,  6.59it/s, v_num=56, train_loss_step=1.610, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.184]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1168: 'val_loss' reached 1.60951 (best 1.60951), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 292/292 [00:48<00:00,  6.01it/s, v_num=56, train_loss_step=1.610, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.184]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1460: 'val_loss' reached 1.60948 (best 1.60948), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 292/292 [00:46<00:00,  6.30it/s, v_num=56, train_loss_step=1.610, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.184]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1752: 'val_loss' reached 1.60947 (best 1.60947), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 292/292 [00:45<00:00,  6.39it/s, v_num=56, train_loss_step=1.610, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.183]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 2044: 'val_loss' reached 1.60947 (best 1.60947), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 292/292 [00:46<00:00,  6.25it/s, v_num=56, train_loss_step=1.610, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.184]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 1.610. Signaling Trainer to stop.\n",
      "Epoch 7, global step 2336: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 292/292 [00:46<00:00,  6.24it/s, v_num=56, train_loss_step=1.610, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.184]\n",
      "checkpoint_callback.best_model_path='C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_2.ckpt'\n",
      "config={'type': 'Linear', 'input_dim': 27, 'output_dim': 27}\n",
      "config={'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.2, 'bidirectional': False}\n",
      "config={'type': 'Linear', 'input_dim': 32, 'output_dim': 5}\n",
      "config={'type': 'Softmax', 'dim': 1}\n",
      "Testing DataLoader 0: 100%|██████████| 48/48 [00:01<00:00, 38.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.200439453125       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.6094847917556763     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.200439453125      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.6094847917556763    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | layers    | ModuleList       | 17.2 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "17.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.2 K    Total params\n",
      "0.069     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result[0]={'test_loss_epoch': 1.6094847917556763, 'test_acc_epoch': 0.200439453125}\n",
      "Optim cpt=2 checkpoint_callback.best_model_path='C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_2.ckpt' result[0]['test_acc_epoch']=0.200439453125\n",
      "config={'type': 'Linear', 'input_dim': 27, 'output_dim': 27}\n",
      "config={'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.2, 'bidirectional': False}\n",
      "config={'type': 'Linear', 'input_dim': 32, 'output_dim': 5}\n",
      "config={'type': 'Softmax', 'dim': 1}\n",
      "Epoch 0: 100%|██████████| 292/292 [00:32<00:00,  8.88it/s, v_num=57, train_loss_step=1.590, train_acc_step=0.254, val_loss_step=1.600, val_acc_step=0.244, val_loss_epoch=1.610, val_acc_epoch=0.199, train_loss_epoch=1.620, train_acc_epoch=0.214]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.613\n",
      "Epoch 0, global step 292: 'val_loss' reached 1.61308 (best 1.61308), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_3.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 292/292 [00:35<00:00,  8.19it/s, v_num=57, train_loss_step=1.600, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.620, train_acc_epoch=0.209]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.611\n",
      "Epoch 1, global step 584: 'val_loss' reached 1.61101 (best 1.61101), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_3.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 292/292 [00:36<00:00,  8.09it/s, v_num=57, train_loss_step=1.600, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.191]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 876: 'val_loss' reached 1.61015 (best 1.61015), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_3.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 292/292 [00:37<00:00,  7.74it/s, v_num=57, train_loss_step=1.600, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.191]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1168: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 292/292 [00:34<00:00,  8.40it/s, v_num=57, train_loss_step=1.610, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.189]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 1.610\n",
      "Epoch 4, global step 1460: 'val_loss' reached 1.60980 (best 1.60980), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_3.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 292/292 [00:35<00:00,  8.14it/s, v_num=57, train_loss_step=1.600, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.188] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1752: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 292/292 [00:34<00:00,  8.48it/s, v_num=57, train_loss_step=1.620, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.201] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 2044: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 292/292 [00:35<00:00,  8.13it/s, v_num=57, train_loss_step=1.610, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.190]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 2336: 'val_loss' reached 1.60975 (best 1.60975), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_3.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 292/292 [00:34<00:00,  8.50it/s, v_num=57, train_loss_step=1.610, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.184]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 2628: 'val_loss' reached 1.60972 (best 1.60972), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_3.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 292/292 [00:33<00:00,  8.61it/s, v_num=57, train_loss_step=1.610, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.186]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 1.610. Signaling Trainer to stop.\n",
      "Epoch 9, global step 2920: 'val_loss' reached 1.60970 (best 1.60970), saving model to 'C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_3.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 292/292 [00:33<00:00,  8.60it/s, v_num=57, train_loss_step=1.610, train_acc_step=0.209, val_loss_step=1.610, val_acc_step=0.203, val_loss_epoch=1.610, val_acc_epoch=0.200, train_loss_epoch=1.610, train_acc_epoch=0.186]\n",
      "checkpoint_callback.best_model_path='C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_3.ckpt'\n",
      "config={'type': 'Linear', 'input_dim': 27, 'output_dim': 27}\n",
      "config={'type': 'LSTM', 'input_dim': 27, 'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.2, 'bidirectional': False}\n",
      "config={'type': 'Linear', 'input_dim': 32, 'output_dim': 5}\n",
      "config={'type': 'Softmax', 'dim': 1}\n",
      "Testing DataLoader 0: 100%|██████████| 48/48 [00:01<00:00, 36.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2002360075712204     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.6096946001052856     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2002360075712204    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.6096946001052856    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result[0]={'test_loss_epoch': 1.6096946001052856, 'test_acc_epoch': 0.2002360075712204}\n",
      "Optim cpt=3 checkpoint_callback.best_model_path='C:\\\\Projets\\\\Data\\\\Models\\\\PARIS_TREND_1D_20D_V2_lstm_pytorch_v1_20240723_1_3.ckpt' result[0]['test_acc_epoch']=0.2002360075712204\n",
      "Optim fail cpt=3 param suivant cpt_param=2\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "###### REFACTO USING PYTORCH LIGHTNING ########\n",
    "###############################################\n",
    "\n",
    "# Define LSTM model\n",
    "class DynamicLSTMModel(pl.LightningModule):\n",
    "    def __init__(self, layer_configs, lr, criterion):\n",
    "        super(DynamicLSTMModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for config in layer_configs:\n",
    "            # print(f\"{config=}\")\n",
    "            if config['type'] == 'LSTM':\n",
    "                layer = nn.LSTM(input_size=config['input_dim'], hidden_size=config['hidden_dim'], num_layers=config['num_layers'],\n",
    "                                batch_first=True, dropout=config['dropout'], bidirectional=config['bidirectional'])\n",
    "            elif config['type'] == 'Linear':\n",
    "                layer = nn.Linear(config['input_dim'], config['output_dim'])\n",
    "            elif config['type'] == 'Softmax':\n",
    "                layer = nn.Softmax(dim=config['dim'])\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported layer type: {config['type']}\")\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.criterion = criterion\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.LSTM):\n",
    "                # LSTM layers require special handling for initial states\n",
    "                batch_size = x.size(0)\n",
    "                hidden_dim = layer.hidden_size\n",
    "                num_layers = layer.num_layers * 2 if layer.bidirectional else layer.num_layers\n",
    "                h0 = torch.zeros(num_layers, batch_size,\n",
    "                                 hidden_dim).to(x.device)\n",
    "                c0 = torch.zeros(num_layers, batch_size,\n",
    "                                 hidden_dim).to(x.device)\n",
    "                x, _ = layer(x, (h0, c0))\n",
    "                x = x[:, -1, :]\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        _, predicted = torch.max(y_hat.data, 1)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        correct = (predicted == y).sum().item()\n",
    "        total = len(y)\n",
    "        self.log(\"train_loss\", loss, on_step=True,\n",
    "                 on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc\", correct/total, on_step=True,\n",
    "                 on_epoch=True, prog_bar=True, logger=True)\n",
    "        output = {\"loss\": loss, \"train_loss\": loss,\n",
    "                  \"train_correct\": correct, \"train_total\": total}\n",
    "        return output\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        _, predicted = torch.max(y_hat.data, 1)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        correct = (predicted == y).sum().item()\n",
    "        total = len(y)\n",
    "        # output=f\"val_loss: {loss}, val_correct: {correct}, val_total: {y.size(0)}\"\n",
    "        output = {\"loss\": loss, \"val_loss\": loss,\n",
    "                  \"val_correct\": correct, \"val_total\": total}\n",
    "        # self.log(output)\n",
    "        self.log(\"val_loss\", loss, on_step=True,\n",
    "                 on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", correct/total, on_step=True,\n",
    "                 on_epoch=True, prog_bar=True, logger=True)\n",
    "        return output\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        _, predicted = torch.max(y_hat.data, 1)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        correct = (predicted == y).sum().item()\n",
    "        total = len(y)\n",
    "        # output=f\"val_loss: {loss}, val_correct: {correct}, val_total: {y.size(0)}\"\n",
    "        output = {\"loss\": loss, \"test_loss\": loss, \"test_correct\": correct,\n",
    "                  \"test_total\": total, \"test_acc\": correct/total}\n",
    "        # self.log(output)\n",
    "        self.log(\"test_loss\", loss, on_step=True,\n",
    "                 on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_acc\", correct/total, on_step=True,\n",
    "                 on_epoch=True, prog_bar=True, logger=True)\n",
    "        return output\n",
    "\n",
    "    # def test_epoch_end(self, outputs):\n",
    "    #     avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "    #     self.log('test_loss_epoch', avg_loss)\n",
    "\n",
    "    # def on_validation_epoch_end(self, outputs):\n",
    "    #     avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "    #     total_correct = sum(x['val_correct'] for x in outputs)\n",
    "    #     total = sum(x['val_total'] for x in outputs)\n",
    "    #     tensorboard_logs = {'val_loss': avg_loss}\n",
    "    #     return {'val_loss': avg_loss, 'progress_bar': tensorboard_logs, 'val_acc': total_correct / total}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "input_dim = x_train_tensor.shape[2]\n",
    "num_classes = 5\n",
    "epochs = 10  # 350\n",
    "suffix = \"lstm_pytorch_v1\"\n",
    "tb_directory = \"tb_logs\"\n",
    "debug = False\n",
    "patience = 5\n",
    "\n",
    "obj_acc = 0.25\n",
    "cpt_param = 0\n",
    "try_limit = 3\n",
    "pct_check_class = 0.3  # check if at least n% of the validation set per class\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "len_val = x_val_tensor.shape[0]\n",
    "check_class_limit = (len_val/num_classes)*pct_check_class\n",
    "check_class = False  # check if at least obj_acc accuracy per class\n",
    "\n",
    "list_param_valid = [\n",
    "    {'layer_configs': [\n",
    "        {'type': 'Linear', 'input_dim': input_dim, 'output_dim': input_dim},\n",
    "        {'type': 'LSTM', 'input_dim': input_dim, 'hidden_dim': 64,         'num_layers': 1, 'dropout': 0.0, 'bidirectional': True},\n",
    "        # Note: LSTM bidirectional output is doubled\n",
    "        {'type': 'Linear', 'input_dim': 64 * 2, 'output_dim': num_classes},\n",
    "        {'type': 'Softmax', 'dim': 1}\n",
    "    ], 'optimizer__lr': 0.01},\n",
    "    {'layer_configs': [\n",
    "        {'type': 'Linear', 'input_dim': input_dim, 'output_dim': input_dim},\n",
    "        {'type': 'LSTM', 'input_dim': input_dim, 'hidden_dim': 32,\n",
    "         'num_layers': 2, 'dropout': 0.2, 'bidirectional': False},\n",
    "        # Note: LSTM bidirectional output is doubled\n",
    "        {'type': 'Linear', 'input_dim': 32, 'output_dim': num_classes},\n",
    "        {'type': 'Softmax', 'dim': 1}\n",
    "    ], 'optimizer__lr': 0.01},\n",
    "    # {'fit__batch_size': 256, 'model__dropout': 0.05, 'model__layers': [64, 10], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.9},\n",
    "    # {'fit__batch_size': 32, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.7},\n",
    "    # {'fit__batch_size': 32, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.5},\n",
    "    # {'fit__batch_size': 64, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.9},\n",
    "    # {'fit__batch_size': 64, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.7},\n",
    "    # {'fit__batch_size': 64, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.5},\n",
    "]\n",
    "\n",
    "while (cpt_param < len(list_param_valid) and check_class == False):  # loop for parameters\n",
    "    gc.collect()\n",
    "    param_valid = list_param_valid[cpt_param]  # select the current param line\n",
    "    print(f\"{param_valid=}\")\n",
    "    cpt = 0\n",
    "    filename_tmp_model = dts_name+\"_\"+suffix+\".pckl\"\n",
    "\n",
    "    while (cpt < try_limit and check_class == False):  # loop for train models until good results\n",
    "        cpt += 1\n",
    "\n",
    "        model = DynamicLSTMModel(layer_configs=param_valid['layer_configs'], lr=param_valid['optimizer__lr'], criterion=criterion)\n",
    "\n",
    "        if cpt == 1 and debug:\n",
    "            print(model)\n",
    "            print(len(list(model.parameters())))\n",
    "            for i in range(len(list(model.parameters()))):\n",
    "                print(list(model.parameters())[i].size())\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=PATH_DATA+\"\\\\Models\\\\\",  # Specify the directory to save the model\n",
    "            # Specify the filename format\n",
    "            filename=f\"{dts_name}_{suffix}_{datetime.now().strftime('%Y%m%d')}_{cpt_param}_{cpt}\",\n",
    "            save_top_k=1,  # Save only the top k models according to the monitored quantity\n",
    "            verbose=True,\n",
    "            monitor='val_loss',  # Specify the metric to monitor\n",
    "            mode='min',  # Mode can be either 'min', 'max', or 'auto'\n",
    "            save_last=False  # Optionally, you can choose to save the last model\n",
    "        )\n",
    "\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\", min_delta=0.001, patience=patience, verbose=True, mode=\"min\")\n",
    "        logger = TensorBoardLogger(tb_directory, name=\"my_model\")\n",
    "        trainer = pl.Trainer(max_epochs=epochs, callbacks=[\n",
    "                             early_stop_callback, checkpoint_callback], logger=logger)\n",
    "\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "        writer = SummaryWriter(log_dir=tb_directory+\"/model_summary\")\n",
    "        model_summary = str(model).replace(\n",
    "            '\\n', '<br/>').replace(' ', '&nbsp;')\n",
    "        writer.add_text(\"model_v\"+str(logger.version), model_summary)\n",
    "        writer.close()\n",
    "\n",
    "        # trainer.test(dataloaders=test_loader)\n",
    "        print(f\"{checkpoint_callback.best_model_path=}\")\n",
    "        best_model = DynamicLSTMModel.load_from_checkpoint(\n",
    "            checkpoint_callback.best_model_path)\n",
    "        result = trainer.test(best_model, dataloaders=test_loader)\n",
    "        # print(f\"{result[0]=}\")\n",
    "        # print(\n",
    "        #     f\"Optim {cpt=} {checkpoint_callback.best_model_path=} {result[0]['test_acc_epoch']=}\")\n",
    "\n",
    "        if result[0]['test_acc_epoch'] > obj_acc:\n",
    "            # calculate the confusion matrix\n",
    "            y_pred = best_model(x_val_tensor)\n",
    "            _, y_pred_classes = torch.max(y_pred, 1)\n",
    "            confusion = metrics.confusion_matrix(y_val_tensor, y_pred_classes)\n",
    "\n",
    "            print(confusion)\n",
    "\n",
    "            check_class = True\n",
    "\n",
    "            for i in range(num_classes):\n",
    "                nb_lab = sum(y_pred_classes == i)\n",
    "                if nb_lab < check_class_limit:\n",
    "                    check_class = False\n",
    "                    print(\n",
    "                        f\"Check class {i=} {nb_lab=} {check_class=} {check_class_limit=}\")\n",
    "\n",
    "            # check saved model, load to check it's OK\n",
    "            if check_class:\n",
    "                torch.save(model, filename_tmp_model)\n",
    "                saved_model = torch.load(filename_tmp_model)\n",
    "                saved_model.eval()\n",
    "                y_pred = saved_model(x_val_tensor)\n",
    "                _, y_pred_classes = torch.max(y_pred, 1)\n",
    "                confusion = metrics.confusion_matrix(\n",
    "                    y_val_tensor, y_pred_classes)\n",
    "                print(confusion)\n",
    "\n",
    "    if cpt >= try_limit:\n",
    "        cpt_param += 1\n",
    "        print(f\"Optim fail {cpt=} param suivant {cpt_param=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d0d88f74b8f2df02\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d0d88f74b8f2df02\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "###### SAVE CODE FOR BASIC PYTORCH #####\n",
    "###### BEFORE PYTORCH LIGHTNING ########\n",
    "\n",
    "list_param_valid = [\n",
    "                    {'model__dropout': 0.05, 'model__hidden_dim': 16, 'model__num_layers': 2, 'optimizer__lr': 0.1},\n",
    "                    # {'fit__batch_size': 256, 'model__dropout': 0.05, 'model__layers': [64, 10], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.9},\n",
    "                    # {'fit__batch_size': 32, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.7},\n",
    "                    # {'fit__batch_size': 32, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.5},\n",
    "                    # {'fit__batch_size': 64, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.9},\n",
    "                    # {'fit__batch_size': 64, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.7},\n",
    "                    # {'fit__batch_size': 64, 'fit__epochs': 350, 'model__dropout': 0.05, 'model__layers': [128, 20], 'optimizer__lr': 0.1, 'optimizer__momentum': 0.5},\n",
    "]\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_dim = x_train_tensor.shape[2]\n",
    "num_classes = 5\n",
    "epochs = 6#350\n",
    "suffix=\"lstm_pytorch_v1\"\n",
    "filename_tmp_model = dts_name+\"_\"+suffix+\".pckl\"\n",
    "patience = 3\n",
    "\n",
    "val_accuracy=0.0\n",
    "obj_acc=0.25\n",
    "cpt_param=0 \n",
    "try_limit=5\n",
    "pct_check_class=0.4 # check if at least n% of the validation set per class\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "len_val=x_val_tensor.shape[0]\n",
    "check_class_limit=(len_val/num_classes)*pct_check_class\n",
    "check_class=False # check if at least obj_acc accuracy per class\n",
    "\n",
    "while(cpt_param<len(list_param_valid) and check_class==False):\n",
    "    param_valid=list_param_valid[cpt_param] #select the current param line\n",
    "    print(param_valid)\n",
    "    cpt=0\n",
    "\n",
    "    while(cpt<try_limit and check_class==False):\n",
    "        cpt+=1\n",
    "        \n",
    "        model = LSTMModel(input_dim=input_dim, hidden_dim=param_valid['model__hidden_dim'], num_layers=param_valid['model__num_layers'], num_classes=num_classes, dropout=param_valid['model__dropout'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = Adam(model.parameters(), lr=param_valid['optimizer__lr'])\n",
    "\n",
    "        if cpt==1:\n",
    "            print(model)\n",
    "            print(len(list(model.parameters())))\n",
    "            for i in range(len(list(model.parameters()))):\n",
    "                print(list(model.parameters())[i].size())\n",
    "\n",
    "        # Training loop\n",
    "        hist = np.zeros(epochs)\n",
    "        for epoch in range(epochs):\n",
    "            for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if epoch % 1 == 0 :   #change % \n",
    "                print(f\"Epoch {epoch+1} CrossEntropyLoss: {loss.item()}\")\n",
    "            hist[epoch] = loss.item()\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                outputs = model(x_batch)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        val_accuracy = correct / total\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "        if val_accuracy>obj_acc:\n",
    "            print(f\"Optim success {cpt=} {val_accuracy=}\")\n",
    "            check_class=True #exit directly\n",
    "\n",
    "            # calculate the confusion matrix\n",
    "            y_pred = model(x_val_tensor)\n",
    "            _, y_pred_classes = torch.max(y_pred, 1)\n",
    "            confusion = metrics.confusion_matrix(y_val_tensor, y_pred_classes)\n",
    "            print(confusion)\n",
    "\n",
    "            for i in range(num_classes):\n",
    "                nb_lab=sum(y_pred_classes == i)\n",
    "                if nb_lab<check_class_limit  :\n",
    "                    check_class=False\n",
    "                    print(f\"Check class {i=} {nb_lab=} {check_class=} {check_class_limit=}\")\n",
    "                # print(f\"Categ {i}: real {sum(y_val_tensor == i)} predict {sum(y_pred_classes == i)}\")\n",
    "\n",
    "\n",
    "            #check saved model, load to check it's OK\n",
    "            if check_class:\n",
    "                torch.save(model, filename_tmp_model)\n",
    "                saved_model = torch.load(filename_tmp_model)\n",
    "                saved_model.eval()\n",
    "                y_pred = saved_model(x_val_tensor)\n",
    "                _, y_pred_classes = torch.max(y_pred, 1)\n",
    "                confusion = metrics.confusion_matrix(y_val_tensor, y_pred_classes)\n",
    "                print(confusion)\n",
    "\n",
    "    if cpt>=try_limit :\n",
    "        cpt_param+=1\n",
    "        print(f\"Optim fail {cpt=} param suivant {cpt_param=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[-1]\n",
    "window_size = sequence_length\n",
    "dropout = 0.2\n",
    "num_classes = 4\n",
    "\n",
    "# cat_y_train = keras.utils.to_categorical(col_y_train, num_classes)\n",
    "# cat_y_valid = keras.utils.to_categorical(col_y_valid, num_classes)\n",
    "\n",
    "# df_x_train_exp = np.expand_dims(df_x_train, axis=2)\n",
    "# df_x_valid_exp = np.expand_dims(df_x_valid, axis=2)\n",
    "\n",
    "\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(LSTM(units=20, return_sequences=False,#True\n",
    "               input_shape=(window_size, input_dim)))\n",
    "#,kernel_regularizer=l2(0.1), recurrent_regularizer=l2(0.1), bias_regularizer=l2(0.1)\n",
    "model_LSTM.add(Dropout(rate=dropout))   \n",
    "# model_LSTM.add(Dropout(rate=dropout))\n",
    "# model_LSTM.add(Bidirectional(LSTM((window_size * 2), return_sequences=True)))\n",
    "# model_LSTM.add(Dropout(rate=dropout))\n",
    "# model_LSTM.add(Bidirectional(LSTM(window_size, return_sequences=False)))\n",
    "model_LSTM.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "model_LSTM.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model_LSTM.fit(x_train_lstm, y_train_lstm, batch_size=1024,\n",
    "                         shuffle=False, epochs=20, validation_data=(x_val_lstm, y_val_lstm))#,verbose=0\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Plot loss\n",
    "epochs = range(1, len(train_accuracy) + 1)\n",
    "plt.plot(epochs, train_accuracy, 'bo-', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'ro-', label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12302096189872760406\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# print if keras can use the gpu to train the model\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
